%!TEX root = ../Thesis.tex

\section{Learning machines}
The idea of learning machines is not a new one. Great minds have tinkered with the idea of an intelligent machine ever since Aristotle first defined his logic of syllogisms; perhaps even earlier. More recently, research into \gls{AI} has experienced periods of varying funding and public popularity. The first years of the Cold War saw increased interest in automated computer processing along with the development of the perceptron \cite{Rosenblatt1957} in 1957 which sparked optimistic dreams of an artificial machine intelligence. However, a combination of events such as the so-called Lighthill report \cite{Lighthill1973} and the book by Minsky and Papert \cite{Minsky1969} contributed to the start of a period of decreased funding in \gls{AI} research. This period, starting in the 1970's and ending in the 1990's, often goes by the figurative name of the ``AI winter".

In 1986, Rumelhart, Hinton and Williams popularized the backpropagation algorithm that allowed for efficiently training increasingly advanced, now multilayered, perceptrons by error gradients \cite{Rumelhart1986}. The following years saw the shift from a knowledge-driven approach to machine learning to a data-driven one and with it came developments such as 1D and 2D convolutional neural networks in 1988 and 1989 \cite{Hinton1988, LeCun1989} and the support vector machine in 1995 \cite{Cortes1995}.

In the late 2000's, driven primarily by easy access to large datasets and increased computational power, the machine learning subfield of deep learning has seen a major revival.
Many tasks that were previously dominated by other \gls{ML} models often relying on engineered features have been bested by deep neural networks applied to raw data often rivalling or even exceeding human performance in limited domains \cite{Schmidhuber2014}. Although an artificial general intelligence is far from being reality, new developments and new potential applications of \gls{ML} seem to be everyday occurrences.
%Great results have been achieved within fields such as computer vision and natural language processing that 
%In 2009, the ImageNet dataset was released \cite{Deng2009} and since 2010, the ImageNet project has run the \gls{ILSVRC}. In 2012, this contest was convincingly won by a team using a deep neural network, effectively spawning a golden era for the machine learning subfield of deep learning which has lasted through the 2010's.
%Recent developments within the fields of artificial intelligence and machine learning in recent years has spawned yet another round of immense interest in 
%\todo[inline]{Write soft introduction to machine learning}

\iffalse

\subsection{Role of optimization}
\cite{Nocedal2006}

General introduction through optimization:
\begin{enumerate}
    \item Classical methods (LP, QP, SQP problems; Active set, Interior point algorithms)
    \item Black-box methods (stochastic and variational methods)
\end{enumerate}

Application of optimization for training \glspl{NN}


What?

Why?

\subsection{}

\subsection{History of deep learning}
\cite{Schmidhuber2014}
\fi