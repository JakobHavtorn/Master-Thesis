Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Morokoff1995,
abstract = {The standard Monte Carlo approach to evaluating multidimensional integrals using (pseudo)-random integration nodes is frequently used when quadrature methods are too difficult or expensive to implement. As an alternative to the random methods, it has been suggested that lower error and improved convergence may be obtained by replacing the pseudo-random sequences with more uniformly distributed sequences known as quasi-random. In this paper quasi-random (Halton, Sobol', and Faure) and pseudo-random sequences are compared in computational experiments designed to determine the effects on convergence of certain properties of the integrand, including variance, variation, smoothness, and dimension. The results show that variation, which plays an important role in the theoretical upper bound given by the Koksma-Hlawka inequality, does not affect convergence, while variance, the determining factor in random Monte Carlo, is shown to provide a rough upper bound, but does not accurately predict performance. In general, quasi-Monte Carlo methods are superior to random Monte Carlo, but the advantage may be slight, particularly in high dimensions or for integrands that are not smooth. For discontinuous integrands, we derive a bound which shows that the exponent for algebraic decay of the integration error from quasi-Monte Carlo is only slightly larger than 1/2 in high dimensions. {\textcopyright} 1995 Academic Press. All rights reserved.},
author = {Morokoff, William J. and Caflisch, Russel E.},
doi = {10.1006/jcph.1995.1209},
issn = {00219991},
journal = {Journal of Computational Physics},
month = {dec},
number = {2},
pages = {218--230},
publisher = {Academic Press},
title = {{Quasi-Monte Carlo Integration}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0021999185712090},
volume = {122},
year = {1995}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@article{Bauer2007,
abstract = {In this paper we discuss a relation between Learning Theory and Regularization of linear ill-posed inverse problems. It is well known that Tikhonov regularization can be profitably used in the context of supervised learning, where it usually goes under the name of regularized least-squares algorithm. Moreover, the gradient descent algorithm was studied recently, which is an analog of Landweber regularization scheme. In this paper we show that a notion of regularization defined according to what is usually done for ill-posed inverse problems allows to derive learning algorithms which are consistent and provide a fast convergence rate. It turns out that for priors expressed in term of variable Hilbert scales in reproducing kernel Hilbert spaces our results for Tikhonov regularization match those in Smale and Zhou [Learning theory estimates via integral operators and their approximations, submitted for publication, retrievable at ãhttp://www.tti-c.org/smale.htmlã, 2005] and improve the results for Landweber iterations obtained in Yao et al. [On early stopping in gradient descent learning, Constructive Approximation (2005), submitted for publication]. The remarkable fact is that our analysis shows that the same properties are shared by a large class of learning algorithms which are essentially all the linear regularization schemes. The concept of operator monotone functions turns out to be an important tool for the analysis. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
doi = {10.1016/j.jco.2006.07.001},
isbn = {0885-064X},
issn = {0885064X},
journal = {Journal of Complexity},
keywords = {Learning theory,Non-parametric statistics,Regularization theory},
number = {1},
pages = {52--72},
title = {{On regularization algorithms in learning theory}},
volume = {23},
year = {2007}
}
@inproceedings{Kingma2014a,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
address = {Montr{\'{e}}al, Quebec, Canada},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
eprint = {1406.5298},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
month = {jun},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@article{Simonyan2014,
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
archivePrefix = {arXiv},
arxivId = {1406.2199},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1017/CBO9781107415324.004},
eprint = {1406.2199},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {568--576},
pmid = {25246403},
title = {{Two-Stream Convolutional Networks for Action Recognition in Videos}},
url = {http://arxiv.org/abs/1406.2199},
year = {2014}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses âvalue networks' to evaluate board positions and âpolicy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc},
journal = {Nature},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
volume = {529},
year = {2016}
}
@inproceedings{Karpathy2014,
author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {1725--1732},
title = {{Large-scale video classification with convolutional neural networks}},
year = {2014}
}
@inproceedings{Le2013,
author = {Le, Quoc V.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {8595--8598},
title = {{Building high-level features using large scale unsupervised learning}},
year = {2013}
}
@article{Bergstra2012a,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising successâthey appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bergstra, Bengio - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}
@article{Riedmiller2018,
abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1802.10567},
author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
eprint = {1802.10567},
file = {:Users/Jakob/Documents/Mendeley Desktop/Riedmiller et al. - 2018 - Learning by Playing - Solving Sparse Reward Tasks from Scratch.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}},
url = {http://arxiv.org/abs/1802.10567},
year = {2018}
}
@article{Li2016,
abstract = {This paper introduces the variational R$\backslash$'enyi bound (VR) that extends traditional variational inference to R$\backslash$'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.},
archivePrefix = {arXiv},
arxivId = {1602.02311},
author = {Li, Yingzhen and Turner, Richard E.},
eprint = {1602.02311},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li, Turner - 2016 - R{\'{e}}nyi Divergence Variational Inference.pdf:pdf},
issn = {10495258},
title = {{R{\'{e}}nyi Divergence Variational Inference}},
url = {https://arxiv.org/pdf/1602.02311.pdf http://arxiv.org/abs/1602.02311},
year = {2016}
}
@article{Rolfe2016,
abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
archivePrefix = {arXiv},
arxivId = {1609.02200},
author = {Rolfe, Jason Tyler},
eprint = {1609.02200},
file = {:Users/Jakob/Documents/Mendeley Desktop/Rolfe - 2016 - Discrete Variational Autoencoders.pdf:pdf},
isbn = {1511.02386},
journal = {arXiv preprint},
month = {sep},
title = {{Discrete Variational Autoencoders}},
url = {http://arxiv.org/abs/1609.02200},
year = {2016}
}
@inproceedings{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Fern{\'{a}}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:Users/Jakob/Documents/Mendeley Desktop/Graves et al. - 2006 - Connectionist temporal classification.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
pages = {369--376},
pmid = {1000285842},
title = {{Connectionist temporal classification}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143891},
year = {2006}
}
@inproceedings{Li2016c,
abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
archivePrefix = {arXiv},
arxivId = {1606.01541},
author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Jurafsky, Dan and Galley, Michel and Gao, Jianfeng},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/D16-1127},
eprint = {1606.01541},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generation(2).pdf:pdf},
isbn = {9781509061822},
issn = {24699969},
pages = {1192--1202},
title = {{Deep Reinforcement Learning for Dialogue Generation}},
url = {https://aclweb.org/anthology/D16-1127 http://aclweb.org/anthology/D16-1127},
year = {2016}
}
@incollection{Aggarwal2001,
abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
archivePrefix = {arXiv},
arxivId = {0812.0624},
author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
doi = {10.1007/3-540-44503-X_27},
eprint = {0812.0624},
file = {:Users/Jakob/Documents/Mendeley Desktop/Aggarwal, Hinneburg, Keim - 2001 - On the Surprising Behavior of Distance Metrics in High Dimensional Space.pdf:pdf},
isbn = {978-3-540-41456-8},
issn = {0956-7925},
pages = {420--434},
pmid = {25246403},
title = {{On the Surprising Behavior of Distance Metrics in High Dimensional Space}},
url = {https://bib.dbvis.de/uploadedFiles/155.pdf},
year = {2001}
}
@misc{Petersen2012,
abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
address = {Kongens Lyngby, Denmark},
author = {Petersen, Kaare Breandt and Pedersen, Michael Syskind},
file = {:Users/Jakob/Documents/Mendeley Desktop/Petersen, Pedersen - 2012 - The Matrix Cookbook.pdf:pdf},
keywords = {decompositions,derivative of,derivative of inverse matrix,determinant,di erentiate a matrix,douglas l,esben,formulas,linear algebra,matrix algebra,matrix calculus,matrix identities,matrix relations,multivariate calculus,probability,reference work,vector calculus},
mendeley-tags = {decompositions,formulas,linear algebra,matrix calculus,multivariate calculus,probability,reference work,vector calculus},
publisher = {Technical University of Denmark},
title = {{The Matrix Cookbook}},
url = {http://www2.imm.dtu.dk/pubdb/views/edoc{\_}download.php/3274/pdf/imm3274.pdf},
year = {2012}
}
@article{Sutton,
abstract = {This article introduces a class of incremental learning procedures specialized for prediction--that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the differnce between {\{}$\backslash$em temporally succesive predictions{\}}. Although such {\{}$\backslash$em temporal-difference methods{\}} have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; {\{}$\backslash$em and{\}} they produce more accurate predictions. We argue that most problems to which uspervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
author = {Sutton, Richard S.},
doi = {10.1023/A:1018056104778},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sutton - 1988 - Learning to Predict by the Method of Temporal Differences.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Journal of Machine Learning},
keywords = {Incremental learning,connectionism,credit assignment,evaluation functions,prediction},
number = {1},
pages = {9--44},
pmid = {22182453},
title = {{Learning to Predict by the Method of Temporal Differences}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.132.7760{\&}rep=rep1{\&}type=pdf citeseer.ist.psu.edu/sutton88learning.html},
volume = {3},
year = {1988}
}
@inproceedings{Sedighizadeh2008,
abstract = {A self tuning PID control strategy using reinforcement learning is proposed in this paper to deal with the control of wind energy conversion systems (WECS). Actor-Critic learning is used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of reinforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network is used to approximate the policy function of Actor and the value function of Critic simultaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for WECS and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.},
author = {Sedighizadeh, M. and Rezazadeh, A.},
booktitle = {PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sedighizadeh, Rezazadeh - 2008 - Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control.pdf:pdf},
pages = {257--262},
title = {{Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.3676{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@inproceedings{He2014,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {European Conference on Computer Vision},
pages = {346--361},
title = {{Spatial pyramid pooling in deep convolutional networks for visual recognition}},
year = {2014}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
doi = {1706.02515},
eprint = {1706.02515},
file = {:Users/Jakob/Documents/Mendeley Desktop/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {https://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Jozefowicz,
abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
annote = {One should use an initial bias of 1 or 2 to the forget gate. If the bias of the forget gate is not properly initialized, we may erroneously conclude that the LSTM is incapable of learning to solve problems with long-range dependencies, which is not the case},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
file = {:Users/Jakob/Documents/Mendeley Desktop/Jozefowicz, Zaremba, Sutskever - Unknown - An Empirical Exploration of Recurrent Network Architectures.pdf:pdf},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
url = {http://proceedings.mlr.press/v37/jozefowicz15.pdf}
}
@article{Ren2015,
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {1137--1149},
title = {{Faster R-CNN: Towards Real-Time Object Detection with}},
volume = {39},
year = {2017}
}
@article{LeCun1989a,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Yann A. and Boser, B. and Denker, John S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
file = {:Users/Jakob/Documents/Mendeley Desktop/LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Recognition.pdf:pdf},
journal = {Neural Computation},
number = {1},
pages = {541--551},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
year = {1989}
}
@article{Lehman2017a,
abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
archivePrefix = {arXiv},
arxivId = {1712.06563},
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
eprint = {1712.06563},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lehman et al. - 2017 - Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients.pdf:pdf},
journal = {arXiv preprint},
title = {{Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients}},
url = {http://arxiv.org/abs/1712.06563},
year = {2017}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
doi = {10.1038/nature14236},
eprint = {1607.06450},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
isbn = {978-3-642-04273-7},
issn = {1607.06450},
journal = {arXiv preprint},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@article{Wang2007,
abstract = {Aimed at the lack of self-tuning PID parameters in conventional PID controllers, the structure and learning algorithm of an adaptive PID controller based on reinforcement learning were proposed. Actor-Critic learning was used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of re-inforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network was used to approximate the policy function of Actor and the value function of Critic si-multaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for complex nonlinear systems and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.},
author = {Wang, Xue-Song and Cheng, Yu-Hu and Sun, Wei},
doi = {10.1016/S1006-1266(07)60009-1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wang, Cheng, Sun - 2007 - A Proposal of Adaptive PID Controller Based on Reinforcement Learning.pdf:pdf},
isbn = {8651683995},
issn = {10061266},
journal = {Journal of China University of Mining and Technology},
keywords = {actor-critic learning,adaptive pid control,rbf network,reinforcement learning},
number = {1},
pages = {40--44},
title = {{A Proposal of Adaptive PID Controller Based on Reinforcement Learning}},
url = {http://production.datastore.cvt.dk/filestore?oid=539cd4b160ad71dd2500f789{\&}targetid=539cd4b160ad71dd2500f78b},
volume = {17},
year = {2007}
}
@book{Minsky1969,
abstract = {I. Algebraic theory of linear parallel predicates -- 1. Theory of linear Boolean inequalities -- 2. Group invariance of Boolean inequalities -- 3. Parity and one-in-a-box predicates -- 4. The "and/or" theorem -- II. Geometric theory of linear inequalities -- 5. $\Psi$connected: a geometric property with unbounded order -- 6. Geometric patterns of small order: spectra and context -- 7. Stratification and normalization -- 8. The diameter-limited perceptron -- 9. Geometric predicates and serial algorithms -- III. Learning theory -- 10. Magnitude of the coefficients -- 11. Learning -- 12. Linear separation and learning -- 13. Perceptrons and pattern recognition.},
author = {Minsky, Marvin and Papert, Seymour A.},
booktitle = {MIT Press},
isbn = {0-262-63022-2},
pages = {258},
publisher = {MIT Press},
title = {{An Introduction to Computational Geometry}},
year = {1969}
}
@article{Yuan2017,
abstract = {We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.},
archivePrefix = {arXiv},
arxivId = {1705.02012},
author = {Yuan, Xingdi and Wang, Tong and Gulcehre, Caglar and Sordoni, Alessandro and Bachman, Philip and Subramanian, Sandeep and Zhang, Saizheng and Trischler, Adam},
doi = {10.18653/v1/W17-2603},
eprint = {1705.02012},
file = {:Users/Jakob/Documents/Mendeley Desktop/Yuan et al. - 2017 - Machine Comprehension by Text-to-Text Neural Question Generation.pdf:pdf},
journal = {arXiv preprint},
title = {{Machine Comprehension by Text-to-Text Neural Question Generation}},
url = {http://arxiv.org/abs/1705.02012},
year = {2017}
}
@inproceedings{Toshev2014,
author = {Toshev, Alexander and Szegedy, Christian},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {1653--1660},
title = {{Deeppose: Human pose estimation via deep neural networks}},
year = {2014}
}
@article{Hannun2014,
abstract = {We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.},
archivePrefix = {arXiv},
arxivId = {1408.2873},
author = {Hannun, Awni Y. and Maas, Andrew L. and Jurafsky, Daniel and Ng, Andrew Y.},
eprint = {1408.2873},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hannun et al. - 2014 - First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs.pdf:pdf},
month = {aug},
title = {{First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs}},
url = {http://arxiv.org/abs/1408.2873},
year = {2014}
}
@article{Radford2015,
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
journal = {arXiv preprint},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
year = {2015}
}
@article{Mnih2014,
author = {Mnih, Andriy},
doi = {10.1.1.476.3088},
title = {{Learning word embeddings efficiently with noise-contrastive}},
url = {http://findit.dtu.dk/en/catalog/273153637},
year = {2014}
}
@book{Goodfellow2016,
abstract = {Deep learning draws upon many modeling formalisms that researchers can use to guide their design efforts and describe their algorithms. One of these formalisms is the idea of structured probabilistic models. We have already discussed structured probabilistic models briefly in Chapter 3.14. That brief presentation was sufficient to understand how to use structured probabilistic models as a language to describe some of the algorithms in part II of this book. Now, in part III, structured probabilistic models are a key ingredient of many of the most important research topics in deep learning. In order to prepare to discuss these research ideas, this chapter describes structured probabilistic models in much greater detail. This chapter is intended to be self-contained; the reader does not need to review the earlier introduction before continuing with this chapter. A structured probabilistic model is a way of describing a probability distribu-tion, using a graph to describe which random variables in the probability distri-bution interact with each other directly. Here we use " graph " in the graph theory senseâa set of vertices connected to one another by a set of edges. Because the structure of the model is defined by a graph, these models are often also referred to as graphical models. The graphical models research community is large and has developed many different models, training algorithms, and inference algorithms. In this chap-ter, we provide basic background on some of the most central ideas of graphical models, with an emphasis on the concepts that have proven most useful to the deep learning research community. If you already have a strong background in graphical models, you may wish to skip most of this chapter. However, even a graphical model expert may benefit from reading the final section of this chap-ter, section 13.6, in which we highlight some of the unique ways that graphical 412 CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING models are used for deep learning algorithms. Deep learning practitioners tend to use very different model structures, learning algorithms, and inference procedures than are commonly used by the rest of the graphical models research community. In this chapter, we identify these differences in preferences and explain the reasons for them. In this chapter we first describe the challenges of building large-scale proba-bilistic models in section 13.1. Next, we describe how to use a graph to describe the structure of a probability distribution in section 13.2. We then revisit the challenges we described in section 13.1 and show how the structured approach to probabilistic modeling can overcome these challenges in section 13.3. One of the major difficulties in graphical modeling is understanding which variables need to be able to interact directly, i.e., which graph structures are most suitable for a given problem. We outline two approaches to resolving this difficulty by learning about the dependencies in section 13.4. Finally, we close with a discussion of the unique emphasis that deep learning practitioners place on specific approaches to graphical modeling in section 13.6. 13.1 The Challenge of Unstructured Modeling},
address = {Cambridge, Massachusetts},
author = {Goodfellow, Ian J. and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT Press},
edition = {1},
isbn = {9780262035613},
keywords = {deep learning,machine learning},
mendeley-tags = {deep learning,machine learning},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@article{Nesterov1983,
author = {Nesterov, Yurii},
file = {:Users/Jakob/Documents/Mendeley Desktop/Nesterov - 1983 - A Method of Solving A Convex Programming Problem With Convergence rate O(1k2).pdf:pdf},
journal = {Soviet Mathematics Doklady},
number = {2},
pages = {372--376},
title = {{A Method of Solving A Convex Programming Problem With Convergence rate O(1/k{\^{}}2)}},
url = {http://www.core.ucl.ac.be/{~}nesterov/Research/Papers/DAN83.pdf},
volume = {27},
year = {1983}
}
@inproceedings{Zeiler2014a,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689 LNCS},
year = {2014}
}
@article{Chang2015,
abstract = {This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.},
archivePrefix = {arXiv},
arxivId = {1511.02583},
author = {Chang, Jia-Ren and Chen, Yong-Sheng},
eprint = {1511.02583},
file = {:Users/Jakob/Documents/Mendeley Desktop/Chang, Chen - 2015 - Batch-normalized Maxout Network in Network.pdf:pdf},
journal = {arXiv preprint},
title = {{Batch-normalized Maxout Network in Network}},
url = {http://arxiv.org/abs/1511.02583},
year = {2015}
}
@article{Marcus2018,
abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
archivePrefix = {arXiv},
arxivId = {1801.00631},
author = {Marcus, Gary},
eprint = {1801.00631},
file = {:Users/Jakob/Documents/Mendeley Desktop/Marcus - 2018 - Deep Learning A Critical Appraisal.pdf:pdf},
journal = {arXiv preprint},
pages = {1--27},
title = {{Deep Learning: A Critical Appraisal}},
url = {http://arxiv.org/abs/1801.00631},
year = {2018}
}
@inproceedings{Li2016a,
abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
archivePrefix = {arXiv},
arxivId = {1606.01541},
author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Jurafsky, Dan and Galley, Michel and Gao, Jianfeng},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/D16-1127},
eprint = {1606.01541},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generation.pdf:pdf},
isbn = {9781509061822},
issn = {24699969},
pages = {1192--1202},
title = {{Deep Reinforcement Learning for Dialogue Generation}},
url = {https://arxiv.org/pdf/1606.01541.pdf http://aclweb.org/anthology/D16-1127},
year = {2016}
}
@article{Hoffman2012,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hoffman et al. - 2012 - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {()},
pmid = {19926898},
title = {{Stochastic Variational Inference}},
url = {https://arxiv.org/pdf/1206.7051.pdf http://arxiv.org/abs/1206.7051},
year = {2012}
}
@incollection{Rumelhart2013,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence},
doi = {10.1016/B978-1-4832-1446-7.50035-2},
eprint = {arXiv:1011.1669v3},
isbn = {1558600132},
issn = {1-55860-013-2},
pages = {399--421},
pmid = {25246403},
title = {{Learning Internal Representations by Error Propagation}},
year = {2013}
}
@article{Olah2016,
abstract = {Recurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!},
author = {Olah, Chris and Carter, Shan},
doi = {10.23915/distill.00001},
issn = {2476-0757},
journal = {Distill},
month = {sep},
title = {{Attention and Augmented Recurrent Neural Networks}},
url = {http://distill.pub/2016/augmented-rnns},
year = {2016}
}
@article{Tishby1988,
abstract = {The problem of learning a general input-output relation using a " layered neural network " is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, we anive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables us to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the mining set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. We demonstrate the utility of this criterion for selecting the optimal architecture in the contiguity problem. As a theoretical application of the statistical formalism, we discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.},
author = {Tishby, Naftali and Levin, Esther and Solla, Sara A.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Tishby, Levin, Solla - 1988 - Consistent Inference of Probabilities in Layered Networks Predictions and Generalization.pdf:pdf},
title = {{Consistent Inference of Probabilities in Layered Networks: Predictions and Generalization}},
url = {http://mcn2017.wikispaces.com/file/view/TishbyLevinSolla89.pdf/616288993/TishbyLevinSolla89.pdf?responseToken=ab3d4f2674f2fa5b25a1540a82dc153d},
year = {1988}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Cho, Kyunghyun and Bengio, Yoshua and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
file = {:Users/Jakob/Documents/Mendeley Desktop/Pascanu et al. - 2013 - How to Construct Deep Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {https://arxiv.org/abs/1312.6026},
volume = {abs/1312.6},
year = {2013}
}
@inproceedings{Girshick2015,
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {1440--1448},
title = {{Fast r-cnn}},
year = {2015}
}
@misc{OpenCVDevelopmentTeam2014,
author = {{OpenCV Development Team}},
booktitle = {Documentation},
title = {{OpenCV documentation}},
url = {https://docs.opencv.org/master/},
year = {2018}
}
@article{Honkela2004,
abstract = {The bits-back coding first introduced by Wallace in 1990 and later by Hinton and van Camp in 1993 provides an interesting link between Bayesian learning and information-theoretic minimum-description-length (MDL) learning approaches. The bits-back coding allows interpreting the cost function used in the variational Bayesian method called ensemble learning as a code length in addition to the Bayesian view of misfit of the posterior approximation and a lower bound of model evidence. Combining these two viewpoints provides interesting insights to the learning process and the functions of different parts of the model. In this paper, the problem of variational Bayesian learning of hierarchical latent variable models is used to demonstrate the benefits of the two views. The code-length interpretation provides new views to many parts of the problem such as model comparison and pruning and helps explain many phenomena occurring in learning.},
author = {Honkela, Antti and Valpola, Harri},
doi = {10.1109/TNN.2004.828762},
file = {:Users/Jakob/Documents/Mendeley Desktop/Honkela, Valpola - 2004 - Variational learning and bits-back coding An information-theoretic view to Bayesian learning.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {4},
pages = {800--810},
pmid = {15461074},
title = {{Variational learning and bits-back coding: An information-theoretic view to Bayesian learning}},
url = {http://www.helsinki.fi/{~}ahonkela/papers/infview.pdf},
volume = {15},
year = {2004}
}
@inproceedings{Gers2000,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and proteinâprotein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD â¤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Gers, F.A. and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
file = {:Users/Jakob/Documents/Mendeley Desktop/Gers, Schmidhuber - 2000 - Recurrent nets that time and count.pdf:pdf},
pages = {189--194 vol.3},
title = {{Recurrent nets that time and count}},
url = {http://ieeexplore.ieee.org/document/861302/},
year = {2000}
}
@article{Dumoulin2018,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dumoulin, Visin - 2016 - A Guide to Convolution Arithmetic for Deep Learning.pdf:pdf},
journal = {arXiv preprint},
month = {mar},
title = {{A Guide to Convolution Arithmetic for Deep Learning}},
url = {http://arxiv.org/abs/1603.07285 https://github.com/vdumoulin/conv{\_}arithmetic},
year = {2016}
}
@article{Izbicki2014,
abstract = {The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1404.7063},
author = {Izbicki, Rafael and Lee, Ann B. and Schafer, Chad M.},
eprint = {1404.7063},
file = {:Users/Jakob/Documents/Mendeley Desktop/Izbicki, Lee, Schafer - 2014 - High-dimensional density ratio estimation with extensions to approximate likelihood computation.pdf:pdf},
issn = {15337928},
journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {420--429},
title = {{High-dimensional density ratio estimation with extensions to approximate likelihood computation}},
url = {http://www.stat.cmu.edu/{~}annlee/AISTATS2014.pdf},
volume = {33},
year = {2014}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:pdf},
issn = {18792782},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.idsia.ch},
volume = {61},
year = {2015}
}
@article{Hashemi2018,
abstract = {The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.},
archivePrefix = {arXiv},
arxivId = {1803.02329},
author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
eprint = {1803.02329},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hashemi et al. - 2018 - Learning Memory Access Patterns.pdf:pdf},
title = {{Learning Memory Access Patterns}},
url = {http://arxiv.org/abs/1803.02329},
year = {2018}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
file = {:Users/Jakob/Documents/Mendeley Desktop/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
volume = {86},
year = {1998}
}
@article{Bolukbasi2016,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
archivePrefix = {arXiv},
arxivId = {1607.06520},
author = {Bolukbasi, Tolga and Chang, Kai-wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
eprint = {1607.06520},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings.pdf:pdf},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
month = {jul},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
url = {http://arxiv.org/abs/1607.06520 https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf},
year = {2016}
}
@inproceedings{Le2014,
author = {Le, Quoc V. and Mikolov, Tomas},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents.}},
volume = {14},
year = {2014}
}
@book{Nocedal2006,
annote = {NULL},
author = {Nocedal, Jorge and Wright, Stephen J.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Nocedal, Wright - 2006 - Numerical optimization.pdf:pdf},
isbn = {0-387-30303-0},
publisher = {Springer},
title = {{Numerical optimization}},
year = {2006}
}
@article{Hernandez-Lobato2015,
abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
archivePrefix = {arXiv},
arxivId = {1502.05336},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Adams, Ryan P.},
eprint = {1502.05336},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Lobato, Adams - 2015 - Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
journal = {arXiv preprint},
title = {{Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks}},
url = {http://arxiv.org/abs/1502.05336},
year = {2015}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1007/978-1-4615-3618-5_2},
file = {:Users/Jakob/Documents/Mendeley Desktop/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.pdf:pdf},
isbn = {978-1-4615-3618-5},
journal = {Journal of Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {3},
pages = {229--256},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Turek2017,
abstract = {Symmetric matrices are widely used in machine learning problems such as kernel machines and manifold learning. Using large datasets often requires computing low-rank approximations of these symmetric matrices so that they fit in memory. In this paper, we present a novel method based on biharmonic interpolation for low-rank matrix approximation. The method exploits knowledge of the data manifold to learn an interpolation operator that approximates values using a subset of randomly selected landmark points. This operator is readily sparsified, reducing memory requirements by at least two orders of magnitude without significant loss in accuracy. We show that our method can approximate very large datasets using twenty times more landmarks than other methods. Further, numerical results suggest that our method is stable even when numerical difficulties arise for other methods.},
archivePrefix = {arXiv},
arxivId = {1705.10887},
author = {Turek, Javier S. and Huth, Alexander},
eprint = {1705.10887},
file = {:Users/Jakob/Documents/Mendeley Desktop/Turek, Huth - 2017 - Efficient, sparse representation of manifold distance matrices for classical scaling.pdf:pdf},
journal = {arXiv preprint},
pages = {1--21},
title = {{Efficient, sparse representation of manifold distance matrices for classical scaling}},
url = {http://arxiv.org/abs/1705.10887},
year = {2017}
}
@article{VanHasselt,
abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
author = {van Hasselt, Hado},
file = {:Users/Jakob/Documents/Mendeley Desktop/van Hasselt - 2010 - Double Q-learning.pdf:pdf},
isbn = {9781617823800},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {1--9},
title = {{Double Q-learning}},
url = {https://papers.nips.cc/paper/3964-double-q-learning.pdf},
year = {2010}
}
@inproceedings{Pathak,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:Users/Jakob/Documents/Mendeley Desktop/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
url = {https://arxiv.org/abs/1705.05363},
volume = {2017-July},
year = {2017}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:Users/Jakob/Documents/Mendeley Desktop/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
volume = {12},
year = {2011}
}
@inproceedings{Graves2013,
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey E.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {6645--6649},
title = {{Speech recognition with deep recurrent neural networks}},
year = {2013}
}
@inproceedings{Suri2011,
address = {New York, New York, USA},
author = {Suri, Siddharth and Vassilvitskii, Sergei},
booktitle = {Proceedings of the 20th international conference on World wide web - WWW '11},
doi = {10.1145/1963405.1963491},
isbn = {9781450306324},
pages = {607},
publisher = {ACM Press},
title = {{Counting triangles and the curse of the last reducer}},
url = {http://portal.acm.org/citation.cfm?doid=1963405.1963491},
year = {2011}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1017/CBO9781139058452},
eprint = {1406.2661},
file = {:Users/Jakob/Documents/Mendeley Desktop/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
month = {jun},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Networks}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Zhang2017a,
abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.{\~{}}fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
annote = {Full covariance matrix for low dimensions},
archivePrefix = {arXiv},
arxivId = {1712.02390},
author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
eprint = {1712.02390},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhang et al. - 2017 - Noisy Natural Gradient as Variational Inference.pdf:pdf},
title = {{Noisy Natural Gradient as Variational Inference}},
url = {http://arxiv.org/abs/1712.02390},
year = {2017}
}
@article{Sutskever2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random âdropoutâ procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overï¬tting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiï¬ed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiï¬ed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
author = {Sutskever, Ilya and Martens, James and Dahl, George E. and Hinton, Geoffrey E.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sutskever et al. - 2013 - On the Importance of Initialization and Momentum in Deep Learning.pdf:pdf},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
title = {{On the Importance of Initialization and Momentum in Deep Learning}},
url = {http://www.cs.toronto.edu/{~}hinton/absps/momentum.pdf},
volume = {28},
year = {2013}
}
@inproceedings{Choromanska2014,
abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
archivePrefix = {arXiv},
arxivId = {1412.0233},
author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'{e}}rard Ben and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
eprint = {1412.0233},
file = {:Users/Jakob/Documents/Mendeley Desktop/Choromanska et al. - 2014 - The Loss Surfaces of Multilayer Networks.pdf:pdf},
issn = {15337928},
title = {{The Loss Surfaces of Multilayer Networks}},
url = {http://proceedings.mlr.press/v38/choromanska15.pdf},
volume = {38},
year = {2014}
}
@article{Hanson1989,
abstract = {Rumelhart (1987). has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart{\textperiodcentered}s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
author = {Hanson, Stephen Jos{\'{e}} and Pratt, Lorien},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hanson, Pratt - 1989 - Comparing Biases for Minimal Network Construction with Back-Propagation.pdf:pdf},
isbn = {1-558-60015-9},
journal = {Advances in Neural Information Processing Systems},
pages = {177--185},
title = {{Comparing Biases for Minimal Network Construction with Back-Propagation}},
url = {https://papers.nips.cc/paper/156-comparing-biases-for-minimal-network-construction-with-back-propagation.pdf http://portal.acm.org/citation.cfm?id=89851.89872},
year = {1989}
}
@article{Lee2016,
abstract = {Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.},
archivePrefix = {arXiv},
arxivId = {1610.03017},
author = {Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
eprint = {1610.03017},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lee, Cho, Hofmann - 2016 - Fully Character-Level Neural Machine Translation without Explicit Segmentation.pdf:pdf},
isbn = {9781510827585},
issn = {2307-387X},
journal = {arXiv preprint},
month = {mar},
title = {{Fully Character-Level Neural Machine Translation without Explicit Segmentation}},
url = {http://arxiv.org/abs/1610.03017},
year = {2016}
}
@article{Ganin2016,
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
journal = {Journal of Machine Learning Research},
number = {59},
pages = {1--35},
title = {{Domain-adversarial training of neural networks}},
volume = {17},
year = {2016}
}
@article{Kullback1951,
abstract = {This note generalizes to the abstract case Shannon's definition of information.},
archivePrefix = {arXiv},
arxivId = {1511.00860},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
eprint = {1511.00860},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kullback, Leibler - 1951 - On Information and Sufficiency.pdf:pdf},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {79--86},
pmid = {10896709},
title = {{On Information and Sufficiency}},
url = {http://projecteuclid.org/euclid.aoms/1177729694},
volume = {22},
year = {1951}
}
@article{Block1962,
abstract = {Block, H. D., Knight, B. W., Jr., Rosenblatt, F. (1962) Analysis of a four-layer series-coupled perceptron. II. Rev. Mod. Phys. 34: 135 THE PERC EPTION. I 100 Câ ISPLAY 75-DISPLAY 50 {\~{}} {\~{}}{\~{}}{\~{}}{\~{}} {\~{}} {\~{}} {\~{}} {\~{}} {\~{}} {\~{}} {\~{}} {\~{}}{\~{}}{\~{}}CHANCE EXPECTAHCY I/2 ASSOCIATIOH 3/4 ASSOCIATIOH {\&}{\&} ASSOCIATION UNITS REMOVED UNITS RKMOYED UNITS REIAO+ED IHT ACT PERCEPTROH (240 ASSOCIATIOH IjHITS) I'ro. 13. EGect of association unit removal on trained "E"â "X" discrimination. simple perceptron of Fig. 4 is no longer adequate. We shall find certain temporal e8ects in the paper which follows, but for others it is necessary to introduce time delays into the system. ' A speech recognizing perceptron which utilizes such delays is currently being built at Cornell University. Other activities now in progress' include quantitative studies of cross-coupled and multi-layer systems (by means of analysis and digital simulation), studies of selective attention mechanisms, the effects of geometric constraints on network organization, new types of reinforcement rules, and attempts at relating this research to biological data. Work is also in progress on development of electrolytic and other low-cost inte-grating devices and additional electronic components necessary for the construction of large-scale physical models. It is clear that we are still far from the point of understanding how the brain functions. It is equally clear, we believe, that a promising road is open for further investigation.},
author = {Block, H. D. and Knight, Bruce and Rosenblatt, Frank},
file = {:Users/Jakob/Documents/Mendeley Desktop/Block, Knight, Rosenblatt - 1962 - Analysis of a Four-Layer Series-Coupled Perceptron II.pdf:pdf},
journal = {Reviews of Modern Physics},
number = {1},
title = {{Analysis of a Four-Layer Series-Coupled Perceptron II}},
url = {http://digitalcommons.rockefeller.edu/knight{\_}laboratory},
volume = {34},
year = {1962}
}
@article{Neal1992,
abstract = {Connectionist learning procedures are presented for "sigmoid" and "noisy-OR" varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the "Gibbs sampling" simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for "Boltzmann machines", and like it, allows the use of "hidden" variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the "negative phase" of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge. {\textcopyright} 1992.},
author = {Neal, Radford M.},
doi = {10.1016/0004-3702(92)90065-6},
file = {:Users/Jakob/Documents/Mendeley Desktop/Neal - 1992 - Connectionist learning of belief networks.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1},
pages = {71--113},
title = {{Connectionist learning of belief networks}},
url = {http://www.cs.toronto.edu/{~}bonner/courses/2016s/csc321/readings/Connectionist learning of belief networks.pdf},
volume = {56},
year = {1992}
}
@article{Chen2014,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1412.7062},
isbn = {9783901608353},
issn = {0162-8828},
journal = {arXiv preprint},
pmid = {28463186},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1412.7062},
year = {2014}
}
@article{Kopp2016,
abstract = {Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.},
archivePrefix = {arXiv},
arxivId = {1604.03736},
author = {K{\"{o}}pp, Wiebke and van der Smagt, Patrick and Urban, Sebastian},
eprint = {1604.03736},
file = {:Users/Jakob/Documents/Mendeley Desktop/K{\"{o}}pp, van der Smagt, Urban - 2016 - A Differentiable Transition Between Additive and Multiplicative Neurons.pdf:pdf},
month = {apr},
title = {{A Differentiable Transition Between Additive and Multiplicative Neurons}},
url = {http://arxiv.org/abs/1604.03736},
year = {2016}
}
@inproceedings{Donahue2015,
author = {Donahue, Jeffrey and {Anne Hendricks}, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {2625--2634},
title = {{Long-term recurrent convolutional networks for visual recognition and description}},
year = {2015}
}
@inproceedings{Zhang2016,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
booktitle = {European Conference on Computer Vision},
pages = {649--666},
title = {{Colorful image colorization}},
year = {2016}
}
@article{Oord2016a,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:Users/Jakob/Documents/Mendeley Desktop/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
journal = {arXiv preprint},
month = {sep},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@inproceedings{Han2016,
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {243--254},
title = {{EIE: efficient inference engine on compressed deep neural network}},
year = {2016}
}
@inproceedings{Liu2016,
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
booktitle = {European Conference on Computer Vision},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
year = {2016}
}
@misc{Karpathy2015b,
author = {Karpathy, Andrej},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
urldate = {2018-04-18},
year = {2015}
}
@misc{Andrychowicz2016,
author = {Andrychowicz, Marcin and Denil, Misha and G{\'{o}}mez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {3981--3989},
title = {{Learning to learn by gradient descent by gradient descent}},
year = {2016}
}
@inproceedings{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
doi = {v5},
eprint = {1503.08895},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
keywords = {Memory Networks,Natural Language Processing,Question answering},
mendeley-tags = {Memory Networks,Natural Language Processing,Question answering},
month = {mar},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@misc{Olah2015,
abstract = {Humans don't start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don't throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can't do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It's unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.},
author = {Olah, Chris},
booktitle = {colah.github.io},
file = {:Users/Jakob/Documents/Mendeley Desktop/Olah - 2015 - Understanding LSTM Networks.html:html},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2018-05-26},
year = {2015}
}
@article{Courbariaux2016,
abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
archivePrefix = {arXiv},
arxivId = {1602.02830},
author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
doi = {10.1109/CVPR.2016.90},
eprint = {1602.02830},
isbn = {9781510829008},
issn = {1664-1078},
month = {feb},
pmid = {23554596},
title = {{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}},
url = {http://arxiv.org/abs/1602.02830},
year = {2016}
}
@misc{Huszar2017,
abstract = {In my last post I conveyed my enthusiasm about evolution strategies (ES), and particularly the highly scalable distributed version. I have to admit, this was the first time I had come across this particular formulation, and unexpectedly, people were quick to point out a whole body of research that I probably should have read or known about. Here, I want to highlight two such papers: - Staines and Barber (2013) Optimization by Variational Bounding ESANN - Wierstra et al (2014) Natural Evolution Strategies JMLR And I also highly recommend David's blog post. This post is just a summary of what I've learnt about ES, a bit of an addendum to last week's post. Thanks to David Barber, Olivier Grisel and Nando de Freitas for their comments and pointers.},
author = {Husz{\'{a}}r, Ferenc},
keywords = {Variational optimization,deep learning,evolutionary computing,optimization,reinforcement learning},
mendeley-tags = {Variational optimization,deep learning,evolutionary computing,optimization,reinforcement learning},
title = {{Evolution Strategies, Variational Optimisation and Natural ES}},
url = {http://www.inference.vc/evolution-strategies-variational-optimisation-and-natural-es-2/},
urldate = {2018-03-23},
year = {2017}
}
@phdthesis{Peters2007,
abstract = {Autonomous robots that can assist humans in situations of daily life have been a long standing vision of robotics, artificial intelligence, and cognitive sciences. A first step towards this goal is to create robots that can accomplish a multitude of different tasks, triggered by environmental context or higher level instruction. Early approaches to this goal during the heydays of artificial intelligence research in the late 1980ies, however, made it clear that an approach purely based on reasoning and human insights would not be able to model all the perceptuomotor tasks that a robot should fulfill. Instead, new hope was put in the growing wake of machine learning that promised fully adaptive control algorithms which learn both by observation and trial-and-error. However, to date, learning techniques have yet to fulfill this promise as only few methods manage to scale into the high-dimensional domains of manipulator robotics, or even the new upcoming trend of humanoid robotics, and usually scaling was only achieved in precisely pre-structured domains. In this thesis, we investigate the ingredients for a general approach to motor skill learning in order to get one step closer towards human-like performance. For doing so, we study two major components for such an approach, i.e., firstly, a theoretically well-founded general approach to representing the required control structures for task representation and execution and, secondly, appropriate learning algorithms which can be applied in this setting. As a theoretical foundation, we first study a general framework to generate control laws for real robots with a particular focus on skills represented as dynamical systems in differential constraint form. We present a point-wise optimal control framework resulting from a generalization of Gauss' principle and show how various well-known robot control laws can be derived by modifying the metric of the employed cost function. The framework has been successfully applied to task space tracking control for holonomic systems for several different metrics on the anthropomorphic SARCOS Master Arm. In order to overcome the limiting requirement of accurate robot models, we first employ learning methods to find learning controllers for task space control. However, when learning to execute a redundant control problem, we face the general problem of the non-convexity of the solution space which can force the robot to steer into physically impossible configurations if supervised learning methods are employed without further consideration. This problem can be resolved using two major insights, i.e., the learning problem can be treated as locally convex and the cost function of the analytical framework can be used to ensure global consistency. Thus, we derive an immediate reinforcement learning algorithm from the expectation-maximization point of view which results in a reward-weighted regression technique. This method can be used both for operational space control as well as general immediate reward reinforcement learning problems. We demonstrate the feasibility of the resulting framework on the problem of redundant end-effector tracking for both a simulated 3 degrees of freedom robot arm as well as for a simulated anthropomorphic SARCOS Master Arm. While learning to execute tasks in task space is an essential component to a general framework to motor skill learning, learning the actual task is of even higher importance, particularly as this issue is more frequently beyond the abilities of analytical approaches than execution. We focus on the learning of elemental tasks which can serve as the âbuilding blocks of movement generationâ, called motor primitives. Motor primitives are parameterized task representations based on splines or nonlinear differential equations with desired attractor properties. While imitation learning of parameterized motor primitives is a relatively well-understood problem, the self-improvement by interaction of the system with the environment remains a challenging problem, tackled in the fourth chapter of this thesis. For pursuing this goal, we highlight the difficulties with current reinforcement learning methods, and outline both established and novel algorithms for the gradient-based improvement of parameterized policies. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. In conclusion, in this thesis, we have contributed a general framework for analytically computing robot control laws which can be used for deriving various previous control approaches and serves as foundation as well as inspiration for our learning algorithms. We have introduced two classes of novel reinforcement learning methods, i.e., the Natural Actor-Critic and the Reward-Weighted Regression algorithm. These algorithms have been used in order to replace the analytical components of the theoretical framework by learned representations. Evaluations have been performed on both simulated and real robot arms.},
author = {Peters, Jan},
booktitle = {University of Southern California},
file = {:Users/Jakob/Documents/Mendeley Desktop/Peters - 2007 - Machine Learning of motor Skills for Robotics.pdf:pdf},
number = {April},
school = {University of Southern California},
title = {{Machine Learning of motor Skills for Robotics}},
url = {http://www.ias.tu-darmstadt.de/uploads/Research/Thesis/thesis{\_}1.pdf},
year = {2007}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0140525X16001837},
eprint = {1706.03762},
file = {:Users/Jakob/Documents/Mendeley Desktop/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
isbn = {9781577357384},
issn = {14691825},
journal = {arXiv preprint},
month = {jun},
pmid = {1000303116},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@article{Salimans2016a,
abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1602.07868},
author = {Salimans, Tim and Kingma, Diederik P.},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1602.07868},
file = {:Users/Jakob/Documents/Mendeley Desktop/Salimans, Kingma - 2016 - Weight Normalization A Simple Reparameterization to Accelerate Training of Deep Neural Networks.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
journal = {arXiv preprint},
month = {feb},
pmid = {172668},
title = {{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.07868},
year = {2016}
}
@article{Sorokin2015,
abstract = {A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.},
archivePrefix = {arXiv},
arxivId = {1512.01693},
author = {Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
eprint = {1512.01693},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sorokin et al. - 2015 - Deep Attention Recurrent Q-Network.pdf:pdf},
month = {dec},
title = {{Deep Attention Recurrent Q-Network}},
url = {http://arxiv.org/abs/1512.01693},
year = {2015}
}
@article{Gal2015,
abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1201/9781420049176},
eprint = {1512.05287},
file = {:Users/Jakob/Documents/Mendeley Desktop/Gal, Ghahramani - 2015 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
journal = {arXiv preprint},
pmid = {21803542},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.05287},
year = {2015}
}
@inproceedings{He2016a,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {European Conference on Computer Vision},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
year = {2016}
}
@article{Wu2016,
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
journal = {arXiv preprint},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
year = {2016}
}
@article{Dong2016,
author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {295--307},
publisher = {IEEE},
title = {{Image super-resolution using deep convolutional networks}},
volume = {38},
year = {2016}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
booktitle = {New York: springer.},
isbn = {978-0-387-31073-2},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Grace2017,
abstract = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50{\%} chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
archivePrefix = {arXiv},
arxivId = {1705.08807},
author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
eprint = {1705.08807},
file = {:Users/Jakob/Documents/Mendeley Desktop/Grace et al. - 2017 - When Will AI Exceed Human Performance Evidence from AI Experts.pdf:pdf},
month = {may},
title = {{When Will AI Exceed Human Performance? Evidence from AI Experts}},
url = {http://arxiv.org/abs/1705.08807},
year = {2017}
}
@inproceedings{Belabbas2007,
abstract = {Computing an efficient low-rank approximation of a given positive definite matrix is a ubiquitous task in statistical signal processing and numerical linear algebra. The optimal solution is well known and is given by the singular value decomposition; however, its complexity scales as the cube of the matrix dimension. Here we introduce a low-complexity alternative which approximates this optimal low-rank solution, together with a bound on its worst-case error. Our methodology also reveals a connection between the approximation of matrix products and Schur complements. We present simulation results that verify performance improvements relative to contemporary randomized algorithms for low-rank approximation.},
author = {Belabbas, Mohamed Ali and Wolfe, Patrick J.},
booktitle = {IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
doi = {10.1109/CAMSAP.2007.4498023},
isbn = {9781424417148},
month = {dec},
pages = {293--296},
publisher = {IEEE},
title = {{Fast low-rank approximation for covariance matrices}},
url = {http://ieeexplore.ieee.org/document/4498023/},
year = {2007}
}
@article{Cho2014a,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
annote = {Introduces the Gated Recurrent Unit cell (GRU)},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/Jakob/Documents/Mendeley Desktop/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
journal = {arXiv preprint},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@book{Glasserman2003,
abstract = {This book develops the use of Monte Carlo methods in finance and it also uses simulation as a vehicle for presenting models and ideas from financial engineering. It divides roughly into three parts.},
archivePrefix = {arXiv},
arxivId = {1011.1669v3},
author = {Glasserman, Paul},
doi = {10.1007/978-0-387-21617-1},
eprint = {1011.1669v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Glasserman - 2003 - Monte Carlo Methods in Financial Engineering.pdf:pdf},
isbn = {978-1-4419-1822-2},
issn = {14697688},
pmid = {25246403},
title = {{Monte Carlo Methods in Financial Engineering}},
url = {http://link.springer.com/10.1007/978-0-387-21617-1},
volume = {53},
year = {2003}
}
@inproceedings{Yang2016,
author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {21--29},
title = {{Stacked attention networks for image question answering}},
year = {2016}
}
@article{Masters,
abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the avail-able computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and train-ing duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size m. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient cal-culations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between m = 2 and m = 32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
archivePrefix = {arXiv},
arxivId = {1804.07612},
author = {Masters, Dominic and Luschi, Carlo},
eprint = {1804.07612},
file = {:Users/Jakob/Documents/Mendeley Desktop/Masters, Luschi - Unknown - Revisiting Small Batch Training for Deep Neural Networks.pdf:pdf},
title = {{Revisiting Small Batch Training for Deep Neural Networks}},
url = {https://arxiv.org/pdf/1804.07612.pdf}
}
@inproceedings{Zhou1988,
author = {Zhou, Y. T. and Chellappa, R.},
booktitle = {Proceedings of the IEEE International Conference on Neural Networks},
doi = {10.1109/ICNN.1988.23914},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhou, Chellappa - 1988 - Computation of Optical Flow using a Neural Network.pdf:pdf},
isbn = {0-7803-0999-5},
number = {86},
pages = {71--78 vol.2},
publisher = {IEEE},
title = {{Computation of Optical Flow using a Neural Network}},
url = {http://ieeexplore.ieee.org/document/23914/},
year = {1988}
}
@article{Bengio2013,
abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
archivePrefix = {arXiv},
arxivId = {1306.1091},
author = {Bengio, Yoshua and Thibodeau-Laufer, {\'{E}}ric and Alain, Guillaume and Yosinski, Jason},
eprint = {1306.1091},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bengio et al. - 2013 - Deep Generative Stochastic Networks Trainable by Backprop.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {arXiv preprint},
title = {{Deep Generative Stochastic Networks Trainable by Backprop}},
url = {http://arxiv.org/abs/1306.1091},
year = {2013}
}
@article{Salimans2016b,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian J. and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
file = {:Users/Jakob/Documents/Mendeley Desktop/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:pdf},
isbn = {0924-6495},
issn = {09246495},
journal = {arXiv preprint},
month = {jun},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@article{Singer1982,
abstract = {A PROBLEM-SOLVING SCHEMA for comprehending short stories was augmented by construction of schema-general questions for each story element. Fifteen eleventh-grade students, randomly assigned to the experimental group, were taught to derive story-specific ques- tions from the schema-general questions as they read complex short stories. The control group read to answer questions posed before- hand by the teacher. Each group read six short stories over a three- week period. Criterion-referenced tests administered after each improve in reader-based processing of text and (2) that story gram- mar structures acquired prior to or during elementary school may be adequate for processing simple fables, but more adequate and more appropriate cognitive structures with strategies for making schema- general questions story-specific are necessary for processing, story resulted in statistically significant differences between the two groups. This evidence implies (1) that instruction can help students storing, and retrieving information derived from reading complex short stories.},
author = {Singer, Harry and Donlan, Dan},
doi = {10.2307/747482},
isbn = {0034-0553},
issn = {00340553},
journal = {Reading Research Quarterly},
number = {2},
pages = {166},
publisher = {WileyInternational Literacy Association},
title = {{Active Comprehension: Problem-Solving Schema with Question Generation for Comprehension of Complex Short Stories}},
url = {https://www.jstor.org/stable/747482?origin=crossref http://www.jstor.org/stable/747482?origin=crossref},
volume = {17},
year = {1982}
}
@article{Hansen2006,
abstract = {CMA Tutorial},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Hansen, Nikolaus},
doi = {10.1007/11007937_4},
eprint = {1011.1669},
isbn = {3540290060},
issn = {14349922},
journal = {Studies in Fuzziness and Soft Computing},
pages = {75--102},
pmid = {15003161},
title = {{The CMA evolution strategy: A comparing review}},
url = {http://arxiv.org/abs/1604.00772},
volume = {192},
year = {2006}
}
@inproceedings{Hernandez-Lobato2016,
abstract = {Black-box alpha (BB-{\$}\backslashalpha{\$}) is a new approximate inference method based on the minimization of {\$}\backslashalpha{\$}-divergences. BB-{\$}\backslashalpha{\$} scales to large datasets because it can be implemented using stochastic gradient descent. BB-{\$}\backslashalpha{\$} can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter {\$}\backslashalpha{\$}, the method is able to interpolate between variational Bayes (VB) ({\$}\backslashalpha \backslashrightarrow 0{\$}) and an algorithm similar to expectation propagation (EP) ({\$}\backslashalpha = 1{\$}). Experiments on probit regression and neural network regression and classification problems show that BB-{\$}\backslashalpha{\$} with non-standard settings of {\$}\backslashalpha{\$}, such as {\$}\backslashalpha = 0.5{\$}, usually produces better predictions than with {\$}\backslashalpha \backslashrightarrow 0{\$} (VB) or {\$}\backslashalpha = 1{\$} (EP).},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1511.03243},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Li, Yingzhen and Rowland, Mark and Hern{\'{a}}ndez-Lobato, Daniel and Bui, Thang and Turner, Richard E.},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {1511.03243},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hern{\'{a}}ndez-Lobato et al. - 2016 - Black-box {\$}alpha{\$}-divergence Minimization.pdf:pdf},
month = {nov},
title = {{Black-box {\$}\backslashalpha{\$}-divergence Minimization}},
url = {http://arxiv.org/abs/1511.03243},
year = {2016}
}
@article{Wu2015,
abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
archivePrefix = {arXiv},
arxivId = {1512.00242},
author = {Wu, Haibing and Gu, Xiaodong},
doi = {10.1016/j.neunet.2015.07.007},
eprint = {1512.00242},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wu, Gu - 2015 - Towards dropout training for convolutional neural networks.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Convolutional neural networks,Deep learning,Max-pooling dropout},
pages = {1--10},
pmid = {26277608},
title = {{Towards dropout training for convolutional neural networks}},
url = {https://arxiv.org/pdf/1512.00242.pdf},
volume = {71},
year = {2015}
}
@article{Miyato2017,
abstract = {Adversarial training provides a means of regularizing supervised learning algo-rithms while virtual adversarial training is able to extend supervised learning al-gorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropri-ate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have im-proved in quality and that while training, the model is less prone to overfitting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1605.07725v3},
author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian J.},
eprint = {arXiv:1605.07725v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Miyato, Dai, Goodfellow - 2017 - ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION.pdf:pdf},
journal = {arXiv preprint},
keywords = {()},
title = {{ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION}},
url = {https://arxiv.org/pdf/1605.07725.pdf},
year = {2017}
}
@article{Tucker2017,
abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, $\backslash$emph{\{}unbiased{\}} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.},
archivePrefix = {arXiv},
arxivId = {1703.07370},
author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and Sohl-Dickstein, Jascha},
eprint = {1703.07370},
file = {:Users/Jakob/Documents/Mendeley Desktop/Tucker et al. - 2017 - REBAR Low-variance, unbiased gradient estimates for discrete latent variable models.pdf:pdf},
month = {mar},
title = {{REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models}},
url = {http://arxiv.org/abs/1703.07370},
year = {2017}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
file = {:Users/Jakob/Documents/Mendeley Desktop/Duan et al. - 2016 - RL{\$}2{\$} Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@article{Kim2014,
abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the ar-chitecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
author = {Kim, Yoon},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kim - 2014 - Convolutional Neural Networks for Sentence Classification.pdf:pdf},
journal = {arXiv preprint},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {https://arxiv.org/pdf/1408.5882.pdf},
year = {2014}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
annote = {Foundational paper of Information Theory.

Introduces concepts such as 
- information measured in bits and nats
- entropy of a probability distribution
- relating information to probabilities and uncertainty},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:Users/Jakob/Documents/Mendeley Desktop/Shannon - 1948 - A Mathematical Theory of Communication.pdf:pdf},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A Mathematical Theory of Communication}},
url = {http://math.harvard.edu/{~}ctm/home/text/others/shannon/entropy/entropy.pdf},
volume = {27},
year = {1948}
}
@article{Jozefowicz2016,
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
journal = {arXiv preprint},
title = {{Exploring the limits of language modeling}},
year = {2016}
}
@article{Osterwalder2010,
author = {Osterwalder, Alexander and Pigneur, Yves},
doi = {10.1108/03684921211261761},
isbn = {978-84-234-2841},
issn = {0368-492X},
journal = {Kybernetes},
number = {5/6},
pages = {823--824},
publisher = {Wiley,},
title = {{Business Model Generation20124Alexander Osterwalder and Yves Pigneur. Business Model Generation . New York, NY: Wiley 2011. 288 pp. {\pounds}23.99, ISBN: 9780470876411}},
url = {http://findit.dtu.dk/en/catalog?utf8=â{\&}locale=en{\&}search{\_}field=all{\_}fields{\&}q=business+model+generation https://doi.org/10.1108/03684921211261761 http://www.emeraldinsight.com/doi/10.1108/03684921211261761},
volume = {41},
year = {2012}
}
@article{Chan1979,
abstract = {A general formula is presented for computing the sample v;iiiancc for a sample of size m+ n given the means and variances for two subsnn+lcs of sizes m and n. This formula is used in the construction of a pa.irwisc nl{\~{}}:orithm for computing the variance. Other applications are discussed as v{\~{}}ll, including the use of updating formulae in a parallel computin g cnviornmcn t. Wc present numerical results and rounding error analyses for several numerical schcr{\~{}}{\~{}}s.},
annote = {Provides an numerically stable algorithm for online estimation of the sample variance through online updates to the squared distance of the sample from the sample mean. Also produces an online estimate of the mean.

This is generalized version of the algorithm developed by Welford, B. P. in 1962 which only considered a single new example. This method considers the case of combining two mean estimates of any number of samples.

Chan's method for estimating the mean is numerically unstable when {\{}$\backslash$displaystyle n{\_}{\{}A{\}}$\backslash$approx n{\_}{\{}B{\}}{\}} n{\_}A $\backslash$approx n{\_}B and both are large, because the numerical error in {\{}$\backslash$displaystyle {\{}$\backslash$bar {\{}x{\}}{\}}{\_}{\{}B{\}}-{\{}$\backslash$bar {\{}x{\}}{\}}{\_}{\{}A{\}}{\}} $\backslash$bar x{\_}B - $\backslash$bar x{\_}A is not scaled down in the way that it is in the {\{}$\backslash$displaystyle n{\_}{\{}B{\}}=1{\}} n{\_}B = 1 case. In such cases, prefer {\{}$\backslash$displaystyle {\{}$\backslash$bar {\{}x{\}}{\}}{\_}{\{}X{\}}={\{}$\backslash$frac {\{}n{\_}{\{}A{\}}{\{}$\backslash$bar {\{}x{\}}{\}}{\_}{\{}A{\}}+n{\_}{\{}B{\}}{\{}$\backslash$bar {\{}x{\}}{\}}{\_}{\{}B{\}}{\}}{\{}n{\_}{\{}A{\}}+n{\_}{\{}B{\}}{\}}{\}}{\}} $\backslash$bar x{\_}X = $\backslash$frac{\{}n{\_}A $\backslash$bar x{\_}A + n{\_}B $\backslash$bar x{\_}B{\}}{\{}n{\_}A + n{\_}B{\}}.},
author = {Chan, T. F. and Golub, G. H. and LeVeque, R. J.},
doi = {10.1007/978-3-642-51461-6_3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Chan, Golub, LeVeque - 1979 - Updating formulae and a pairwise algorithm for computing sample variances.pdf:pdf},
isbn = {3705100025},
journal = {Annals of Physics},
pages = {258},
title = {{Updating formulae and a pairwise algorithm for computing sample variances}},
url = {http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Updating+Formulae+and+pairwise+algorithm+for+computing+sample+variances{\#}0},
volume = {54},
year = {1979}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal âhidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating errors.pdf:pdf},
journal = {Nature},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@inproceedings{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
address = {Long Beach, CA, USA},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
eprint = {1710.09829},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules.pdf:pdf},
month = {oct},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{Iandola2016,
author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
journal = {arXiv preprint},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and{\textless} 0.5 MB model size}},
year = {2016}
}
@inproceedings{Scherer2010,
abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57{\%} on the NORB normalized-uniform dataset and 5.6{\%} on the NORB jittered-cluttered dataset.},
author = {Scherer, Dominik and M{\"{u}}ller, Andreas and Behnke, Sven},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15825-4_10},
file = {:Users/Jakob/Documents/Mendeley Desktop/Scherer, M{\"{u}}ller, Behnke - 2010 - Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition.pdf:pdf},
isbn = {3642158242},
issn = {03029743},
number = {PART 3},
pages = {92--101},
title = {{Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition}},
url = {http://www.ais.uni-bonn.de},
volume = {6354 LNCS},
year = {2010}
}
@article{Wingate2013,
abstract = {We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.},
archivePrefix = {arXiv},
arxivId = {1301.1299},
author = {Wingate, David and Weber, Theophane},
eprint = {1301.1299},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wingate, Weber - 2013 - Automated Variational Inference in Probabilistic Programming.pdf:pdf},
journal = {arXiv preprint},
title = {{Automated Variational Inference in Probabilistic Programming}},
url = {http://arxiv.org/abs/1301.1299},
year = {2013}
}
@article{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {1506.03340},
author = {Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
doi = {10.1109/72.410363},
eprint = {1506.03340},
isbn = {1045-9227},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {1693--1701},
pmid = {18263409},
title = {{Teaching Machines to Read and Comprehend}},
url = {http://arxiv.org/abs/1506.03340},
year = {2015}
}
@article{Zhou2016,
abstract = {Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically.},
archivePrefix = {arXiv},
arxivId = {1603.09420},
author = {Zhou, Guo Bing and Wu, Jianxin and Zhang, Chen Lin and Zhou, Zhi-Hua},
doi = {10.1007/s11633-016-1006-2},
eprint = {1603.09420},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhou et al. - 2016 - Minimal gated unit for recurrent neural networks.pdf:pdf},
issn = {17518520},
journal = {International Journal of Automation and Computing},
keywords = {Recurrent neural network,deep learning,gate recurrent unit (GRU),gated unit,long short-term memory (LSTM),minimal gated unit (MGU)},
month = {mar},
number = {3},
pages = {226--234},
title = {{Minimal gated unit for recurrent neural networks}},
url = {http://arxiv.org/abs/1603.09420},
volume = {13},
year = {2016}
}
@article{HonglakLeeAlexisBattleRajatRaina2006,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap- ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimiza- tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur- round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
doi = {10.1.1.69.2112},
eprint = {arXiv:1506.03733v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lee et al. - 2006 - Efficient Sparse coding algorithms.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
keywords = {Stanford University},
number = {2},
pages = {801--808},
pmid = {17051527},
title = {{Efficient Sparse coding algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.2112{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://books.nips.cc/papers/txt/nips19/NIPS2006{\_}0878.txt},
volume = {19},
year = {2006}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kaelbling, Littman, Moore - 1996 - Reinforcement learning A Survey.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {237--285},
title = {{Reinforcement learning: A Survey}},
volume = {4},
year = {1996}
}
@incollection{Rognvaldsson1993,
address = {London},
author = {R{\"{o}}gnvaldsson, Thorsteinn S.},
booktitle = {ICANN '93},
doi = {10.1007/978-1-4471-2063-6_147},
pages = {527--532},
publisher = {Springer London},
title = {{Brownian Motion Updating of Multi-layered Perceptrons}},
url = {http://link.springer.com/10.1007/978-1-4471-2063-6{\_}147},
year = {1993}
}
@article{Conti2017,
abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
archivePrefix = {arXiv},
arxivId = {1712.06560},
author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1712.06560},
file = {:Users/Jakob/Documents/Mendeley Desktop/Conti et al. - 2017 - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents}},
url = {http://arxiv.org/abs/1712.06560},
year = {2017}
}
@inproceedings{Amdahl1967,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
author = {Amdahl, Gene M.},
booktitle = {Proceedings of the Spring Joint Computer Conference (AFIPS)},
doi = {10.1145/1465482.1465560},
file = {:Users/Jakob/Documents/Mendeley Desktop/Amdahl - 1967 - Validity of the single processor approach to achieving large scale computing capabilities.pdf:pdf},
pages = {483},
title = {{Validity of the single processor approach to achieving large scale computing capabilities}},
url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
year = {1967}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lin, Chen, Yan - 2013 - Network In Network.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
pages = {10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Bradbury2016,
abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01576},
author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
eprint = {1611.01576},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bradbury et al. - 2016 - Quasi-Recurrent Neural Networks.pdf:pdf},
isbn = {080581955X},
title = {{Quasi-Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1611.01576},
year = {2016}
}
@article{Schaul2013,
abstract = {Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.},
archivePrefix = {arXiv},
arxivId = {1312.6055},
author = {Schaul, Tom and Antonoglou, Ioannis and Silver, David},
doi = {10.1007/978-3-642-35289-8-3},
eprint = {1312.6055},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schaul, Antonoglou, Silver - 2013 - Unit Tests for Stochastic Optimization.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {arXiv preprint},
pmid = {15003161},
title = {{Unit Tests for Stochastic Optimization}},
url = {http://arxiv.org/abs/1312.6055},
year = {2013}
}
@article{Zhang2017,
abstract = {Because stochastic gradient descent (SGD) has shown promise optimizing neural networks with millions of parameters and few if any alternatives are known to exist, it has moved to the heart of leading approaches to reinforcement learning (RL). For that reason, the recent result from OpenAI showing that a particular kind of evolution strategy (ES) can rival the performance of SGD-based deep RL methods with large neural networks provoked surprise. This result is difficult to interpret in part because of the lingering ambiguity on how ES actually relates to SGD. The aim of this paper is to significantly reduce this ambiguity through a series of MNIST-based experiments designed to uncover their relationship. As a simple supervised problem without domain noise (unlike in most RL), MNIST makes it possible (1) to measure the correlation between gradients computed by ES and SGD and (2) then to develop an SGD-based proxy that accurately predicts the performance of different ES population sizes. These innovations give a new level of insight into the real capabilities of ES, and lead also to some unconventional means for applying ES to supervised problems that shed further light on its differences from SGD. Incorporating these lessons, the paper concludes by demonstrating that ES can achieve 99{\%} accuracy on MNIST, a number higher than any previously published result for any evolutionary method. While not by any means suggesting that ES should substitute for SGD in supervised learning, the suite of experiments herein enables more informed decisions on the application of ES within RL and other paradigms.},
archivePrefix = {arXiv},
arxivId = {1712.06564},
author = {Zhang, Xingwen and Clune, Jeff and Stanley, Kenneth O.},
eprint = {1712.06564},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhang, Clune, Stanley - 2017 - On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent.pdf:pdf},
journal = {arXiv preprint},
title = {{On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent}},
url = {http://arxiv.org/abs/1712.06564},
year = {2017}
}
@article{Oord2016,
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
journal = {arXiv preprint},
title = {{Pixel recurrent neural networks}},
year = {2016}
}
@article{Schulman,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
eprint = {1506.02438},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schulman et al. - 2015 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:pdf},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@techreport{Lighthill1973,
abstract = {Lighthill's report was commissioned by the Science Research Council (SRC) to give an unbiased view of the state of AI research primarily in the UK in 1973. The two main research groups were at Sussex and Edinburgh. There was pressure from Edinburgh to buy a US machine, the Digital Equipment Corporation DEC10 which was used by most US researchers. AI research was funded by the Engineering Board of SRC as part of its Computer Science funding. The Lighthill Report was published early in 1973. Although it supported AI research related to automation and to computer simulation of neurophysiological and psychological processes, it was highly critical of basic research in the foundation areas such as robotics and language processing. Lighthill's report provoked a massive loss of confidence in AI by the academic establishment in the UK including the funding body. It persisted for almost a decade. AI research continued but the next attempt to mount a major activity in the area did not come until the September 1982 Research Area Review Meeting on Intelligent Knowledge-Based Systems. The findings of which were accepted by SERC (Science and Engineering Research Council, a change of name) and became the IKBS part of the Alvey Programme.},
address = {London, United Kingdom},
author = {Lighthill, James},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lighthill - 1973 - Lighthill Report Artificial Intelligence A Paper Symposium.pdf:pdf},
institution = {Science Research Council},
title = {{Lighthill Report: Artificial Intelligence: A Paper Symposium}},
url = {http://www.chilton-computing.org.uk/inf/literature/reports/lighthill{\_}report/contents.htm{\%}5Cnhttp://www.aiai.ed.ac.uk/events/lighthill1973/.},
year = {1973}
}
@article{Katharopoulos2017,
abstract = {Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel. This method allows in particular to utilize a biased gradient estimate that implicitly optimizes a soft max-loss, and leads to better generalization performance. While such method suffers from a prohibitively high variance of the gradient estimate when using a standard stochastic optimizer, we show that when it is combined with our sampling mechanism, it results in a reliable procedure. We showcase the generality of our method by testing it on both image classification and language modeling tasks using deep convolutional and recurrent neural networks. In particular, our method results in 30{\%} faster training of a CNN for CIFAR10 than when using uniform sampling.},
archivePrefix = {arXiv},
arxivId = {1706.00043},
author = {Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
eprint = {1706.00043},
file = {:Users/Jakob/Documents/Mendeley Desktop/Katharopoulos, Fleuret - 2017 - Biased Importance Sampling for Deep Neural Network Training.pdf:pdf},
journal = {arXiv preprint},
month = {may},
title = {{Biased Importance Sampling for Deep Neural Network Training}},
url = {http://arxiv.org/abs/1706.00043},
year = {2017}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{Coles2018,
abstract = {As quantum computers have become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classic computer programs for most of their career. While currently available quantum computers have less than 100 qubits, quantum computer hardware is widely expected to grow in terms of qubit counts, quality, and connectivity. Our article aims to explain the principles of quantum programming, which are quite different from classical programming, with straight-forward algebra that makes understanding the underlying quantum mechanics optional (but still fascinating). We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succintc and self-contained fashion; we show how they are implemented on IBM's quantum computer; and in each case we discuss the results of the implementation with respect to differences of the simulator and the actual hardware runs. This article introduces computer scientists and engineers to quantum algorithms and provides a blueprint for their implementations.},
archivePrefix = {arXiv},
arxivId = {1804.03719},
author = {Coles, Patrick J. and Eidenbenz, Stephan and Pakin, Scott and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Lokhov, Andrey and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O'Malley, Dan and Oyen, Diane and Prasad, Lakshman and Roberts, Randy and Romero, Phil and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter and Vuffray, Marc and Wendelberger, Jim and Yoon, Boram and Zamora, Richard and Zhu, Wei},
eprint = {1804.03719},
file = {:Users/Jakob/Documents/Mendeley Desktop/Coles et al. - 2018 - Quantum Algorithm Implementations for Beginners.pdf:pdf},
journal = {arXiv preprint},
title = {{Quantum Algorithm Implementations for Beginners}},
url = {http://arxiv.org/abs/1804.03719},
year = {2018}
}
@inproceedings{Schaul2011,
abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization. NES follows the natural gradient of the expected fitness on the parameters of its search distribution. While general in its formulation, previous research has focused on multivariate Gaussian search distributions. Here we exhibit problem classes for which other search distributions are more appropriate, and then derive corresponding NES-variants. First, for separable distributions we obtain SNES, whose complexity is only O(d) instead of O(d(3)). We apply SNES to problems of previously unattainable dimensionality, recovering lowest-energy structures on the Lennard-Jones atom clusters, and obtaining state-of-the-art results on neuro-evolution benchmarks. Second, we develop a new, equivalent formulation based on invariances. This allows for generalizing NES to heavy-tailed distributions, even those with undefined variance, which aids in overcoming deceptive local optima.},
author = {Schaul, Tom and Glasmachers, Tobias and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
doi = {10.1145/2001576.2001692},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schaul, Glasmachers, Schmidhuber - 2011 - High Dimensions and Heavy Tails for Natural Evolution Strategies.pdf:pdf},
isbn = {9781450305570},
keywords = {black-box optimization,global optimization,natural gradient},
title = {{High Dimensions and Heavy Tails for Natural Evolution Strategies}},
url = {http://people.idsia.ch/{~}juergen/gecco2011snes.pdf},
year = {2011}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:Users/Jakob/Documents/Mendeley Desktop/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://proceedings.mlr.press/v32/silver14.pdf},
year = {2014}
}
@article{Simonyan2014a,
author = {Simonyan, Karen and Zisserman, Andrew},
journal = {arXiv preprint},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2014}
}
@incollection{Schurer2004,
abstract = {Quasi-Monte Carlo (QMC) routines are one of the most common tech-niques for solving integration problems in high dimensions. However, their efficiency degrades if the variation of the integrand is concentrated in small areas of the inte-gration domain. Adaptive algorithms cope with this situation by adjusting the flow of computation based on previous integrand evaluations. We explore ways to modify the Monte Carlo based adaptive algorithms MISER and VEGAS such that low-discrepancy point sets are used instead of random sam-ples. Experimental results show that the proposed algorithms outperform plain QMC as well as the original adaptive integration routine for certain classes of test cases.},
author = {Sch{\"{u}}rer, Rudolf},
booktitle = {Monte Carlo and Quasi-Monte Carlo Methods 2002},
doi = {10.1007/978-3-642-18743-8_25},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sch{\"{u}}rer - 2004 - Adaptive Quasi-Monte Carlo Integration Based on MISER and VEGAS.pdf:pdf},
isbn = {978-3-540-20466-4},
keywords = {MISER,VEGAS},
pages = {393--406 TS -- CrossRef},
title = {{Adaptive Quasi-Monte Carlo Integration Based on MISER and VEGAS}},
url = {https://pdfs.semanticscholar.org/7a10/632726eaaf031025f2cc77008264abd9b5fe.pdf},
year = {2004}
}
@article{Jeffreys1946,
abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
author = {Jeffreys, H.},
doi = {10.1098/rspa.1946.0056},
file = {:Users/Jakob/Documents/Mendeley Desktop/Jeffreys - 1946 - An Invariant Form for the Prior Probability in Estimation Problems.pdf:pdf},
isbn = {1364-5021},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
number = {1007},
pages = {453--461},
pmid = {20998741},
title = {{An Invariant Form for the Prior Probability in Estimation Problems}},
url = {http://rspa.royalsocietypublishing.org/content/royprsa/186/1007/453.full.pdf},
volume = {186},
year = {1946}
}
@inproceedings{Sordoni2015,
abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
archivePrefix = {arXiv},
arxivId = {1506.06714},
author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
booktitle = {Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
eprint = {1506.06714},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sordoni et al. - 2015 - A Neural Network Approach to Context-Sensitive Generation of Conversational Responses.pdf:pdf},
isbn = {9781941643495},
month = {jun},
title = {{A Neural Network Approach to Context-Sensitive Generation of Conversational Responses}},
url = {https://arxiv.org/abs/1506.06714},
year = {2015}
}
@article{Salimans2017,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
annote = {Main inspiration for my Master's Thesis},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
eprint = {1703.03864},
file = {:Users/Jakob/Documents/Mendeley Desktop/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.03864},
year = {2017}
}
@article{McAuley2015a,
author = {McAuley, Julian},
title = {{Inferring Networks of Substitutable and Complementary Products}},
year = {2015}
}
@article{LeCun1989,
abstract = {Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.},
author = {LeCun, Yann A. and Jackel, L. D. and Boser, B. and Denker, J. S. and Graf, H. P. and Guyon, I. and Henderson, D. and Howard, R. E. and Hubbard, W.},
doi = {10.1109/35.41400},
file = {:Users/Jakob/Documents/Mendeley Desktop/LeCun et al. - 1989 - Handwritten Digit Recognition Applications of Neural Network Chips and Automatic Learning.pdf:pdf},
journal = {IEEE Communications Magazine},
number = {11},
pages = {41--46},
title = {{Handwritten Digit Recognition: Applications of Neural Network Chips and Automatic Learning}},
url = {http://ieeexplore.ieee.org/document/41400/},
volume = {27},
year = {1989}
}
@inproceedings{Nguyen2015a,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
issn = {10636919},
pages = {427--436},
pmid = {24309266},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
volume = {07-12-June},
year = {2015}
}
@article{Burda2015,
abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
address = {Toronto},
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan R.},
eprint = {1509.00519},
file = {:Users/Jakob/Documents/Mendeley Desktop/Burda, Grosse, Salakhutdinov - 2015 - Importance Weighted Autoencoders.pdf:pdf},
institution = {University of Toronto},
isbn = {1509.00519},
issn = {1312.6114v10},
pages = {8},
title = {{Importance Weighted Autoencoders}},
url = {https://arxiv.org/abs/1509.00519 http://arxiv.org/abs/1509.00519},
year = {2015}
}
@inproceedings{Zhou2017,
abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1702.08835},
author = {Zhou, Zhi Hua and Feng, Ji},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/497},
eprint = {1702.08835},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhou, Feng - 2017 - Deep forest Towards an Alternative to Deep Neural Networks.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
keywords = {Machine Learning: Deep Learning,Machine Learning: Ensemble Methods},
pages = {3553--3559},
title = {{Deep forest: Towards an Alternative to Deep Neural Networks}},
year = {2017}
}
@article{Karpathy2015a,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1506.02078},
file = {:Users/Jakob/Documents/Mendeley Desktop/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pmid = {26353135},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2015}
}
@article{Dahl2012,
author = {Dahl, George E. and Yu, Dong and Deng, Li and Acero, Alex},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {30--42},
publisher = {IEEE},
title = {{Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition}},
volume = {20},
year = {2012}
}
@article{Makhzani2015,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
eprint = {1511.05644},
file = {:Users/Jakob/Documents/Mendeley Desktop/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:pdf},
isbn = {9781509008063},
issn = {9781509008063},
journal = {arXiv preprint},
month = {nov},
title = {{Adversarial Autoencoders}},
url = {http://arxiv.org/abs/1511.05644},
year = {2015}
}
@inproceedings{Yi2009,
abstract = {To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alter-native to standard stochastic search meth-ods. It maintains a multinormal distribution on the set of solution candidates. The Nat-ural Gradient is used to update the distribu-tion's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information ma-trix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness eval-uations. The algorithm yields competitive re-sults on a number of benchmarks.},
address = {Montreal, Quebec, Canada},
annote = {Introduces effecient natural evolution strategies (sNES) along with optimal fitness baselines and importance mixing.

Companion paper to "Efficient Natural Evolution Strategies" which presented the theory while here experiments showing the usefullness of importance mixing and fitness baselines were made. 

Importance mixing reduces the number of required fitness evaluations by a factor of 5.

Used NES and importance mixing to learn pole balancing task (similar to CartPole) using recurrent neural network (RNN) but the network had only 21 weights.},
author = {Yi, Sun and Wierstra, Daan and Schaul, Tom and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1145/1553374.1553522},
file = {:Users/Jakob/Documents/Mendeley Desktop/Yi et al. - 2009 - Stochastic Search using the Natural Gradient.pdf:pdf},
isbn = {9781605585161},
keywords = {evolution strategies,natural gradient,stochastic search},
pages = {1161--1168},
title = {{Stochastic Search using the Natural Gradient}},
url = {http://people.idsia.ch/{~}daan/papers/icml2009.pdf},
year = {2009}
}
@article{Bahdanau2016,
abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
archivePrefix = {arXiv},
arxivId = {1508.04395},
author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
eprint = {1508.04395},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
month = {aug},
title = {{End-to-End Attention-based Large Vocabulary Speech Recognition}},
url = {http://arxiv.org/abs/1508.04395},
year = {2015}
}
@article{Paulus2017,
abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit "exposure bias" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.},
archivePrefix = {arXiv},
arxivId = {1705.04304},
author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
doi = {10.1051/0004-6361/201527329},
eprint = {1705.04304},
file = {:Users/Jakob/Documents/Mendeley Desktop/Paulus, Xiong, Socher - 2017 - A Deep Reinforced Model for Abstractive Summarization.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint},
pmid = {23459267},
title = {{A Deep Reinforced Model for Abstractive Summarization}},
url = {http://arxiv.org/abs/1705.04304},
year = {2017}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6â8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9â11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com.proxy.findit.dtu.dk/nature/journal/v518/n7540/pdf/nature14236.pdf},
volume = {518},
year = {2015}
}
@techreport{Rosenblatt1957,
annote = {First publication about the perceptron},
author = {Rosenblatt, Frank},
doi = {85-460-1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Rosenblatt - 1957 - The Perceptron - A Perceiving and Recognizing Automaton.pdf:pdf},
institution = {Cornell Aeronautical Laboratory},
pages = {Report 85--460--1},
title = {{The Perceptron - A Perceiving and Recognizing Automaton}},
year = {1957}
}
@inproceedings{Ranzato2007,
abstract = {We present an unsupervised method for learning a hi-erarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extrac-tor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each fil-ter output within adjacent windows, and a point-wise sig-moid non-linearity. A second level of larger and more in-variant features is obtained by training the same algorithm on patches of features from the first level. Training a su-pervised classifier on these features yields 0.64{\%} error on MNIST, and 54{\%} average recognition rate on Caltech 101 with 30 training samples per category. While the result-ing architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely super-vised learning procedures, and yields good performance with very few labeled training samples.},
author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann A.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
doi = {10.1109/CVPR.2007.383157},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ranzato et al. - 2007 - Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
month = {jun},
pages = {1--8},
pmid = {4270182},
title = {{Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition}},
url = {http://ieeexplore.ieee.org/document/4270182/},
year = {2007}
}
@inproceedings{Li2018a,
abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1804.08838},
author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1804.08838},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li et al. - 2018 - Measuring the Intrinsic Dimension of Objective Landscapes.pdf:pdf},
title = {{Measuring the Intrinsic Dimension of Objective Landscapes}},
url = {http://arxiv.org/abs/1804.08838},
year = {2018}
}
@inproceedings{Weston2014,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {1410.3916},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
doi = {v0},
eprint = {1410.3916},
file = {:Users/Jakob/Documents/Mendeley Desktop/Weston, Chopra, Bordes - 2014 - Memory Networks.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
keywords = {Memory Networks,Natural Language Processing,Question answering},
mendeley-tags = {Memory Networks,Natural Language Processing,Question answering},
pmid = {9377276},
title = {{Memory Networks}},
url = {http://arxiv.org/abs/1410.3916},
year = {2014}
}
@article{Melis2017,
abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
archivePrefix = {arXiv},
arxivId = {1707.05589},
author = {Melis, G{\'{a}}bor and Dyer, Chris and Blunsom, Phil},
eprint = {1707.05589},
file = {:Users/Jakob/Documents/Mendeley Desktop/Melis, Dyer, Blunsom - 2017 - On the State of the Art of Evaluation in Neural Language Models.pdf:pdf},
isbn = {9781604562170},
journal = {arXiv preprint},
month = {jul},
title = {{On the State of the Art of Evaluation in Neural Language Models}},
url = {http://arxiv.org/abs/1707.05589},
year = {2017}
}
@article{Molchanov2017,
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
eprint = {1701.05369},
file = {:Users/Jakob/Documents/Mendeley Desktop/Molchanov, Ashukha, Vetrov - 2017 - Variational Dropout Sparsifies Deep Neural Networks.pdf:pdf},
issn = {1938-7228},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
url = {https://arxiv.org/pdf/1701.05369.pdf http://arxiv.org/abs/1701.05369},
year = {2017}
}
@article{Campbell2002,
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: â¢a single-chip chess search engine,â¢a massively parallel system with multiple levels of parallelism,â¢a strong emphasis on search extensions,â¢a complex evaluation function, andâ¢effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
author = {Campbell, Murray and {Hoane Jr.}, A. Joseph and Hsu, Feng-hsiung},
doi = {10.1016/S0004-3702(01)00129-1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Campbell, Hoane Jr., Hsu - 2002 - Deep Blue.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {computer chess,evaluation,game tree search,parallel search,search extensions,selective search},
number = {1-2},
pages = {57--83},
title = {{Deep Blue}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1C7AC0F9698C1F48E94AEBBB256EB2FD?doi=10.1.1.99.2714{\&}rep=rep1{\&}type=pdf http://www.sciencedirect.com/science/article/pii/S0004370201001291},
volume = {134},
year = {2002}
}
@inproceedings{Zhao2014,
abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Gradient Descent (prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization with importance sampling, which improves the convergence rate by reducing the stochastic variance. Specifically, we study prox-SGD (actually, stochastic mirror descent) with importance sampling and prox-SDCA with importance sampling. For prox-SGD, instead of adopting uniform sampling throughout the training process, the proposed algorithm employs importance sampling to minimize the variance of the stochastic gradient. For prox-SDCA, the proposed importance sampling scheme aims to achieve higher expected dual value at each dual coordinate ascent step. We provide extensive theoretical analysis to show that the convergence rates with the proposed importance sampling methods can be significantly improved under suitable conditions both for prox-SGD and for prox-SDCA. Experiments are provided to verify the theoretical analysis.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1401.2753},
author = {Zhao, Peilin and Zhang, Tong},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {1401.2753},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhao, Zhang - 2014 - Stochastic Optimization with Importance Sampling.pdf:pdf},
title = {{Stochastic Optimization with Importance Sampling}},
url = {http://arxiv.org/abs/1401.2753},
year = {2014}
}
@article{Yao2015,
abstract = {In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.},
annote = {Extension of LSTMs to include a depth gate that connects memory cells of adjacent layers},
archivePrefix = {arXiv},
arxivId = {1508.03790},
author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
eprint = {1508.03790},
file = {:Users/Jakob/Documents/Mendeley Desktop/Yao et al. - 2015 - Depth-Gated Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Depth-Gated Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1508.03790},
year = {2015}
}
@article{Aytar2018,
abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.},
archivePrefix = {arXiv},
arxivId = {1805.11592},
author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and de Freitas, Nando},
eprint = {1805.11592},
file = {:Users/Jakob/Documents/Mendeley Desktop/Aytar et al. - 2018 - Playing hard exploration games by watching YouTube.pdf:pdf},
month = {may},
title = {{Playing hard exploration games by watching YouTube}},
url = {http://arxiv.org/abs/1805.11592},
year = {2018}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
doi = {10.1109/IJCNN.2016.7727519},
eprint = {1411.1792},
isbn = {978-1-5090-0620-5},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
year = {2014}
}
@article{VanHasselt2015,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
doi = {10.1016/j.artint.2015.09.002},
eprint = {1509.06461},
file = {:Users/Jakob/Documents/Mendeley Desktop/van Hasselt, Guez, Silver - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
month = {sep},
pmid = {26150344},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461},
year = {2015}
}
@article{Sugiyama2010,
abstract = {The ratio of two probability density functions is becoming a quantity of interest these days in the machine learning and data mining communities since it can be used for various data processing tasks such as non-stationarity adaptation, outlier detection, and feature selection. Recently, several methods have been developed for directly estimating the density ratio without going through density estimation and were shown to work well in various practical problems. However, these methods still perform rather poorly when the dimensionality of the data domain is high. In this paper, we propose to incorporate a dimensionality reduction scheme into a density-ratio estimation procedure and experimentally show that the estimation accuracy in high-dimensional cases can be improved. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Sugiyama, Masashi and Kawanabe, Motoaki and Chui, Pui Ling},
doi = {10.1016/j.neunet.2009.07.007},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sugiyama, Kawanabe, Chui - 2010 - Dimensionality Reduction for Density Ratio Estimation in High-dimensional Spaces.pdf:pdf},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
keywords = {Density ratio estimation,Dimensionality reduction,Local Fisher discriminant analysis,Unconstrained least-squares importance fitting},
number = {1},
pages = {44--59},
pmid = {19631506},
title = {{Dimensionality Reduction for Density Ratio Estimation in High-dimensional Spaces}},
url = {https://pdfs.semanticscholar.org/c7b6/9a3927a768e25f6caa3848dea24af8471401.pdf},
volume = {23},
year = {2010}
}
@article{Li2018,
abstract = {This paper first answers the question "why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as "variance shift") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks.},
archivePrefix = {arXiv},
arxivId = {1801.05134},
author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
eprint = {1801.05134},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li et al. - 2018 - Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift.pdf:pdf},
journal = {arXiv preprint},
title = {{Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift}},
url = {http://arxiv.org/abs/1801.05134},
year = {2018}
}
@article{Denker1991,
abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "soft max" scheme.},
archivePrefix = {arXiv},
arxivId = {11359-901120-05},
author = {Denker, John S. and LeCun, Yann A.},
doi = {10.1.1.32.7096},
eprint = {11359-901120-05},
file = {:Users/Jakob/Documents/Mendeley Desktop/Denker, LeCun - 1991 - Transforming Neural-Net Output Levels to Probability Distributions.pdf:pdf},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {853--859},
title = {{Transforming Neural-Net Output Levels to Probability Distributions}},
url = {http://papers.nips.cc/paper/419-transforming-neural-net-output-levels-to-probability-distributions.pdf{\%}5Cnfiles/4250/Denker ? LeCun - 1991 - Transforming Neural-Net Output Levels to Probabili.pdf{\%}5Cnfiles/4251/419-transforming-neural-net-output-levels-to-},
year = {1991}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
eprint = {1502.03167},
file = {:Users/Jakob/Documents/Mendeley Desktop/Glorot, Bordes, Bengio - 2011 - Deep sparse rectifier neural networks.pdf:pdf},
journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {315--323},
title = {{Deep sparse rectifier neural networks}},
url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
volume = {15},
year = {2011}
}
@article{Luong2015,
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
journal = {arXiv preprint},
title = {{Effective approaches to attention-based neural machine translation}},
year = {2015}
}
@inproceedings{Hinton1993,
abstract = {Supervised neural networks generalize there is much less information learning, well if in the weights than there is in the output vectors of the train- ing cases. So during it is impor- tant to keep the weights simple by penaliz- ing the amount of information The amount of information be controlled they contain. in a weight can by adding Gaussian noise and the noise level can be adapted during learning to optimize a method of computing the trade-off between the expected squared error of the network and the amount of information in the noisy weights can be computed is required in the weights. We describe the derivatives work that contains a layer of non-linear time-consuming that of the expected squared error and of the amount of information in a net- hidden units. Provided the output units are linear, the exact derivatives without efficiently tions. The idea of minimizing information Monte Carlo simula- the amount of to communicate the weights of a neural network leads to a number of intereating schemes for encoding the weights.},
annote = {Derives a diagonal Gaussian variational approximation to the Bayesian network weights but couched in a minimum description length information theory language.},
author = {Hinton, Geoffrey E. and van Camp, Drew},
booktitle = {Proceedings of Annual Conference on Computational Learning Theory (COLT)},
doi = {10.1145/168304.168306},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hinton, van Camp - 1993 - Keeping the neural networks simple by minimizing the description length of the weights.pdf:pdf},
isbn = {0897916115},
pages = {5--13},
title = {{Keeping the neural networks simple by minimizing the description length of the weights}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/colt93.pdf http://portal.acm.org/citation.cfm?doid=168304.168306},
year = {1993}
}
@inproceedings{Dai2017,
abstract = {Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few ({\~{}}2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperforms the CNN with 3 weight layers by over 15{\%} in absolute accuracy for an environmental sound recognition task and is competitive with the performance of models using log-mel features.},
archivePrefix = {arXiv},
arxivId = {1610.00087},
author = {Dai, W and Dai, Chia and Qu, Shuhui and Li, Juncheng and Das, Samarjit},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7952190},
eprint = {1610.00087},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dai et al. - 2017 - Very deep convolutional neural networks for raw waveforms.pdf:pdf},
isbn = {VO -},
issn = {19909772},
keywords = {Acoustic Modeling,Acoustics,Biological system modeling,CNN,Convolution,Convolutional Neural Networks,Environmental Sound,Neural Networks,Neural networks,Raw Waveform,Speech recognition,Time-domain analysis,Training,acoustic models,acoustic signal processing,acoustic waveforms,acoustic waves,band-pass filters,bandpass filters,batch normalization,convolution,convolutional layers,down-sampling,environmental sound recognition,high-level discriminative features,log-mel features,model capacity,neural nets,raw waveforms,receptive fields,representation learning,residual learning,time-domain analysis,time-domain waveforms,very deep convolutional neural networks,waveform analysis},
pages = {421--425},
title = {{Very deep convolutional neural networks for raw waveforms}},
url = {https://arxiv.org/pdf/1610.00087.pdf},
year = {2017}
}
@article{Mania2018,
abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classi-cal problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free meth-ods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparam-eter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
archivePrefix = {arXiv},
arxivId = {1803.07055},
author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
eprint = {1803.07055},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mania, Guy, Recht - 2018 - Simple random search provides a competitive approach to reinforcement learning.pdf:pdf},
title = {{Simple random search provides a competitive approach to reinforcement learning}},
url = {https://arxiv.org/abs/1803.07055},
year = {2018}
}
@book{Murphy2012,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
author = {Murphy, Kevin P.},
edition = {1},
isbn = {9780262018029},
issn = {0262018020},
pmid = {20236947},
publisher = {MIT Press},
title = {{Machine Learning}},
url = {http://www.springerreference.com/index/doi/10.1007/SpringerReference{\_}35834},
year = {2012}
}
@article{Borsos2018,
abstract = {Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform importance sampling techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling datapoints, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.},
archivePrefix = {arXiv},
arxivId = {1802.04715},
author = {Borsos, Zal{\'{a}}n and Krause, Andreas and Levy, Kfir Y.},
eprint = {1802.04715},
file = {:Users/Jakob/Documents/Mendeley Desktop/Borsos, Krause, Levy - 2018 - Online Variance Reduction for Stochastic Optimization.pdf:pdf},
title = {{Online Variance Reduction for Stochastic Optimization}},
url = {https://arxiv.org/pdf/1802.04715.pdf http://arxiv.org/abs/1802.04715},
year = {2018}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main componentsâa momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:pdf},
journal = {International Conference on Learning Representations (ICLR) Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ},
year = {2016}
}
@article{Amodei2015,
author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg},
journal = {arXiv preprint},
title = {{Deep speech 2: End-to-end speech recognition in english and mandarin}},
year = {2015}
}
@misc{Scholkopf2015,
abstract = {An artificial-intelligence system uses machine learning from massive training sets to teach itself to play 49 classic computer games, demonstrating that it can adapt to a variety of tasks. See Letter p.529},
author = {Sch{\"{o}}lkopf, Bernhard},
booktitle = {Nature},
doi = {10.1038/518486a},
isbn = {doi:10.1038/518486a},
issn = {14764687},
number = {7540},
pages = {486--487},
pmid = {25719660},
title = {{Artificial intelligence: Learning to see and act}},
volume = {518},
year = {2015}
}
@inproceedings{Hertel2015,
abstract = {Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-the-art approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC2012 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 {\%} on CIFAR-100, compared to the previous state-of-the-art result of 65.43 {\%}. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.},
archivePrefix = {arXiv},
arxivId = {1710.02286},
author = {Hertel, Lars and Barth, Erhardt and Kaster, Thomas and Martinetz, Thomas},
booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2015.7280683},
eprint = {1710.02286},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hertel et al. - 2015 - Deep convolutional neural networks as generic feature extractors.pdf:pdf},
isbn = {9781479919604},
issn = {2161-4393},
keywords = {Art,Convolution,Handwriting recognition,Kernel},
title = {{Deep convolutional neural networks as generic feature extractors}},
url = {http://www.isip.uni-luebeck.de/fileadmin/uploads/tx{\_}wapublications/hertel{\_}ijcnn{\_}2015.pdf},
volume = {2015-Septe},
year = {2015}
}
@article{Buehrer,
author = {Buehrer, Daniel J.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Buehrer - Unknown - A Mathematical Framework for Superintelligent Machines.pdf:pdf},
title = {{A Mathematical Framework for Superintelligent Machines}}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
annote = {McCulloch-Pitts neuron},
author = {McCulloch, Warren S. and Pitts, W.},
doi = {10.1007/BF02478259},
file = {:Users/Jakob/Documents/Mendeley Desktop/McCulloch, Pitts - 1943 - A Logical Calculus of the Idea Immanent in Nervous Activity.pdf:pdf},
issn = {0007-4985},
journal = {Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/{~}coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@article{Graves2014a,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
file = {:Users/Jakob/Documents/Mendeley Desktop/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
isbn = {0028-0836},
issn = {2041-1723},
journal = {arXiv preprint},
month = {oct},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Ly2017,
abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
archivePrefix = {arXiv},
arxivId = {1705.01064},
author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
eprint = {1705.01064},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ly et al. - 2017 - A Tutorial on Fisher Information.pdf:pdf},
journal = {arXiv preprint},
keywords = {Confidence intervals,Jef-freys's prior,and phrases,hypothesis testing,minimum description length,model complexity,model selec-tion,statistical modeling},
month = {oct},
title = {{A Tutorial on Fisher Information}},
url = {http://arxiv.org/abs/1705.01064},
year = {2017}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@misc{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
booktitle = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
urldate = {2018-05-28},
year = {2015}
}
@article{Schulman2015a,
abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
archivePrefix = {arXiv},
arxivId = {1506.05254},
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
eprint = {1506.05254},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schulman et al. - 2015 - Gradient Estimation Using Stochastic Computation Graphs.pdf:pdf},
issn = {10495258},
title = {{Gradient Estimation Using Stochastic Computation Graphs}},
url = {http://arxiv.org/abs/1506.05254},
year = {2015}
}
@article{Zheng2015,
author = {Zheng, Shuai and Jayasumana, Sadeep and Torr, Philip H. S.},
doi = {10.1109/ICCV.2015.179},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {1529--1537},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
year = {2015}
}
@inproceedings{Arthur2007,
abstract = {" ... Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, held January 7-9, 2007, in New Orleans, Louisiana ... jointly sponsored by the SIAM Activity Group on Discrete Mathematics and by SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory."--Page xiii. Title from PDF title page (viewed Mar. 26, 2010).},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
doi = {10.1145/1283383.1283494},
isbn = {9780898716245},
issn = {0898716241},
pages = {1027--1025},
pmid = {1000164511},
title = {{k-means++: The Advantages of Careful Seeding}},
year = {2007}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:Users/Jakob/Documents/Mendeley Desktop/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf:pdf},
journal = {arXiv preprint},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {http://arxiv.org/abs/1708.07747},
year = {2017}
}
@article{He2016,
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/Jakob/Documents/Mendeley Desktop/He et al. - 2015 - Deep residual learning for image recognition.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
month = {dec},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@inproceedings{Abdolmaleki2017,
abstract = {CMA-ES is one of the most popular stochastic search algorithms. It performs favourably in many tasks without the need of extensive parameter tuning. The algorithm has many beneficial properties, including automatic step-size adaptation, efficient covariance up-dates that incorporates the current samples as well as the evolution path and its invariance properties. Its update rules are composed of well established heuristics where the theoretical foundations of some of these rules are also well understood. In this paper we will fully derive all CMA-ES update rules within the framework of expectation-maximisation-based stochastic search algorithms using information-geometric trust regions. We show that the use of the trust region results in similar updates to CMA-ES for the mean and the covariance matrix while it allows for the derivation of an improved update rule for the step-size. Our new algorithm, Trust-Region Co-variance Matrix Adaptation Evolution Strategy (TR-CMA-ES) is fully derived from first order optimization principles and performs favourably in compare to standard CMA-ES algorithm.},
address = {Berlin, Germany},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Abdolmaleki, Abbas and Price, Bob and Lau, Nuno and Reis, Luis Paulo and Neumann, Gerhard},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference on   - GECCO '17},
doi = {10.1145/3071178.3071252},
eprint = {arXiv:1602.05561v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Abdolmaleki et al. - 2017 - Deriving and improving CMA-ES with information geometric trust regions.pdf:pdf},
isbn = {9781450349208},
issn = {16130073},
number = {17},
pages = {657--664},
title = {{Deriving and improving CMA-ES with information geometric trust regions}},
url = {http://dl.acm.org/citation.cfm?doid=3071178.3071252},
year = {2017}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:Users/Jakob/Documents/Mendeley Desktop/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
journal = {arXiv preprint},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@article{Wierstra2008,
abstract = {This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued 'black box' function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the 'vanilla' gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. {\textcopyright} 2008 IEEE.},
archivePrefix = {arXiv},
arxivId = {1106.4487},
author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/CEC.2008.4631255},
eprint = {1106.4487},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wierstra et al. - 2008 - Natural Evolution Strategies.pdf:pdf},
isbn = {978-1-4244-1822-0},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {black-box optimization,evolution strategies,natural gradient,sampling,stochastic search},
pages = {3381--3387},
title = {{Natural Evolution Strategies}},
url = {http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf},
volume = {15},
year = {2008}
}
@book{Madsen2008,
address = {Copenhagen},
author = {Madsen, Henrik},
edition = {1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Madsen - 2008 - Time Series Analysis.pdf:pdf},
isbn = {978-1-4200-5967-0},
keywords = {Time series analysis},
mendeley-tags = {Time series analysis},
pages = {380},
publisher = {Chapman and Hall/CRC},
title = {{Time Series Analysis}},
year = {2008}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
doi = {10.3389/fphys.2016.00108},
eprint = {1606.05908},
file = {:Users/Jakob/Documents/Mendeley Desktop/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
isbn = {1532-4435},
issn = {1664042X},
journal = {arXiv preprint},
pmid = {27148061},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}
@article{Hinton2015,
author = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeff},
journal = {arXiv preprint},
title = {{Distilling the knowledge in a neural network}},
year = {2015}
}
@phdthesis{Schaul2011a,
abstract = {Optimization is the research field that studies that studies the design of algorithms for finding the best solutions to problems we humans throw at them. While the whole domain is of important practical utility, the present thesis will focus on the subfield of continuous black-box optimization, presenting a collection of novel, state-of-the-art algorithms for solving problems in that class. First, we introduce a general-purpose algorithm called Natural Evolution Strategies (NES). In contrast to typical evolutionary algorithms which search in the vicinity of the fittest individuals in a population, evolution strategies aim at repeating the type of mutations that led to those individuals. We can characterize those mutations by a search distribution. The key idea of NES is to ascend the gradient on the parameters of that distribution towards higher expected fitness. We show how plain gradient ascent is destined to fail, and provide a viable alternative that instead descends along the natural gradient to adapt the search distribution, which appropriately normalizes the update step with respect to its uncertainty. Being derived from first principles, the NES approach can be extended to all types of search distributions that allow a parametric form, not just the classical multivariate Gaussian one. We derive a number of NES variants for different distributions, and show how they are useful on different problem classes. In addition, we rein in the computational cost, avoiding costly matrix inversions through an incremental change of coordinates. Two additional, novel techniques, importance mixing and adaptation sampling, allow us to automatically tune the learning rate and batch size to the problem, and thereby further reduce the average number of required fitness evaluations. A third technique, restart strategies, provides the algorithm with additional robustness in the presence of multiple local optima, or noise. Second, we introduce a new approach to costly black-box optimization, when fitness evaluations are very expensive. Here, we model the fitness function using state-of-the-art Gaussian process regression, and use the principle of artificial curiosity to direct exploration towards the most informative next evaluation candidate. Both the expected fitness improvement and the expected information gain can be derived explicitly from the Gaussian process model, and our method constructs a front of Pareto-optimal points according to these two criteria. This makes the exploration-exploitation trade-off explicit, and permits maximally informed candidate selection. In summary, this dissertation presents a collection of novel algorithms, for the general problem of continuous black-box optimization as well as a number of special cases, each validated empirically.},
author = {Schaul, Tom and Br{\"{u}}gge, B.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schaul, Br{\"{u}}gge - 2011 - Studies in Continuous Black-box Optimization.pdf:pdf},
school = {Technical University of Munich},
title = {{Studies in Continuous Black-box Optimization}},
type = {PhD Thesis},
url = {https://pdfs.semanticscholar.org/eb2d/7fb3105cd98646943b5dccf799d2bb8b09ed.pdf},
year = {2011}
}
@inproceedings{Girshick2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@techreport{Schmidhuber1999,
abstract = {Reinforcement learning based on direct search in policy space requires few assumptions about the environment. Hence it is applicable in certain situations where most traditional reinforcement learning algorithms based on dynamic programming are not, especially in partially observable, deterministic worlds. In realistic settings, however, reliable policy evaluations are complicated by numerous sources of uncertainty, such as stochasticity in policy and environment. Given a limited life-time, how much time should a direct policy searcher spend on policy evaluations to obtain reliable statistics? De-spite the fundamental nature of this question it has not received much attention yet. Our efficient approach based on the success-story algorithm (SSA) is radical in the sense that it never stops evaluating any pre-vious policy modification except those it undoes for lack of empirical evidence that they have contributed to lifelong reward accelerations. Here we identify SSA's fundamental advantages over traditional direct policy search (such as stochastic hill-climbing) on problems involving several sources of stochasticity and uncer-taint),. INTRODUCTION In this paper a learner's modifiable parameters that determine its behavior are called its policy. An al-gorithm that modifies the policy is called a learn-ing algorithm. In the context of reinforcement learn-ing (RL) there are two broad classes of learning algo-rithms: (1) methods based on value functions (VFs), and (2) direct search in policy space. VF-based algo-rithms learn a mapping from input-action pairs to ex-pected discounted future reward and use online vari-ants of dynamic programming (DP) (Bellman 1961) for constructing rewarding policies,},
author = {Schmidhuber, J{\"{u}}rgen and Zhao, Jieyu},
booktitle = {AAAI Technical Report SS-99-07},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schmidhuber, Zhao - 1999 - Direct Policy Search and Uncertain Policy Evaluation.pdf:pdf},
pages = {119--124},
title = {{Direct Policy Search and Uncertain Policy Evaluation}},
url = {https://pdfs.semanticscholar.org/dd17/8d3f30d801922c98cec9c2d90db05395f244.pdf},
year = {1999}
}
@article{Nguyen2017,
abstract = {We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a "wide" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer, we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima.},
archivePrefix = {arXiv},
arxivId = {1710.10928},
author = {Nguyen, Quynh and Hein, Matthias},
eprint = {1710.10928},
file = {:Users/Jakob/Documents/Mendeley Desktop/Nguyen, Hein - 2017 - The loss surface and expressivity of deep convolutional neural networks.pdf:pdf},
issn = {1938-7228},
month = {oct},
title = {{The loss surface and expressivity of deep convolutional neural networks}},
url = {http://arxiv.org/abs/1710.10928},
year = {2017}
}
@inproceedings{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/Jakob/Documents/Mendeley Desktop/Long, Shelhamer, Darrell - 2015 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf},
year = {2015}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch â a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Chanan, Gregory and Lin, Zeming and Gross, Sam and Yang, Edward and Antiga, Luca and Devito, Zachary},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
file = {:Users/Jakob/Documents/Mendeley Desktop/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:pdf},
title = {{Automatic differentiation in PyTorch}},
url = {https://pytorch.org/},
year = {2017}
}
@inproceedings{Yin2016,
abstract = {This paper presents an end-to-end neural net-work model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can ef-fectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demon-strates that the proposed model can outperform an embedding-based QA model as well as a neural di-alogue model trained on the same data.},
author = {Yin, Jun and Jiang, Xin and Lu, Zhengdong and Shang, Lifeng and Li, Hang and Li, Xiaoming},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
file = {:Users/Jakob/Documents/Mendeley Desktop/Yin et al. - 2016 - Neural Generative Question Answering.pdf:pdf},
keywords = {Natural Language Processing,Question answering},
mendeley-tags = {Natural Language Processing,Question answering},
title = {{Neural Generative Question Answering}},
url = {https://www.ijcai.org/Proceedings/16/Papers/422.pdf},
year = {2016}
}
@article{Nesterov2017,
abstract = {In this paper, we prove new complexity bounds for methods of convex optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true for both nonsmooth and smooth problems. For the latter class, we present also an accelerated scheme with the expected rate of convergence O(n{\^{}}2/k{\^{}}2), where k is the iteration counter. For stochastic optimization, we propose a zero-order scheme and justify its expected rate of convergence O(n/k{\^{}}1/2).We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, for both smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.},
author = {Nesterov, Yurii and Spokoiny, Vladimir},
doi = {10.1007/s10208-015-9296-2},
file = {:Users/Jakob/Documents/Mendeley Desktop/Nesterov, Spokoiny - 2017 - Random Gradient-Free Minimization of Convex Functions.pdf:pdf},
issn = {16153383},
journal = {Foundations of Computational Mathematics},
keywords = {Complexity bounds,Convex optimization,Derivative-free methods,Random methods,Stochastic optimization},
number = {2},
pages = {527--566},
title = {{Random Gradient-Free Minimization of Convex Functions}},
url = {http://production.datastore.cvt.dk/filestore?oid=58d660db5010df4e490ca70c{\&}targetid=58d660db5010df4e490ca70e},
volume = {17},
year = {2017}
}
@article{Clark2016,
abstract = {A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1606.01323},
author = {Clark, Kevin and Manning, Christopher D.},
doi = {10.18653/v1/P16-1061},
eprint = {1606.01323},
file = {:Users/Jakob/Documents/Mendeley Desktop/Clark, Manning - 2016 - Improving Coreference Resolution by Learning Entity-Level Distributed Representations.pdf:pdf},
isbn = {9781510827585},
journal = {arXiv preprint},
title = {{Improving Coreference Resolution by Learning Entity-Level Distributed Representations}},
url = {http://arxiv.org/abs/1606.01323},
year = {2016}
}
@article{Kaelbling1998,
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a comer; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the northeast comer of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and {Cassandra '}, Anthony R.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kaelbling, Littman, Cassandra ' - 1998 - Artificial Intelligence Planning and acting in partially observable stochastic domains.pdf:pdf},
journal = {Artificial IntelligenceArti{\$}cial Intelligence},
keywords = {Partially observable Markov decision processes,Planning,Uncertainty},
number = {101},
pages = {99--134},
title = {{Artificial Intelligence Planning and acting in partially observable stochastic domains}},
url = {http://production.datastore.cvt.dk/filestore?oid=539d2d94e2a1a1d725047a22{\&}targetid=539d2d94e2a1a1d725047a26},
volume = {101},
year = {1998}
}
@article{Graves2016,
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John},
journal = {Nature},
number = {7626},
pages = {471--476},
publisher = {Nature Research},
title = {{Hybrid computing using a neural network with dynamic external memory}},
volume = {538},
year = {2016}
}
@article{Mogren2016,
abstract = {Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.},
archivePrefix = {arXiv},
arxivId = {1611.09904},
author = {Mogren, Olof},
eprint = {1611.09904},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mogren - 2016 - C-RNN-GAN Continuous recurrent neural networks with adversarial training.pdf:pdf},
title = {{C-RNN-GAN: Continuous recurrent neural networks with adversarial training}},
url = {http://arxiv.org/abs/1611.09904},
year = {2016}
}
@article{Spall1992,
abstract = {The problem of finding a root of the multivariate gradient equation that arises in function minimization is considered. When only noisy measurements of the function are available, a stochastic approximation (SA) algorithm for the general Kiefer-Wolfowitz type is appropriate for estimating the root. The paper presents an SA algorithm that is based on a simultaneous perturbation gradient approximation instead of the standard finite-difference approximation of Keifer-Wolfowitz type procedures. Theory and numerical experience indicate that the algorithm can be significantly more efficient than the standard algorithms in large-dimensional problems.},
author = {Spall, James C.},
doi = {10.1109/9.119632},
file = {:Users/Jakob/Documents/Mendeley Desktop/Spall - 1992 - Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation.pdf:pdf},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
keywords = {Acceleration,Adaptive control,Approximation algorithms,Convergence,Design for experiments,Differential equations,Finite difference methods,Kiefer-Wolfowitz type,Neural networks,Q measurement,Stochastic processes,function approximation,function minimization,multivariate gradient equation,noisy measurements,root,simultaneous perturbation gradient approximation,stochastic approximation},
number = {3},
pages = {332--341},
title = {{Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation}},
volume = {37},
year = {1992}
}
@inproceedings{Carpentier2012,
abstract = {We consider the problem of adaptive stratified sampling for Monte Carlo integra-tion of a differentiable function given a finite number of evaluations to the func-tion. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost similarly accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and provide a finite-sample analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.5345v1},
author = {Carpentier, Alexandra and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1210.5345v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Carpentier, Munos - 2012 - Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {1--23},
title = {{Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions.}},
url = {https://arxiv.org/pdf/1210.5345.pdf https://papers.nips.cc/paper/4519-adaptive-stratified-sampling-for-monte-carlo-integration-of-differentiable-functions.pdf},
year = {2012}
}
@article{Dauphin2014,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
issn = {10495258},
journal = {arXiv preprint},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@misc{Kosiorek2017,
abstract = {Attention mechanisms in neural networks, otherwise known as neural attention or just attention, have recently attracted a lot of attention (pun intended). In this post, I will try to find a common denominator for different mechanisms and use-cases and I will describe (and implement!) two mechanisms of soft visual attention.
},
author = {Kosiorek, Adam},
title = {{Attention in Neural Networks and How to Use It}},
url = {http://akosiorek.github.io/ml/2017/10/14/visual-attention.html},
urldate = {2018-04-18},
year = {2017}
}
@article{Chang2018,
abstract = {Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.},
archivePrefix = {arXiv},
arxivId = {1803.05859},
author = {Chang, Oscar and Lipson, Hod},
eprint = {1803.05859},
file = {:Users/Jakob/Documents/Mendeley Desktop/Chang, Lipson - 2018 - Neural Network Quine.pdf:pdf},
journal = {arXiv preprint},
title = {{Neural Network Quine}},
url = {http://arxiv.org/abs/1803.05859},
year = {2018}
}
@article{Lever2014,
author = {Lever, Guy},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
title = {{Deterministic policy gradient algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.636.826 http://www.jmlr.org/proceedings/papers/v32/silver14.pdf{\%}0Ahttp://jmlr.org/proceedings/papers/v32/silver14.pdf},
year = {2014}
}
@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
author = {Krizhevsky, Alex},
file = {:Users/Jakob/Documents/Mendeley Desktop/Krizhevsky - 2014 - One weird trick for parallelizing convolutional neural networks.pdf:pdf},
title = {{One weird trick for parallelizing convolutional neural networks}},
url = {https://arxiv.org/pdf/1404.5997.pdf},
year = {2014}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@inproceedings{Karpathy2015,
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {3128--3137},
title = {{Deep visual-semantic alignments for generating image descriptions}},
year = {2015}
}
@inproceedings{Koutnik2016,
abstract = {A new indirect scheme for encoding neural network connec-tion weights as sets of wavelet-domain coefficients is pro-posed in this paper. It exploits spatial regularities in the weight-space to reduce the gene-space dimension by con-sidering the low-frequency wavelet coefficients only. The wavelet-based encoding builds on top of a frequency-domain encoding, but unlike when using a Fourier-type transform, it offers gene locality while preserving continuity of the genotype-phenotype mapping. We argue that this added property al-lows for more efficient evolutionary search and demonstrate this on the octopus-arm control task, where superior solu-tions were found in fewer generations. The scalability of the wavelet-based encoding is shown by evolving networks with many parameters to control game-playing agents in the Arcade Learning Environment.},
address = {Denver, CO, USA},
author = {Koutn{\'{i}}k, Jan and Driessens, Kurt},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)},
doi = {10.1145/2908812.2908905},
file = {:Users/Jakob/Documents/Mendeley Desktop/Koutn{\'{i}}k, Driessens - 2016 - A Wavelet-based Encoding for Neuroevolution.pdf:pdf},
isbn = {9781450342063},
keywords = {Generative and developmental approaches,Keywords Neuroevolution,Reinforce-ment learning,gene-locality,indirect encoding,wavelets},
pages = {517--524},
title = {{A Wavelet-based Encoding for Neuroevolution}},
url = {http://delivery.acm.org.proxy.findit.dtu.dk/10.1145/2910000/2908905/p517-van-steenkiste.pdf?ip=192.38.67.116{\&}id=2908905{\&}acc=ACTIVE SERVICE{\&}key=36332CD97FA87885.5A225593D1BFDB6E.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1523957938{\_}11d8b20d0a6afb8a7ea619e1b},
year = {2016}
}
@article{Neal1993,
abstract = {The attempt to find a single "optimal" weight vector in conven-tional network training can lead to overfitting and poor generaliza-tion. Bayesian methods avoid this, without the need for a valida-tion set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.},
annote = {First Markov Chain Monte Carlo (MCMC) sampling algorithm for Bayesian neural networks. Uses Hamiltonian Monte Carlo (HMC), a sophisticated MCMC algorithm that makes use of gradients to sample efficiently.},
author = {Neal, Radford M.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Neal - 1993 - Bayesian learning via stochastic dynamics.pdf:pdf},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {475},
title = {{Bayesian learning via stochastic dynamics}},
url = {https://pdfs.semanticscholar.org/d275/cf94e620bf5b3776bba8a88acccdcfcd9a19.pdf},
year = {1993}
}
@inproceedings{Park2017,
abstract = {Regularizing neural networks is an important task to reduce overfitting. Dropout [1] has been a widelyused regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization effect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the effect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image filters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2]. {\textcopyright} Springer International Publishing AG 2017.},
annote = {Concludes that dropout can also be applied with benefit to convolutional layers with low dropout rates: p = 0.1 to 0.2. This was not done in the introducing articles, which only applied it to the fully connected parts.},
author = {Park, Sungheon and Kwak, Nojun},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-54184-6_12},
file = {:Users/Jakob/Documents/Mendeley Desktop/Park, Kwak - 2017 - Analysis on the Dropout Effect in Convolutional Neural Networks.pdf:pdf},
isbn = {9783319541839},
issn = {16113349},
pages = {189--204},
title = {{Analysis on the Dropout Effect in Convolutional Neural Networks}},
url = {http://mipal.snu.ac.kr/images/1/16/Dropout{\_}ACCV2016.pdf},
volume = {10112 LNCS},
year = {2017}
}
@incollection{Lemarechal1989,
abstract = {This chapter discusses the nondifferentiable optimization (NDO). Nondifferentiable optimization or nonsmooth optimization (NSO) deals with the situations in operations research where a function that fails to have derivatives for some values of the variables has to be optimized. For this situation, new tools are required to replace standard differential calculus, and these new tools come from convex analysis. Functions with discontinuous derivatives are frequent in operations research. Sometimes they arise when modeling the problem, sometimes they are introduced artificially during the solution procedure. The chapter discusses the necessary concepts and the basic properties and some examples of practical problems motivating the use of NSO. It is shown how and why classical methods fail. The chapter also discusses some possibilities that can be used when a special structure exists in the nonsmooth problem. It also presents subgradient methods and more recent methods and also covers some orientations for future research. {\textcopyright} 1989 Elsevier Science Publishers B.V.},
author = {Lemar{\'{e}}chal, Claude},
booktitle = {Handbooks in Operations Research and Management Science},
chapter = {VII},
doi = {10.1016/S0927-0507(89)01008-X},
isbn = {9780444872845},
issn = {09270507},
number = {C},
pages = {529--572},
title = {{Nondifferentiable optimization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092705078901008X},
volume = {1},
year = {1989}
}
@article{Botev2017,
abstract = {We present an efficient block-diagonal ap- proximation to the Gauss-Newton matrix for feedforward neural networks. Our result- ing algorithm is competitive against state- of-the-art first order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a labo- rious process, our approach can provide good performance even when used with default set- tings. A side result of our work is that for piecewise linear transfer functions, the net- work objective function can have no differ- entiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
archivePrefix = {arXiv},
arxivId = {1706.03662},
author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
eprint = {1706.03662},
file = {:Users/Jakob/Documents/Mendeley Desktop/Botev, Ritter, Barber - 2017 - Practical Gauss-Newton Optimisation for Deep Learning.pdf:pdf},
title = {{Practical Gauss-Newton Optimisation for Deep Learning}},
url = {http://proceedings.mlr.press/v70/botev17a/botev17a.pdf http://arxiv.org/abs/1706.03662},
year = {2017}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@book{Sutton1998,
abstract = {Reinforcement learning, one of the most active research areas in artificialintelligence, is a computational approach to learning whereby an agent tries to maximize the totalamount of reward it receives when interacting with a complex, uncertain environment. InReinforcement Learning, Richard Sutton and Andrew Barto provide a clear andsimple account of the key ideas and algorithms of reinforcement learning. Their discussion rangesfrom the history of the field's intellectual foundations to the most recent developments andapplications. The only necessary mathematical background is familiarity with elementary concepts ofprobability.},
author = {Sutton, Richard S. and Barto, Andrew G.},
isbn = {9780262193986},
issn = {1600-0579},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@inproceedings{Gal2015a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1506.02142},
file = {:Users/Jakob/Documents/Mendeley Desktop/Gal, Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
month = {jun},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
year = {2015}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1007/978-3-319-16817-3},
eprint = {1606.03657},
file = {:Users/Jakob/Documents/Mendeley Desktop/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
isbn = {978-3-319-16816-6},
issn = {978-3-319-16807-4},
pmid = {23459267},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{Liu2017,
abstract = {Machine reading using differentiable rea-soning models has recently shown re-markable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual rea-soning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particu-larly due to the necessity of more com-plex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regu-lation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architec-ture (GMemN2N). From the machine learn-ing perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision sig-nal which is, as far as our knowledge goes, the first of its kind. Our experi-ments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Di-alog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
archivePrefix = {arXiv},
arxivId = {1610.04211},
author = {Liu, Fei and Perez, Julien},
doi = {10.1038/nature20101},
eprint = {1610.04211},
file = {:Users/Jakob/Documents/Mendeley Desktop/Liu, Perez - 2017 - Gated End-to-End Memory Networks.pdf:pdf},
isbn = {0896-6273},
issn = {0028-0836},
journal = {Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
keywords = {Memory Networks,Natural Language Processing,Question answering},
mendeley-tags = {Memory Networks,Natural Language Processing,Question answering},
pages = {1--10},
pmid = {26774160},
title = {{Gated End-to-End Memory Networks}},
url = {http://www.aclweb.org/anthology/E17-1001},
volume = {1},
year = {2017}
}
@inproceedings{Pham2014,
abstract = {Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.},
archivePrefix = {arXiv},
arxivId = {1312.4569},
author = {Pham, Vu and Bluche, Theodore and Kermorvant, Christopher and Louradour, Jerome},
booktitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition (ICFHR)},
doi = {10.1109/ICFHR.2014.55},
eprint = {1312.4569},
file = {:Users/Jakob/Documents/Mendeley Desktop/Pham et al. - 2014 - Dropout Improves Recurrent Neural Networks for Handwriting Recognition.pdf:pdf},
isbn = {9781479943340},
issn = {21676453},
keywords = {Dropout,Handwriting Recognition,Recurrent Neural Networks},
pages = {285--290},
title = {{Dropout Improves Recurrent Neural Networks for Handwriting Recognition}},
year = {2014}
}
@article{Hasanpour2016,
abstract = {Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded system or system with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made available at: https://github.com/Coderx7/SimpleNet},
archivePrefix = {arXiv},
arxivId = {1608.06037},
author = {Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
eprint = {1608.06037},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hasanpour et al. - 2016 - Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures.pdf:pdf},
journal = {arXiv preprint},
title = {{Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures}},
url = {http://arxiv.org/abs/1608.06037},
year = {2016}
}
@article{Hansen2016,
abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
archivePrefix = {arXiv},
arxivId = {1604.00772},
author = {Hansen, Nikolaus},
doi = {10.1007/11007937_4},
eprint = {1604.00772},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf:pdf},
isbn = {3540290060},
issn = {14349922},
journal = {arXiv preprint},
keywords = {()},
pmid = {15003161},
title = {{The CMA Evolution Strategy: A Tutorial}},
url = {http://arxiv.org/abs/1604.00772},
year = {2016}
}
@article{See,
abstract = {Neural sequence-to-sequence models have provided a viable new approach for ab-stractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the origi-nal text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we pro-pose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate repro-duction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail sum-marization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
author = {See, Abigail and Liu, Peter J. and Brain, Google and Manning, Christopher D.},
file = {:Users/Jakob/Documents/Mendeley Desktop/See et al. - Unknown - Get To The Point Summarization with Pointer-Generator Networks.pdf:pdf},
title = {{Get To The Point: Summarization with Pointer-Generator Networks}},
url = {https://arxiv.org/pdf/1704.04368.pdf}
}
@book{Press2007,
abstract = {From Preface: âI was just going to say, when I was interrupted:::â begins Oliver Wendell Holmes in the second series of his famous essays, The Autocrat of the Breakfast Table. The interruption referred towas a gap of 25 years. In our case, as the autocrats of Numerical Recipes, the gap between our second and third editions has been âonlyâ 15 years. Scientific computing has changed enormously in that time. The first edition of Numerical Recipes was roughly coincident with the first commercial success of the personal computer. The second edition came at about the time that the Internet, as we know it today, was created. Now, as we launch the third edition, the practice of science and engineering, and thus scientific computing, has been profoundly altered by the mature Internet and Web. It is no longer difficult to find somebody's algorithm, and usually free code, for almost any conceivable scien- tific application. The critical questions have instead become, âHow does it work?â and âIs it any good?â Correspondingly, the second edition of Numerical Recipes has come to be valued more and more for its text explanations, concise mathematical derivations, critical judgments, and advice, and less for its code implementations per se. Recognizing the change, we have expanded and improved the text in many places in this edition and addedmany completely newsections. We seriously consid- ered leaving the code out entirely, or making it available only on theWeb. However, in the end, we decided that without code, it wouldn't be Numerical Recipes.That is, without code you, the reader, could never know whether our advice was in fact hon- est, implementable, and practical. Many discussions of algorithms in the literature and on the Web omit crucial details that can only be uncovered by actually coding (our job) or reading compilable code (your job). Also, we needed actual code to teach and illustrate the large number of lessons about object-oriented programming that are implicit and explicit in this edition.},
author = {Press, William H. and Teukolsky, Saul a. and Vetterling, William T. and Flannery, Brian P.},
booktitle = {Cambridge University Press},
isbn = {0-521-88068-8},
number = {4},
pages = {1256},
title = {{Numerical recipes}},
url = {numerical.recipes},
volume = {29},
year = {2007}
}
@inproceedings{Donahue2014,
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
pages = {647--655},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.}},
volume = {32},
year = {2014}
}
@article{Loshchilov2017,
abstract = {L{\$}{\_}2{\$} regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is $\backslash$emph{\{}not{\}} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L{\$}{\_}2{\$} regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
archivePrefix = {arXiv},
arxivId = {1711.05101},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1711.05101},
file = {:Users/Jakob/Documents/Mendeley Desktop/Loshchilov, Hutter - 2017 - Fixing Weight Decay Regularization in Adam.pdf:pdf},
journal = {arXiv preprint},
month = {nov},
title = {{Fixing Weight Decay Regularization in Adam}},
url = {http://arxiv.org/abs/1711.05101},
year = {2017}
}
@phdthesis{HavtornJakobDrachmann;SkovhusThorbjÃ¸rn;Blaabjerg2014,
author = {Havtorn, Jakob D. and Skovhus, Thorbj{\o}rn and Blaabjerg, Lasse},
file = {:Users/Jakob/Documents/Mendeley Desktop/Havtorn, Skovhus, Blaabjerg - 2014 - Physics Project Contact Resistance Measurement Rig for Thermoelectric Materials.pdf:pdf},
school = {Technical University of Denmark},
title = {{Physics Project Contact Resistance Measurement Rig for Thermoelectric Materials}},
type = {Second Year Project},
year = {2014}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/Jakob/Documents/Mendeley Desktop/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@incollection{Bengtsson2008,
abstract = {It has been widely realized that Monte Carlo methods (approximation via a sample ensemble) may fail in large scale systems. This work offers some theoretical insight into this phenomenon in the context of the particle filter. We demonstrate that the maximum of the weights associated with the sample ensemble converges to one as both the sample size and the system dimension tends to infinity. Specifically, under fairly weak assumptions, if the ensemble size grows sub-exponentially in the cube root of the system dimension, the convergence holds for a single update step in state-space models with independent and identically distributed kernels. Further, in an important special case, more refined arguments show (and our simulations suggest) that the convergence to unity occurs unless the ensemble grows super-exponentially in the system dimension. The weight singularity is also established in models with more general multivariate likelihoods, e.g. Gaussian and Cauchy. Although presented in the context of atmospheric data assimilation for numerical weather prediction, our results are generally valid for high-dimensional particle filters.},
archivePrefix = {arXiv},
arxivId = {0805.3034},
author = {Bengtsson, Thomas and Bickel, Peter and Li, Bo},
booktitle = {Probability and Statistics: Essays in Honor of David A. Freedman},
doi = {10.1214/193940307000000518},
eprint = {0805.3034},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bengtsson, Bickel, Li - 2008 - Curse-of-dimensionality revisited Collapse of the particle filter in very large scale systems.pdf:pdf},
isbn = {0940600749},
pages = {316--334},
title = {{Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems}},
url = {https://www.stat.berkeley.edu/{~}bickel/696.pdf http://projecteuclid.org/euclid.imsc/1207580091},
year = {2008}
}
@article{Alain2015,
abstract = {Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.},
archivePrefix = {arXiv},
arxivId = {1511.06481},
author = {Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
eprint = {1511.06481},
file = {:Users/Jakob/Documents/Mendeley Desktop/Alain et al. - 2015 - Variance Reduction in SGD by Distributed Importance Sampling.pdf:pdf},
journal = {arXiv preprint},
month = {nov},
title = {{Variance Reduction in SGD by Distributed Importance Sampling}},
url = {https://arxiv.org/abs/1511.06481},
year = {2015}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@article{Mikolov,
abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:Users/Jakob/Documents/Mendeley Desktop//Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
journal = {arXiv preprint},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {https://arxiv.org/pdf/1301.3781.pdf},
year = {2013}
}
@article{McAuley2015,
author = {McAuley, Julian},
title = {{Image-based Recommendations on Styles and Substitutes}},
year = {2015}
}
@article{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\~{}}junhua.mao/m-RNN.html .},
archivePrefix = {arXiv},
arxivId = {1412.6632},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
doi = {10.1.1.17.4650},
eprint = {1412.6632},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mao et al. - 2014 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
isbn = {1558608044},
issn = {10477349},
journal = {arXiv preprint},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
url = {http://arxiv.org/abs/1412.6632},
year = {2014}
}
@article{Lawrence2004,
abstract = {In this paper we introduce a new underlying probabilistic model for prin-cipal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance func-tion constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian pro-cess latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.},
author = {Lawrence, Neil D.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lawrence - 2004 - Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data.pdf:pdf},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
title = {{Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=38DBE5D621D9CC33004B45244387077F?doi=10.1.1.9.3440{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@inproceedings{LEcuyer2016,
abstract = {We survey basic ideas and results on randomized quasi-Monte Carlo (RQMC) methods, discuss their practical aspects, and give numerical illustrations. RQMC can improve accuracy compared with standard Monte Carlo (MC) when estimating an integral interpreted as a mathematical expectation. RQMC estimators are unbiased and their variance converges at a faster rate (under certain conditions) than MC estimators, as a function of the sample size. Variants of RQMC also work for the simulation of Markov chains, for function approximation and optimization, for solving partial differential equations, etc. In this introductory survey, we look at how RQMC point sets and sequences are constructed, how we measure their uniformity, why they can work for high-dimensional integrals, and how can they work when simulating Markov chains over a large number of steps.},
author = {L'Ecuyer, Pierre},
booktitle = {International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing (MCQMC)},
file = {:Users/Jakob/Documents/Mendeley Desktop/L'Ecuyer - 2016 - Randomized Quasi-Monte Carlo An Introduction for Practitioners.pdf:pdf},
title = {{Randomized Quasi-Monte Carlo: An Introduction for Practitioners}},
url = {https://hal.inria.fr/hal-01561550/document},
year = {2016}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li - 2017 - Deep Reinforcement Learning An Overview.pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
journal = {arXiv preprint},
keywords = {Deep learning,Overview,Reinforcement learning,Review,Survey,a n o verview,com,earning,ep r einforcement l,gmail,yuxi li,yuxili},
mendeley-tags = {Deep learning,Overview,Reinforcement learning,Review,Survey},
month = {jan},
pages = {1--70},
pmid = {15040217},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Johnson2013,
abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this prob-lem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast con-vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the stor-age of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
author = {Johnson, Rie and Zhang, Tong},
file = {:Users/Jakob/Documents/Mendeley Desktop/Johnson, Zhang - 2013 - Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf:pdf},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
keywords = {To-Read},
number = {3},
pages = {315--323},
pmid = {880145},
title = {{Accelerating Stochastic Gradient Descent using Predictive Variance Reduction}},
url = {https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf{\%}5Cnhttp://papers.nips.cc/p},
volume = {1},
year = {2013}
}
@article{Mazidi2016,
abstract = {We present a fresh approach to automatic question generation that significantly in-creases the percentage of acceptable questions compared to prior state-of-the-art systems. In our evaluation of the top 20 questions, our sys-tem generated 71{\%} more acceptable questions by informing the generation process with Nat-ural Language Understanding techniques. The system also introduces our DeconStructure al-gorithm which creates an intuitive and prac-tical structure for easily accessing sentence functional constituents in NLP applications.},
author = {Mazidi, Karen and Tarau, Paul},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mazidi, Tarau - 2016 - Infusing NLU into Automatic Question Generation.pdf:pdf},
journal = {Proceedings of the International Conference on Natural Language Generation (INLG)},
number = {2011},
pages = {51--60},
title = {{Infusing NLU into Automatic Question Generation}},
url = {http://www.aclweb.org/anthology/W16-6609},
year = {2016}
}
@inproceedings{Zhu2016,
author = {Zhu, Jun-Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
booktitle = {European Conference on Computer Vision},
pages = {597--613},
title = {{Generative visual manipulation on the natural image manifold}},
year = {2016}
}
@article{Wei,
abstract = {Knowledge on protein folding has a profound impact on understanding the heterogeneity and molecular function of proteins, further facilitating drug design. Predicting the 3D structure (fold) of a protein is a key problem in molecular biology. Determination of the fold of a protein mainly relies on molecular experimental methods. With the development of next-generation sequencing techniques, the discovery of new protein sequences has been rapidly increasing. With such a great number of proteins, the use of experimental techniques to determine protein folding is extremely difficult because these techniques are time consuming and expensive. Thus, developing computational prediction methods that can automatically, rapidly, and accurately classify unknown protein sequences into specific fold categories is urgently needed. Computational recognition of protein folds has been a recent research hotspot in bioinformatics and computational biology. Many computational efforts have been made, generating a variety of computational prediction methods. In this review, we conduct a comprehensive survey of recent computational methods, especially machine learning-based methods, for protein fold recognition. This review is anticipated to assist researchers in their pursuit to systematically understand the computational recognition of protein folds.},
author = {Zhu, Jianwei and Zhang, Haicang and Li, Shuai Cheng and Wang, Chao and Kong, Lupeng and Sun, Shiwei and Zheng, Wei-mou and Bu, Dongbo and Wei, Leyi and Zou, Quan},
doi = {10.3390/ijms17122118},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zhu et al. - 2016 - Recent progress in machine learning-based methods for protein fold recognition.pdf:pdf},
isbn = {8617092261008},
issn = {14220067},
journal = {International Journal of Molecular Sciences},
keywords = {Computational method,Machine learning,Protein fold recognition},
number = {12},
pages = {1--13},
pmid = {27999256},
title = {{Recent progress in machine learning-based methods for protein fold recognition}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5187918/pdf/ijms-17-02118.pdf},
volume = {17},
year = {2016}
}
@inproceedings{Goodfellow2015,
abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve opti-mization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct train-ing with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
archivePrefix = {arXiv},
arxivId = {1412.6544v5},
author = {Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1412.6544v5},
file = {:Users/Jakob/Documents/Mendeley Desktop/Goodfellow, Vinyals, Saxe - 2015 - Qualitatively characterizing neural network optimization problems.pdf:pdf},
pages = {1--11},
title = {{Qualitatively characterizing neural network optimization problems}},
url = {http://arxiv.org/abs/1412.6544},
year = {2015}
}
@article{AlQuraishi2018,
abstract = {Accurate prediction of protein structure is one of the central challenges of biochemistry. Despite significant progress made by co-evolution methods to predict protein structure from signatures of residue-residue coupling found in the evolutionary record, a direct and explicit mapping between protein sequence and structure remains elusive, with no substantial recent progress. Meanwhile, rapid developments in deep learning, which have found remarkable success in computer vision, natural language processing, and quantum chemistry raise the question of whether a deep learning based approach to protein structure could yield similar advancements. A key ingredient of the success of deep learning is the reformulation of complex, human-designed, multi-stage pipelines with differentiable models that can be jointly optimized end-to-end. We report the development of such a model, which reformulates the entire structure prediction pipeline using differentiable primitives. Achieving this required combining four technical ideas: (1) the adoption of a recurrent neural architecture to encode the internal representation of protein sequence, (2) the parameterization of (local) protein structure by torsional angles, which provides a way to reason over protein conformations without violating the covalent chemistry of protein chains, (3) the coupling of local protein structure to its global representation via recurrent geometric units, and (4) the use of a differentiable loss function to capture deviations between predicted and experimental structures. To our knowledge this is the first end-to-end differentiable model for learning of protein structure. We test the effectiveness of this approach using two challenging tasks: the prediction of novel protein folds without the use of co-evolutionary information, and the prediction of known protein folds without the use of structural templates. On the first task the model achieves state-of-the-art performance, even when compared to methods that rely on co-evolutionary data. On the second task the model is competitive with methods that use experimental protein structures as templates, achieving 3-7{\AA} accuracy despite being template-free. Beyond protein structure prediction, end-to-end differentiable models of proteins represent a new paradigm for learning and modeling protein structure, with potential applications in docking, molecular dynamics, and protein design.},
author = {AlQuraishi, Mohammed},
doi = {10.1101/265231},
file = {:Users/Jakob/Documents/Mendeley Desktop/AlQuraishi - 2018 - End-to-end differentiable learning of protein structure.pdf:pdf},
journal = {bioRxiv},
title = {{End-to-end differentiable learning of protein structure}},
url = {http://dx.doi.org/10.1101/265231 https://www.biorxiv.org/content/early/2018/02/14/265231},
year = {2018}
}
@article{Turing1950,
abstract = {Will computers and robots ever think and communicate$\backslash$nthe way humans do? When a computer crosses the$\backslash$nthreshold into self-consciousness, will it immediately$\backslash$njump into the Internet and create a World Mind? This is$\backslash$nan exploration of both the philosophical and$\backslash$nmethodological issues surrounding the search for true$\backslash$nartificial intelligence.},
author = {Turing, Alan M.},
journal = {Mind},
number = {236},
pages = {433--460},
title = {{Computing machinery and intelligence}},
url = {http://phil415.pbworks.com/f/TuringComputing.pdf},
volume = {59},
year = {1950}
}
@inproceedings{Wan2013,
abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1109/TPAMI.2017.2703082},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wan et al. - 2013 - Regularization of Neural Networks using Dropconnect.pdf:pdf},
issn = {0162-8828},
number = {1},
pages = {109--111},
pmid = {797520},
title = {{Regularization of Neural Networks using Dropconnect}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}wan13},
year = {2013}
}
@misc{Barber2017,
abstract = {A simple connection between evolutionary optimisation and variational methods},
author = {Barber, David},
file = {:Users/Jakob/Documents/Mendeley Desktop/Barber - 2017 - Evolutionary Optimization as a Variational Method.html:html},
keywords = {Deep learning,Optimization,Variational optimization,evolutionary computing,reinforcement learning},
mendeley-tags = {Deep learning,Optimization,Variational optimization,evolutionary computing,reinforcement learning},
title = {{Evolutionary Optimization as a Variational Method}},
url = {https://davidbarber.github.io/blog/2017/04/03/variational-optimisation/},
urldate = {2018-03-23},
year = {2017}
}
@article{Gaetz2018,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Gaetz, William and Kessler, Sudha K. and Roberts, Tim P. L. and Berman, Jeffrey I. and Levy, Todd J. and Hsia, Michelle and Humpl, Deborah and Schwartz, Erin S. and Amaral, Sandra and Chang, Ben and Levin, Lawrence Scott},
doi = {10.1002/acn3.501},
eprint = {1712.01815},
file = {:Users/Jakob/Documents/Mendeley Desktop/Gaetz et al. - 2018 - Massive cortical reorganization is reversible following bilateral transplants of the hands evidence from the first.pdf:pdf},
isbn = {3013372370},
issn = {23289503},
journal = {Annals of Clinical and Translational Neurology},
month = {dec},
number = {1},
pages = {92--97},
pmid = {29376095},
title = {{Massive cortical reorganization is reversible following bilateral transplants of the hands: evidence from the first successful bilateral pediatric hand transplant patient}},
url = {http://arxiv.org/abs/1712.01815},
volume = {5},
year = {2018}
}
@article{Weber2015,
author = {Weber, Theophane and Heess, Nicolas and Eslami, S. M. Ali and Schulman, John and Wingate, David and Silver, David},
file = {:Users/Jakob/Documents/Mendeley Desktop/Weber et al. - 2015 - Reinforced Variational Inference.pdf:pdf},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
number = {2},
pages = {1--9},
title = {{Reinforced Variational Inference}},
url = {http://arkitus.com/files/nips-15-weber-reinforced-inference.pdf},
year = {2015}
}
@article{Jung2014,
abstract = {This paper presents an overview of existing research on wind speed and power forecasting. It first discusses state-of-the-art wind speed and power forecasting approaches. Then, forecasting accuracy is presented based on variable factors. Finally, potential techniques to improve the accuracy of forecasting models are reviewed. A full survey on all existing models is not presented, but attempts to highlight the most promising body of knowledge concerning wind speed and power forecasting. {\textcopyright} 2014 Elsevier Ltd.},
author = {Jung, Jaesung and Broadwater, Robert P.},
doi = {10.1016/j.rser.2013.12.054},
file = {:Users/Jakob/Documents/Mendeley Desktop/Jung, Broadwater - 2014 - Current status and future advances for wind speed and power forecasting.pdf:pdf},
isbn = {1364-0321},
issn = {13640321},
journal = {Renewable and Sustainable Energy Reviews},
keywords = {Offshore forecasting,Probabilistic forecasting,Regional forecasting,Spatial correlation forecasting,Wind power,Wind speed},
pages = {762--777},
title = {{Current status and future advances for wind speed and power forecasting}},
url = {https://ac-els-cdn-com.proxy.findit.dtu.dk/S1364032114000094/1-s2.0-S1364032114000094-main.pdf?{\_}tid=159865d0-a4de-11e7-ad81-00000aab0f6b{\&}acdnat=1506666121{\_}9cf459f8fac5b072f300a92310ab330b},
volume = {31},
year = {2014}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
doi = {10.1167/16.12.326},
eprint = {1508.06576},
isbn = {2200000006},
issn = {1534-7362},
journal = {arXiv preprint},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
url = {http://arxiv.org/abs/1508.06576},
year = {2015}
}
@inproceedings{Sun2009,
abstract = {Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.},
annote = {Introduces effecient natural evolution strategies (sNES) along with optimal fitness baselines and importance mixing.

Experiments are in companion paper "Stochastic search using the natural gradient"},
archivePrefix = {arXiv},
arxivId = {1209.5853v1},
author = {Sun, Yi and Wierstra, Daan and Schaul, Tom and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
doi = {10.1145/1569901.1569976},
eprint = {1209.5853v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sun et al. - 2009 - Efficient Natural Evolution Strategies.pdf:pdf},
isbn = {9781605583259},
keywords = {Control Methods,G16 [Optimization],Heuristic methods,and Search],natural gradient,optimization},
pages = {539},
title = {{Efficient Natural Evolution Strategies}},
url = {https://arxiv.org/abs/1209.5853},
year = {2009}
}
@inproceedings{Blundell2015a,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {1505.05424},
file = {:Users/Jakob/Documents/Mendeley Desktop/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Weight Uncertainty in Neural Networks}},
url = {https://arxiv.org/pdf/1505.05424.pdf http://arxiv.org/abs/1505.05424},
year = {2015}
}
@article{Arik2017,
abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.07825},
author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew Y. and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
eprint = {1702.07825},
file = {:Users/Jakob/Documents/Mendeley Desktop/Arik et al. - 2017 - Deep Voice Real-time Neural Text-to-Speech.pdf:pdf},
issn = {1938-7228},
month = {feb},
title = {{Deep Voice: Real-time Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1702.07825},
year = {2017}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Bayesian Forecasting and Dynamic Models},
doi = {10.1007/b94608},
edition = {2},
isbn = {978-0-387-84857-0},
issn = {0172-7397},
keywords = {machine learning,optimization,statistical learning,statistics},
mendeley-tags = {machine learning,optimization,statistical learning,statistics},
pmid = {12377617},
publisher = {Springer},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
year = {2009}
}
@article{McClelland1995,
abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
doi = {10.1037/0033-295X.102.3.419},
file = {:Users/Jakob/Documents/Mendeley Desktop/McClelland, McNaughton, O'Reilly - 1995 - Why there are complementary learning systems in the hippocampus and neocortex Insights from th.pdf:pdf},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {0033295X},
journal = {Psychological Review},
number = {3},
pages = {419--457},
pmid = {7624455},
title = {{Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory}},
volume = {102},
year = {1995}
}
@inproceedings{Ioffe2015a,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {1502.03167},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
keywords = {Batch normalization,Deep learning,Regularization},
mendeley-tags = {Batch normalization,Deep learning,Regularization},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Rosenblatt1962,
author = {Rosenblatt, Frank},
file = {:Users/Jakob/Documents/Mendeley Desktop/Rosenblatt - 1962 - Analytic Techniques for the Study of Neural Nets.pdf:pdf},
journal = {Proceedings of AIEE Joint Automatic Control Conference},
pages = {285--292},
title = {{Analytic Techniques for the Study of Neural Nets}},
volume = {401},
year = {1962}
}
@article{Lillicrap2015,
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
journal = {arXiv preprint},
title = {{Continuous control with deep reinforcement learning}},
year = {2015}
}
@article{Graves2011,
abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
author = {Graves, Alex},
file = {:Users/Jakob/Documents/Mendeley Desktop/Graves - 2011 - Practical Variational Inference for Neural Networks.pdf:pdf},
isbn = {9781618395993},
journal = {The 32nd Annual Conference on Neural Information Processing Systems (NIPS-2011)},
pages = {1--9},
title = {{Practical Variational Inference for Neural Networks.}},
url = {https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf},
year = {2011}
}
@article{Clevert2015,
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork-Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
doi = {10.3233/978-1-61499-672-9-1760},
eprint = {1511.07289},
file = {:Users/Jakob/Documents/Mendeley Desktop/Clevert, Unterthiner, Hochreiter - 2015 - Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).pdf:pdf},
isbn = {9781614996712},
issn = {09226389},
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}},
url = {http://arxiv.org/abs/1511.07289},
year = {2015}
}
@article{Buntine1991,
abstract = {Connectionist feed-forward networks, t rained with back-prop agat ion, can be used both for nonlinear regression and for (dis-crete one-of-C) classification. T his pap er present s approximate Bayes-ian meth ods to statistical compo nents of back-pro pagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior prob-ability), pruning insignifican t weights, est imat ing th e uncertainty of weights, predict ing for new pat tern s ("out -of-sample") , est imating t he uncertainty in the choice of t his prediction ("erro r bars"), estima t-ing the generalizat ion erro r, comparing different network st ructures, and handling missing values in t he t raining pattern s. These meth -ods extend some heuristic techniques suggested in the literature, and in most cases require a small additional facto r in comp ut ation during back-propagation, or computation once back-pro pagat ion has finished. 1. I nt r o d u ction Back-propagati on [32] is a p opular sche me for t raining feed-for ward connec -t ionist networks. It can b e applied t o bo t h t he tasks of cla ssification (predic-t ion of discret e variables t aking one of C mutua lly exclusive and exha ust ive va lues) and regr ession (p redi ct ion of rea l va ria bles). Design issu es in t his sche me are prim arily com puta tional-for inst ance, what varia t ions of gradi-ent descen t should b e used-or probabilistic -for inst a nce, what cost func-ti on should b e used a nd how generalizat ion err or ca n b e pred ict ed . Her e we frame t he probabilistic component of back-prop agation in a Bayesian conte xt [6, 3, 7, 27]. In adopt ing a Bayesian justi fica t ion for t he method s present ed , we are not claiming any neurological valid ity for our metho ds. We view},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Buntine, Wray L.},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {arXiv:1011.1669v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Buntine - 1991 - Bayesian Back-Propagation.pdf:pdf},
isbn = {0030-8870},
issn = {00308870},
journal = {Complex Systems},
pages = {603--643},
pmid = {24439530},
title = {{Bayesian Back-Propagation}},
url = {https://pdfs.semanticscholar.org/c836/84f6207697c12850db423fd9747572cf1784.pdf},
volume = {5},
year = {1991}
}
@article{Shehab2013,
author = {Shehab, M. E. and Badran, K. and Salama, G. I.},
doi = {10.1.1.278.8123},
journal = {International Journal of Computer Applications},
keywords = {dynamic threshold classifier,feature extraction,learnable evolution,model,pattern recognition},
number = {11},
pages = {27--32},
title = {{A Generic Feature Extraction Model using Learnable Evolution Models (LEM+ ID3).}},
url = {http://findit.dtu.dk/en/catalog/2353090475 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Generic+Feature+Extraction+Model+using+Learnable+Evolution+Models+(+LEM+++ID3+){\#}0},
volume = {64},
year = {2013}
}
@inproceedings{Qian1999,
abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning- rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
author = {Qian, Ning},
booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1016/S0893-6080(98)00116-6},
file = {:Users/Jakob/Documents/Mendeley Desktop/Qian - 1999 - On the momentum term in gradient descent learning algorithms.pdf:pdf},
isbn = {1212543521},
issn = {08936080},
keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
number = {1},
pages = {145--151},
pmid = {12662723},
title = {{On the momentum term in gradient descent learning algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612{\&}rep=rep1{\&}type=pdf},
volume = {12},
year = {1999}
}
@article{Yin2011,
abstract = {A novel dynamic state space model was established for Global Positioning System (GPS) navigation by adopting the polynomial predictive filtering idea and state dimension expansion. We called the new model expanded state space model which was established without the exact knowledge of the original state dynamics, i.e., we way use the proposed model to describe the state dynamics no matter we know the original state propagation well or not. A correspondent Expanded State Space Kalman filter (ESSKF) was then presented based on the proposed model. The results of the GPS navigation examples demonstrated that the proposed method did work better than the existed Extended Kalman Filter (EKF), especially in the situations that the state dynamics were not known well. {\textcopyright} 2011 Asian Network for Scientific Information.},
author = {Yin, Jianjun and Gu, Ming and Zhang, Jianqiu},
doi = {10.3923/itj.2011.2091.2097},
issn = {18125638},
journal = {Information Technology Journal},
keywords = {Expanded state space kalman filter,GPS navigation,Kalman filter},
month = {nov},
number = {11},
pages = {2091--2097},
title = {{The Expanded State Space Kalman filter for GPS navigation}},
url = {http://www.scialert.net/abstract/?doi=itj.2011.2091.2097},
volume = {10},
year = {2011}
}
@article{Lample2018,
abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of bitexts, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage automatic generation of parallel data by backtranslating with a backward model operating in the other direction, and the denoising effect of a language model trained on the target side. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT14 English-French and WMT16 German-English benchmarks, our models respectively obtain 27.1 and 23.6 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points.},
archivePrefix = {arXiv},
arxivId = {1804.07755},
author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
eprint = {1804.07755},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lample et al. - 2018 - Phrase-Based {\&} Neural Unsupervised Machine Translation.pdf:pdf},
journal = {arXiv preprint},
title = {{Phrase-Based {\&} Neural Unsupervised Machine Translation}},
url = {http://arxiv.org/abs/1804.07755},
year = {2018}
}
@misc{Leung2016,
abstract = {In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.},
author = {Leung, Michael K. K. and Delong, Andrew and Alipanahi, Babak and Frey, Brendan J.},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2015.2494198},
isbn = {0018-9219 VO - 104},
issn = {00189219},
number = {1},
title = {{Machine learning in genomic medicine: A review of computational problems and data sets}},
volume = {104},
year = {2016}
}
@article{Lenz2015,
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
journal = {The International Journal of Robotics Research},
number = {4-5},
pages = {705--724},
publisher = {SAGE Publications Sage UK: London, England},
title = {{Deep learning for detecting robotic grasps}},
volume = {34},
year = {2015}
}
@inproceedings{Morse2016,
abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fi elds of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards efficient optimization. This paper challenges these views, suggesting that EAs can be made to run signi cantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the fi rst viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
author = {Morse, Gregory and Stanley, Kenneth O.},
booktitle = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
doi = {10.1145/2908812.2908916},
file = {:Users/Jakob/Documents/Mendeley Desktop/Morse, Stanley - 2016 - Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks.pdf:pdf},
isbn = {9781450342063},
pages = {477--484},
title = {{Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks}},
url = {http://dl.acm.org/citation.cfm?doid=2908812.2908916},
year = {2016}
}
@article{Welford1962,
annote = {Provides an numerically stable algorithm for online estimation of the sample variance through online updates to the squared distance of the sample from the sample mean. Also produces an online estimate of the mean},
author = {Welford, B. P.},
issn = {00401706},
journal = {Technometrics},
number = {3},
pages = {419--420},
title = {{Note on a Method for Calculating Correct Sums of Squares and Products}},
url = {http://findit.dtu.dk/en/catalog/2255540095},
volume = {4},
year = {1962}
}
@article{Zaremba2015,
abstract = {The expressive power of a machine learning model is closely related to the num-ber of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can per-form a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessi-tates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00521v1},
author = {Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1063/1.4939753},
eprint = {arXiv:1505.00521v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zaremba, Sutskever - 2015 - Reinforcement Learning Neural Turing Machines.pdf:pdf},
isbn = {9781424438617},
issn = {00036951},
journal = {arXiv preprint},
pages = {1--14},
title = {{Reinforcement Learning Neural Turing Machines}},
year = {2015}
}
@article{Dauphin,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dauphin et al. - Unknown - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf}
}
@inproceedings{Ulyanov2016,
author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
title = {{Texture networks: Feed-forward synthesis of textures and stylized images}},
year = {2016}
}
@article{Risi2015,
abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
archivePrefix = {arXiv},
arxivId = {1410.7326},
author = {Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2015.2494596},
eprint = {1410.7326},
file = {:Users/Jakob/Documents/Mendeley Desktop/Risi, Togelius - 2017 - Neuroevolution in Games State of the Art and Open Challenges.pdf:pdf},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
number = {1},
title = {{Neuroevolution in Games: State of the Art and Open Challenges}},
url = {https://arxiv.org/abs/1410.7326},
volume = {9},
year = {2017}
}
@book{Christensen2010,
abstract = {The purpose of this book is to present some mathematical tools that play key roles in mathematics as well as in appliedmathematics, physics, and en- gineering. The treatment is mathematical in nature, and we do not go into concrete applications; but it is important to stress that all the considered topics are selected because they actually play a role outside pure mathe- matics. The hope is that the book will be useful for students in many fields of science and engineering, and professionals who want a deeper insight in some of the topics appearing in the scientific literature.},
address = {Boston},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Christensen, Ole},
doi = {10.1007/978-0-8176-4980-7},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-8176-4979-1},
issn = {1098-6596},
pmid = {25246403},
publisher = {Birkh{\"{a}}user},
title = {{Functions, Spaces, and Expansions}},
url = {http://link.springer.com/10.1007/978-0-8176-4980-7},
year = {2010}
}
@article{Xiong2016,
author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
journal = {arXiv preprint},
title = {{Dynamic memory networks for visual and textual question answering}},
volume = {1603},
year = {2016}
}
@article{MacKay1992,
abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
author = {MacKay, David J. C.},
doi = {10.1162/neco.1992.4.3.448},
file = {:Users/Jakob/Documents/Mendeley Desktop/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation Networks.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {448--472},
title = {{A Practical Bayesian Framework for Backpropagation Networks}},
url = {https://pdfs.semanticscholar.org/e5c6/a695a4455a526ec8955dcc0fa2d6810089e9.pdf},
volume = {4},
year = {1992}
}
@inproceedings{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6550v4},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1145/1390156.1390294},
eprint = {arXiv:1412.6550v4},
file = {:Users/Jakob/Documents/Mendeley Desktop/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
pages = {1096--1103},
pmid = {15540460},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
@techreport{Hinton1988,
author = {Hinton, Geoffrey E. and McClelland, James L.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hinton, McClelland - 1988 - The Development of Time-Delay Neural Network Architecture for Speech Recognition.pdf:pdf},
institution = {Carnegie Mellon University},
isbn = {0780374029},
title = {{The Development of Time-Delay Neural Network Architecture for Speech Recognition}},
url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a221540.pdf},
year = {1988}
}
@article{Hinton2012a,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
annote = {From Duplicate 2 (Improving neural networks by preventing co-adaptation of feature detectors - Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.)

Introduces Dropout as regularization technique},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptation of feature detectors.pdf:pdf},
journal = {arXiv preprint},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
doi = {10.1021/ct2009208},
eprint = {1312.6199},
file = {:Users/Jakob/Documents/Mendeley Desktop/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
isbn = {1549-9618},
issn = {15499618},
month = {dec},
pmid = {22545027},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2013}
}
@misc{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:Users/Jakob/Documents/Mendeley Desktop/Arulkumaran et al. - 2017 - Deep reinforcement learning A brief survey.pdf:pdf},
isbn = {9781424469178},
issn = {10535888},
month = {aug},
number = {6},
pages = {26--38},
pmid = {25719670},
title = {{Deep reinforcement learning: A brief survey}},
url = {http://arxiv.org/abs/1708.05866},
volume = {34},
year = {2017}
}
@inproceedings{Ali2010,
abstract = {Question Generation (QG) and Question Answering (QA) are key challenges facing systems that interact with natural languages. The potential benefits of using automated systems to generate questions helps reduce the dependency on humans to generate questions and other needs associated with systems interacting with natural languages. In this paper we consider a system that automates generation of questions from a sentence, given a sentence, the system, will generate all possible questions which this sentence contain these questions answers. Since the given sentence may be a complex sentence, the system will generate elementary sentences, from the input complex sentences, using a syntactic parser. A part of speech tagger and a named entity recogniser are used to encode needed information. Based on the subject, verb, object and preposition the sentence will be classified, in order determine the type of questions that can possibly be generated from this sentence. We use development data provided by the Question Generation Shared Task Evaluation Challenge 2010. Keywords:},
author = {Ali, Husam and Chali, Yllias and Hasan, S.},
booktitle = {Proceedings of QG2010: The Third Workshop on Question Generation},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ali, Chali, Hasan - 2010 - Automation of Question Generation from Sentences.pdf:pdf},
keywords = {Elementary Sentence,Named Entity Tagging,Natural Language Processing,POS Tagging,Question Generation,Question generation,Recall.,Syntactic Parsing},
mendeley-tags = {Natural Language Processing,Question generation},
pages = {58--67},
title = {{Automation of Question Generation from Sentences}},
url = {http://www.sadidhasan.com/sadid-QG.pdf},
year = {2010}
}
@inproceedings{Oquab2014,
author = {Oquab, Maxime and Bottou, L{\'{e}}on and Laptev, Ivan and Sivic, Josef},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {1717--1724},
title = {{Learning and transferring mid-level image representations using convolutional neural networks}},
year = {2014}
}
@article{Kingma2015a,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
eprint = {1506.02557},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
journal = {arXiv preprint},
month = {jun},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://arxiv.org/abs/1506.02557},
year = {2015}
}
@article{Clark2015,
abstract = {We describe how a question-answering system can learn about its domain from conver-sational dialogs. Our system learns to relate concepts in science questions to propositions in a fact corpus, stores new concepts and re-lations in a knowledge graph (KG), and uses the graph to solve questions. We are the first to acquire knowledge for question-answering from open, natural language dialogs without a fixed ontology or domain model that predeter-mines what users can say. Our relation-based strategies complete more successful dialogs than a query expansion baseline, our task-driven relations are more effective for solving science questions than relations from general knowledge sources, and our method is practical enough to generalize to other domains.},
author = {Clark, Peter and Hixon, Ben},
file = {:Users/Jakob/Documents/Mendeley Desktop/Clark, Hixon - 2015 - Learning Knowledge Graphs for Question Answering through Conversational Dialog.pdf:pdf},
isbn = {9781941643495},
journal = {The Annual Conference of the North American Chapter of the ACL},
pages = {851--861},
title = {{Learning Knowledge Graphs for Question Answering through Conversational Dialog}},
url = {http://www.aclweb.org/anthology/N15-1086},
year = {2015}
}
@article{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1708.00107},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
eprint = {1708.00107},
file = {:Users/Jakob/Documents/Mendeley Desktop/McCann et al. - 2017 - Learned in Translation Contextualized Word Vectors.pdf:pdf},
title = {{Learned in Translation: Contextualized Word Vectors}},
url = {http://arxiv.org/abs/1708.00107},
year = {2017}
}
@article{Lehman2017,
abstract = {An evolution strategy (ES) variant recently attracted significant attention due to its surprisingly good performance at optimizing neural networks in challenging deep reinforcement learning domains. It searches directly in the parameter space of neural networks by generating perturbations to the current set of parameters, checking their performance, and moving in the direction of higher reward. The resemblance of this algorithm to a traditional finite-difference approximation of the reward gradient in parameter space naturally leads to the assumption that it is just that. However, this assumption is incorrect. The aim of this paper is to definitively demonstrate this point empirically. ES is a gradient approximator, but optimizes for a different gradient than just reward (especially when the magnitude of candidate perturbations is high). Instead, it optimizes for the average reward of the entire population, often also promoting parameters that are robust to perturbation. This difference can channel ES into significantly different areas of the search space than gradient descent in parameter space, and also consequently to networks with significantly different properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are far less robust to parameter perturbation than ES-based policies that solve the same task. While the implications of such robustness and robustness-seeking remain open to further study, the main contribution of this work is to highlight that such differences indeed exist and deserve attention.},
archivePrefix = {arXiv},
arxivId = {1712.06568},
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
eprint = {1712.06568},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lehman et al. - 2017 - ES Is More Than Just a Traditional Finite-Difference Approximator.pdf:pdf},
journal = {arXiv preprint},
title = {{ES Is More Than Just a Traditional Finite-Difference Approximator}},
url = {http://arxiv.org/abs/1712.06568},
year = {2017}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
isbn = {9781510829008},
issn = {1938-7228},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
year = {2016}
}
@inproceedings{Socher2013,
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y. and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1642},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
volume = {1631},
year = {2013}
}
@inproceedings{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {1502.05477},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Stanley2009,
abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algo-rithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Aug-menting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce con-nectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, con-nective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual dis-crimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
author = {Stanley, Kenneth O. and {D 'ambrosio}, David and Gauci, Jason},
doi = {10.1162/artl.2009.15.2.15202},
file = {:Users/Jakob/Documents/Mendeley Desktop/Stanley, D 'ambrosio, Gauci - 2009 - A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks.pdf:pdf},
issn = {1064-5462},
journal = {Artificial Life Journal},
keywords = {CPPNs,Compositional Pattern Producing Networks,HyperNEAT,generative and developmental systems,indirect encoding,large-scale artificial neural networks},
number = {2},
pages = {1--28},
pmid = {19199382},
title = {{A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks}},
volume = {15},
year = {2009}
}
@article{Im2015,
abstract = {Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06406},
author = {Im, Daniel Jiwoong and Ahn, Sungjin and Memisevic, Roland and Bengio, Yoshua},
eprint = {1511.06406},
file = {:Users/Jakob/Documents/Mendeley Desktop/Im et al. - 2015 - Denoising Criterion for Variational Auto-Encoding Framework.pdf:pdf},
journal = {arXiv preprint},
month = {nov},
title = {{Denoising Criterion for Variational Auto-Encoding Framework}},
url = {http://arxiv.org/abs/1511.06406},
year = {2015}
}
@book{Jones2015,
abstract = {Sixth edition.},
author = {Jones, Gareth R. and George, Jennifer M.},
isbn = {9780077862534},
pages = {545},
publisher = {McGraw-Hill Education,},
title = {{Essentials of contemporary management}},
url = {http://findit.dtu.dk/en/catalog?utf8=â{\&}locale=en{\&}search{\_}field=all{\_}fields{\&}q=essentials+of+contemporary+management+Gareth+jones},
year = {2015}
}
@article{Amari1998,
abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
author = {Amari, Shun-Ichi},
doi = {10.1162/089976698300017746},
file = {:Users/Jakob/Documents/Mendeley Desktop/Amari - 1998 - Natural Gradient Works Efficiently in Learning.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {251--276},
title = {{Natural Gradient Works Efficiently in Learning}},
volume = {10},
year = {1998}
}
@article{Xiong2017,
abstract = {Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1{\%} exact match accuracy and 83.1{\%} F1, while the ensemble obtains 78.9{\%} exact match accuracy and 86.0{\%} F1.},
archivePrefix = {arXiv},
arxivId = {1711.00106},
author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
eprint = {1711.00106},
file = {:Users/Jakob/Documents/Mendeley Desktop/Xiong, Zhong, Socher - 2017 - DCN Mixed Objective and Deep Residual Coattention for Question Answering.pdf:pdf},
journal = {arXiv preprint},
month = {oct},
title = {{DCN+: Mixed Objective and Deep Residual Coattention for Question Answering}},
url = {http://arxiv.org/abs/1711.00106},
year = {2017}
}
@article{Mohamed2012,
author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey E.},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {14--22},
publisher = {IEEE},
title = {{Acoustic modeling using deep belief networks}},
volume = {20},
year = {2012}
}
@article{Cortes1995,
abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
author = {Cortes, Corinna and Vapnik, Vladimir},
file = {:Users/Jakob/Documents/Mendeley Desktop/Cortes, Vapnik - 1995 - Support-Vector Networks.pdf:pdf},
journal = {Journal of Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
title = {{Support-Vector Networks}},
url = {http://image.diku.dk/imagecanon/material/cortes{\_}vapnik95.pdf},
volume = {20},
year = {1995}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@article{Mann1947,
abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
author = {Mann, H. B. and Whitney, D. R.},
doi = {10.1214/aoms/1177730491},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mann, Whitney - 1947 - On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {50--60},
pmid = {10226},
title = {{On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other}},
url = {http://projecteuclid.org/euclid.aoms/1177730491},
volume = {18},
year = {1947}
}
@article{Giryes2015,
abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
archivePrefix = {arXiv},
arxivId = {1504.08291},
author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
doi = {10.1109/TSP.2016.2546221},
eprint = {1504.08291},
file = {:Users/Jakob/Documents/Mendeley Desktop/Giryes, Sapiro, Bronstein - 2015 - Deep Neural Networks with Random Gaussian Weights A Universal Classification Strategy.pdf:pdf},
journal = {arXiv preprint},
month = {apr},
title = {{Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?}},
url = {http://arxiv.org/abs/1504.08291},
year = {2015}
}
@article{VanDerMaaten2008b,
abstract = {We present a new technique called ât-SNEâ that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {{Van Der Maaten}, L. J. P. and Hinton, Geoffrey E.},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
file = {:Users/Jakob/Documents/Mendeley Desktop/Van Der Maaten, Hinton - 2008 - Visualizing high-dimensional data using t-sne.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing high-dimensional data using t-sne}},
url = {https://lvdmaaten.github.io/publications/papers/JMLR{\_}2008.pdf{\%}0Ahttp://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@inproceedings{Desjardins2015,
abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1507.00210},
author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
eprint = {1507.00210},
file = {:Users/Jakob/Documents/Mendeley Desktop/Desjardins et al. - 2015 - Natural Neural Networks.pdf:pdf},
issn = {10495258},
title = {{Natural Neural Networks}},
url = {https://arxiv.org/pdf/1507.00210.pdf},
year = {2015}
}
@article{Vinyals2015,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {cs/1411.4555},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
eprint = {1411.4555},
isbn = {9781467369640},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {3156--3164},
primaryClass = {cs},
title = {{Show and Tell: A Neural Image Caption Generator}},
url = {http://arxiv.org/abs/1411.4555},
year = {2015}
}
@article{Montufar,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
author = {Mont{\'{u}}far, Guido and Cho, Kyunghyun and Bengio, Yoshua},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mont{\'{u}}far, Cho, Bengio - Unknown - On the Number of Linear Regions of Deep Neural Networks.pdf:pdf},
keywords = {Deep learning,input space partition,maxout,neural network,rectifier},
title = {{On the Number of Linear Regions of Deep Neural Networks}},
url = {http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf}
}
@article{Gregor2015,
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
journal = {arXiv preprint},
title = {{DRAW: A recurrent neural network for image generation}},
year = {2015}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:Users/Jakob/Documents/Mendeley Desktop/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Ciregan2012,
abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
archivePrefix = {arXiv},
arxivId = {1202.2745},
author = {CireÅan, Dan and Meier, Ueli and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/CVPR.2012.6248110},
eprint = {1202.2745},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
month = {feb},
pages = {3642--3649},
pmid = {22386783},
title = {{Multi-column Deep Neural Networks for Image Classification}},
url = {http://arxiv.org/abs/1202.2745},
year = {2012}
}
@article{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
archivePrefix = {arXiv},
arxivId = {1502.01852v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {1026--1034},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Xu2015,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice Y. and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
url = {http://arxiv.org/abs/1502.03044},
volume = {5},
year = {1994}
}
@inproceedings{Barber1998,
abstract = {Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.},
annote = {Full covariance Gaussian variational approximation to the Bayesian network weights.},
author = {Barber, David and Bishop, Christopher M.},
booktitle = {Generalization in Neural Networks and Machine Learning},
file = {:Users/Jakob/Documents/Mendeley Desktop/Barber, Bishop - 1998 - Ensemble learning in Bayesian neural networks.pdf:pdf},
pages = {215--237},
publisher = {Springer},
title = {{Ensemble learning in Bayesian neural networks}},
url = {https://www.microsoft.com/en-us/research/publication/ensemble-learning-in-bayesian-neural-networks/},
year = {1998}
}
@article{Wang2018,
abstract = {Computational protein design has a wide variety of applications. Despite its remarkable success, designing a protein for a given structure and function is still a challenging task. On the other hand, the number of solved protein structures is rapidly increasing while the number of unique protein folds has reached a steady number, suggesting more structural information is being accumulated on each fold. Deep learning neural network is a powerful method to learn such big data set and has shown superior performance in many machine learning fields. In this study, we applied the deep learning neural network approach to computational protein design for predicting the probability of 20 natural amino acids on each residue in a protein. A large set of protein structures was collected and a multi-layer neural network was constructed. A number of structural properties were extracted as input features and the best network achieved an accuracy of 38.3{\%}. Using the network output as residue type restraints was able to improve the average sequence identity in designing three natural proteins using Rosetta. Moreover, the predictions from our network show {\~{}}3{\%} higher sequence identity than a previous method. Results from this study may benefit further development of computational protein design methods.},
archivePrefix = {arXiv},
arxivId = {1801.07130},
author = {Wang, Jingxue and Cao, Huali and Zhang, John Z. H. and Qi, Yifei},
eprint = {1801.07130},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wang et al. - 2018 - Computational Protein Design with Deep Learning Neural Networks.pdf:pdf},
month = {jan},
title = {{Computational Protein Design with Deep Learning Neural Networks}},
url = {http://arxiv.org/abs/1801.07130},
year = {2018}
}
@misc{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
file = {:Users/Jakob/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe - 2017 - Variational Inference A Review for Statisticians.pdf:pdf},
isbn = {1601.00670},
issn = {1537274X},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
month = {jan},
number = {518},
pages = {859--877},
pmid = {303902},
title = {{Variational Inference: A Review for Statisticians}},
url = {http://arxiv.org/abs/1601.00670},
volume = {112},
year = {2017}
}
@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:Users/Jakob/Documents/Mendeley Desktop/Pennington, Socher, Manning - 2014 - Glove Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
volume = {14},
year = {2014}
}
@article{Graves2013a,
author = {Graves, Alex},
journal = {arXiv preprint},
title = {{Generating sequences with recurrent neural networks}},
year = {2013}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {1701--1708},
title = {{Deepface: Closing the gap to human-level performance in face verification}},
year = {2014}
}
@article{Olshausen1996,
abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1â4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7â12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13â18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
author = {Olshausen, Bruno A. and Field, David J.},
doi = {10.1038/381607a0},
file = {:Users/Jakob/Documents/Mendeley Desktop/Olshausen, Field - 1996 - Emergence of simple-cell receptive field properties by learning a sparse code for natural images.pdf:pdf},
isbn = {9781612849379},
issn = {00280836},
journal = {Nature},
number = {6583},
pages = {607--609},
pmid = {8637596},
title = {{Emergence of simple-cell receptive field properties by learning a sparse code for natural images}},
volume = {381},
year = {1996}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bengio - 2012 - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
url = {https://arxiv.org/pdf/1206.5533v2.pdf},
volume = {7700 LECTU},
year = {2012}
}
@inproceedings{Salakhutdinov2007,
abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6{\%} better than the score of Netflix's own system.},
archivePrefix = {arXiv},
arxivId = {1606.07129},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey E.},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1145/1273496.1273596},
eprint = {1606.07129},
isbn = {978-1-59593-793-3},
issn = {1468-1218},
keywords = {Stochastic network},
pages = {791--798},
pmid = {19932002},
title = {{Restricted Boltzmann Machines for Collaborative Filtering}},
url = {http://dx.doi.org/10.1145/1273496.1273596},
volume = {227},
year = {2007}
}
@article{Such2017,
abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.$\backslash$ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}4 hours on one desktop or {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
archivePrefix = {arXiv},
arxivId = {1712.06567},
author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1712.06567},
file = {:Users/Jakob/Documents/Mendeley Desktop/Such et al. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforc.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.06567},
year = {2017}
}
@article{Cormen2001,
author = {Cormen, T. H. and Leiserson, C. E. and Rivest, R. L. and Stein, C.},
doi = {10.2307/2583667},
file = {:Users/Jakob/Documents/Mendeley Desktop/Cormen et al. - 2001 - Introduction to algorithms - Selected Solutions.pdf:pdf},
isbn = {9780262032933},
issn = {01605682},
pages = {600--663},
title = {{Introduction to algorithms - Selected Solutions}},
year = {2001}
}
@article{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most âclassicalâ second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus Robert},
doi = {10.1007/978-3-642-35289-8-3},
eprint = {9809069v1},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {9--48},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Efficient backprop}},
volume = {7700 LECTU},
year = {2012}
}
@inproceedings{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
doi = {10.1109/CVPRW.2009.5206848},
file = {:Users/Jakob/Documents/Mendeley Desktop/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://www.image-net.org/papers/imagenet{\_}cvpr09.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@inproceedings{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bahdanau, Cho, Bengio - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
month = {sep},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{Mescheder2017,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1701.04722},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mescheder, Nowozin, Geiger - 2017 - Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint},
month = {jan},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.04722},
year = {2017}
}
@article{Szegedy2016a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
isbn = {978-1-4673-8851-1},
issn = {08866236},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@inproceedings{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ï¬rst observe the inï¬uence of the non-linear activations functions. We ï¬nd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ï¬nd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ï¬nd that a new non-linearity that saturates less can often be beneï¬cial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difï¬cult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {:Users/Jakob/Documents/Mendeley Desktop/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Nadarajah2011,
abstract = {A truncated version of the Cauchy distribution is introduced. Un-like the Cauchy distribution, this possesses finite moments of all orders and could therefore be a better model for certain practical situations. More than 10 practical situations where the truncated distribution could be applied are dis-cussed. Explicit expressions are derived for the moments, L moments, mean deviations, moment generating function, characteristic function, convolution properties, Bonferroni curve, Lorenz curve, entropies, order statistics and the asymptotic distribution of the extreme order statistics. Estimation procedures are detailed by the method of moments and the method of maximum likeli-hood and expressions derived for the associated Fisher information matrix. Simulation issues are discussed. Finally, an application is illustrated for con-sumer price indices from the six major economics.},
author = {Nadarajah, Saralees},
doi = {10.1214/09-BJPS112},
file = {:Users/Jakob/Documents/Mendeley Desktop/Nadarajah - 2011 - Making the Cauchy work.pdf:pdf},
issn = {01030752},
journal = {Brazilian Journal of Probability and Statistics},
keywords = {Cauchy distribution,Convolution,Estimation,Moments,Order statistics,Truncated cauchy distribution},
number = {1},
pages = {99--120},
title = {{Making the Cauchy work}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.bjps/1291387776},
volume = {25},
year = {2011}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann A.},
eprint = {1312.6229},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sermanet et al. - 2013 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
pages = {1312.6229},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Ji2013,
author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {1},
pages = {221--231},
publisher = {IEEE},
title = {{3D convolutional neural networks for human action recognition}},
volume = {35},
year = {2013}
}
@article{Magdon-Ismail2010,
abstract = {Covariance matrices capture correlations that are invaluable in mod-eling real-life datasets. Using all d 2 elements of the covariance (in d dimensions) is costly and could result in over-fitting; and the simple diagonal approximation can be over-restrictive. We present an algorithm that improves upon the diagonal matrix by allowing a low rank perturbation. The efficiency is comparable to the diagonal approximation, yet one can capture correlations among the dimensions. We show that this method outperforms the diagonal when training GMMs on both synthetic and real-world data.},
author = {Magdon-Ismail, Malik and Purnell, Jonathan T.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Magdon-Ismail, Purnell - 2010 - Approximating the Covariance Matrix with Low-rank Perturbations.pdf:pdf},
journal = {Intelligent Data Engineering and Automated Learning (IDEAL)},
keywords = {e-m,efficient,gaussian mixture models,maximum likelihood},
pages = {300----307},
title = {{Approximating the Covariance Matrix with Low-rank Perturbations}},
url = {https://pdfs.semanticscholar.org/481c/60b9e639eeaa438fd8e122cb66ea75edd72f.pdf},
year = {2010}
}
@article{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, Andrew M. and Le, Quoc V.},
eprint = {1511.01432},
file = {:Users/Jakob/Documents/Mendeley Desktop/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
issn = {10495258},
journal = {arXiv preprint},
keywords = {()},
pmid = {414454},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@article{Komarek2005,
abstract = {Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiers are accurate, fast, and automatic (i.e. no parameter tuning). Naive Bayes and decision trees are fast and parameter-free, but their accuracy is often below state-of-the-art. Linear support vector machines (SVM) are fast and have good accuracy, but current implementations are sensitive to the capacity parameter. SVMs with radial basis function kernels are accurate but slow, and have multiple parameters that require tuning. In this paper we demonstrate that a very simple parameter-free implementation of logistic regression (LR) is suffi-ciently accurate and fast to compete with state-of-the-art binary classifiers on large real-world datasets. The accuracy is comparable to per-dataset tuned linear SVMs and, in higher dimensions, to tuned RBF SVMs. A combination of reg-ularization, truncated-Newton methods, and iteratively re-weighted least squares make this implementation faster than SVMs and relatively insensitive to parameters. Our fitting procedure, TR-IRLS, appears to outperform several common LR fitting procedures in our experiments. TR-IRLS is robust to linear dependencies and scaling problems in the data, and no data preprocessing is necessary. TR-IRLS is easy to implement and can be used anywhere that IRLS is used. Convergence guarantees can be stated for generalized linear models with canonical links.},
author = {Komarek, Paul and Moore, Andrew},
file = {:Users/Jakob/Documents/Mendeley Desktop/Komarek, Moore - 2005 - Making Logistic Regression A Core Data Mining Tool.pdf:pdf},
title = {{Making Logistic Regression A Core Data Mining Tool}},
url = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=1217{\&}context=robotics},
year = {2005}
}
@incollection{Slaney1993a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and proteinâprotein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD â¤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Slaney, Malcolm and Lyon, Richard F.},
booktitle = {Visual Representations of Speech Signals},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Slaney, Lyon - 1993 - On the importance of time- A temporal representatin of sound.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {95--116},
pmid = {25246403},
title = {{On the importance of time- A temporal representatin of sound}},
url = {https://pdfs.semanticscholar.org/42ad/88836ecc471cd0667843ecc34098264363aa.pdf},
year = {1993}
}
@article{Duan2014,
abstract = {In this paper, we consider the low rank approximation of the symmetric positive semidefinite matrix, which arises in machine learning, quantum chemistry and inverse problem. We first characterize the feasible set by X=YYT,Y{\^{a}}ÌÌRnÃk, and then transform low rank approximation into an unconstrained optimization problem. Finally, we use the nonlinear conjugate gradient method with exact line search to compute the optimal low rank symmetric positive semidefinite approximation of the given matrix. Numerical examples show that the new method is feasible and effective. {\textcopyright} 2013 Elsevier Inc.},
author = {Duan, Xuefeng and Li, Jiaofen and Wang, Qingwen and Zhang, Xinjun},
doi = {10.1016/j.cam.2013.09.080},
file = {:Users/Jakob/Documents/Mendeley Desktop/Duan et al. - 2014 - Low rank approximation of the symmetric positive semidefinite matrix.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Feasible set,Low rank approximation,Nonlinear conjugate gradient method,Symmetric positive semidefinite matrix,Unconstrained optimization},
pages = {236--243},
title = {{Low rank approximation of the symmetric positive semidefinite matrix}},
url = {www.elsevier.com/locate/cam},
volume = {260},
year = {2014}
}
@article{Wang2013,
abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
author = {Wang, Sida I. and Manning, Christopher D.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wang, Manning - 2013 - Fast dropout training.pdf:pdf},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
pages = {118--126},
title = {{Fast dropout training}},
url = {http://proceedings.mlr.press/v28/wang13a.pdf http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
volume = {28},
year = {2013}
}
@article{Sehnke2010,
abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradi-ent by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the pa-rameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
author = {Sehnke, Frank and Osendorfer, Christian and R{\"{u}}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2009.12.004},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sehnke et al. - 2010 - Parameter-exploring policy gradients.pdf:pdf},
isbn = {9780769543000},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
month = {may},
number = {4},
pages = {551--559},
pmid = {20061118},
title = {{Parameter-exploring policy gradients}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104{\&}rep=rep1{\&}type=pdf http://linkinghub.elsevier.com/retrieve/pii/S0893608009003220},
volume = {23},
year = {2010}
}
@article{Bahdanau2016a,
abstract = {We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
eprint = {1607.07086},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bahdanau et al. - 2016 - An Actor-Critic Algorithm for Sequence Prediciton.pdf:pdf},
month = {jul},
title = {{An Actor-Critic Algorithm for Sequence Prediciton}},
url = {http://arxiv.org/abs/1607.07086 https://arxiv.org/pdf/1607.07086.pdf},
year = {2016}
}
@article{Staines2012,
abstract = {We discuss a general technique that can be used to form a differentiable bound on the optima of non-differentiable or discrete objective functions. We form a unified description of these methods and consider under which circumstances the bound is concave. In particular we consider two concrete applications of the method, namely sparse learning and support vector classification.},
archivePrefix = {arXiv},
arxivId = {1212.4507},
author = {Staines, Joe and Barber, David},
eprint = {1212.4507},
file = {:Users/Jakob/Documents/Mendeley Desktop/Staines, Barber - 2012 - Variational Optimization.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Variational Optimization}},
url = {http://arxiv.org/abs/1212.4507},
year = {2012}
}
@article{Farabet2013,
author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann A.},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {8},
pages = {1915--1929},
publisher = {IEEE},
title = {{Learning hierarchical features for scene labeling}},
volume = {35},
year = {2013}
}
@article{Girshick2016,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {1},
pages = {142--158},
publisher = {IEEE},
title = {{Region-based convolutional networks for accurate object detection and segmentation}},
volume = {38},
year = {2016}
}
@article{Levine2016a,
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
journal = {Journal of Machine Learning Research},
number = {39},
pages = {1--40},
title = {{End-to-end training of deep visuomotor policies}},
volume = {17},
year = {2016}
}
@article{Park2013,
abstract = {Triangle counting problem is one of the fundamental prob-lem in various domains. The problem can be utilized for computation of clustering coefficient, transitivity, trianglu-lar connectivity, trusses, etc. The problem have been exten-sively studied in internal memory but the algorithms are not scalable for enormous graphs. In recent years, the MapRe-duce has emerged as a de facto standard framework for pro-cessing large data through parallel computing. A MapRe-duce algorithm was proposed for the problem based on graph partitioning. However, the algorithm redundantly generates a large number of intermediate data that cause network over-load and prolong the processing time. In this paper, we pro-pose a new algorithm based on graph partitioning with a novel idea of triangle classification to count the number of triangles in a graph. The algorithm substantially reduces the duplication by classifying triangles into three types and processing each triangle differently according to its type. In the experiments, we compare the proposed algorithm with recent existing algorithms using both synthetic datasets and real-world datasets that are composed of millions of nodes and billions of edges. The proposed algorithm outperforms other algorithms in most cases. Especially, for a twitter dataset, the proposed algorithm is more than twice as fast as existing MapReduce algorithms. Moreover, the performance gap increases as the graph becomes larger and denser.},
author = {Park, Ha-Myung and Chung, Chin-Wan},
doi = {10.1145/2505515.2505563},
isbn = {9781450322638},
journal = {CIKM},
keywords = {graph,mapreduce,triangle},
title = {{An efficient MapReduce algorithm for counting triangles in a very large graph}},
year = {2013}
}
@article{Schaul2012,
abstract = {Natural Evolution Strategies (NES) are a recent member of the class of real-valued optimization algorithms that are based on adapting search distributions. Exponential NES (xNES) are the most common instantiation of NES, and particularly appropriate for the BBOB 2012 benchmarks, given that many are non-separable, and their relatively small problem dimensions. The technique of adaptation sampling, which adapts learning rates online further improves the al-gorithm's performance. This report provides an extensive empirical comparison to study the impact of adaptation sampling in xNES, both on the noise-free and noisy BBOB testbeds.},
author = {Schaul, Tom},
doi = {10.1145/2330784.2330817},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schaul - 2012 - Investigating the Impact of Adaptation Sampling in Natural Evolution Strategies on Black-box Optimization Testbeds(2).pdf:pdf},
isbn = {9781450311786},
journal = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
keywords = {benchmarking,evolution strategies,natural gradient},
pages = {221},
title = {{Investigating the Impact of Adaptation Sampling in Natural Evolution Strategies on Black-box Optimization Testbeds}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=310C1A13DBCF68AA1BE351398A89596B?doi=10.1.1.296.1961{\&}rep=rep1{\&}type=pdf},
year = {2012}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:Users/Jakob/Documents/Mendeley Desktop/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
journal = {arXiv preprint},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Ji2016,
abstract = {We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion of words.},
archivePrefix = {arXiv},
arxivId = {1511.06909},
author = {Ji, Shihao and Vishwanathan, S. V. N. and Satish, Nadathur and Anderson, Michael J. and Dubey, Pradeep},
eprint = {1511.06909},
file = {:Users/Jakob/Documents/Mendeley Desktop/Ji et al. - 2016 - BlackOut Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies.pdf:pdf},
journal = {Under review of ICLR},
pages = {1--12},
title = {{BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies}},
url = {http://arxiv.org/abs/1511.06909},
year = {2016}
}
@inproceedings{Reddi2018,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
author = {Reddi, S. J. and Kale, S. and Kumar, S.},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
file = {:Users/Jakob/Documents/Mendeley Desktop/Reddi, Kale, Kumar - 2018 - On the Convergence of Adam and Beyond.pdf:pdf},
pages = {1--23},
title = {{On the Convergence of Adam and Beyond}},
url = {https://openreview.net/pdf?id=ryQu7f-RZ},
year = {2018}
}
@phdthesis{Neal1996,
abstract = {Artificial ``neural networks'' are now widely used as flexible models for regression and classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the ``overfitting'' that can occur with traditional neural network learning methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. Use of these models in practice is made possible using Markov chain Monte Carlo techniques. Both the theoretical and computational aspects of this work are of wider statistical interest, as they contribute to a better understanding of how Bayesian methods can be applied to complex problems. Presupposing only basic knowledge of probability and statistics, this book should be of interest to many researchers in Statistics, Engineering, and Artificial Intelligence. Software for Unix systems that implements the methods described is freely available over the Internet.},
annote = {Thesis establishes link between BNNs and Gaussian processes and describes ARD (automatic relevance determination) among other things.},
author = {Neal, Radford M.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Neal - 1995 - Bayesian Learning for Neural Networks.pdf:pdf},
school = {University of Toronto},
title = {{Bayesian Learning for Neural Networks}},
year = {1995}
}
@article{Goodfellow2014b,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1412.6572},
file = {:Users/Jakob/Documents/Mendeley Desktop/Goodfellow, Shlens, Szegedy - 2014 - Explaining and Harnessing Adversarial Examples.pdf:pdf},
isbn = {1412.6572},
issn = {0012-7183},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {https://arxiv.org/pdf/1412.6572.pdf http://arxiv.org/abs/1412.6572},
year = {2014}
}
@article{Ciresan2012,
abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46{\%}. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination. {\textcopyright} 2012 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.2745v1},
author = {CireÅan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2012.02.023},
eprint = {arXiv:1202.2745v1},
isbn = {0893-6080},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
keywords = {Deep neural networks,Image classification,Image preprocessing,Traffic signs},
month = {aug},
number = {Sp. Iss. SI},
pages = {333--338},
pmid = {22386783},
title = {{Multi-column deep neural network for traffic sign classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608012000524},
volume = {32},
year = {2012}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/Jakob/Documents/Mendeley Desktop/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
journal = {arXiv preprint},
title = {{Proximal Policy Optimization Algorithms}},
url = {https://arxiv.org/pdf/1707.06347v2.pdf http://arxiv.org/abs/1707.06347},
year = {2017}
}
@inproceedings{Collobert2008,
abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1603.06111},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
doi = {10.1145/1390156.1390177},
eprint = {1603.06111},
file = {:Users/Jakob/Documents/Mendeley Desktop/Collobert, Weston - 2008 - A unified architecture for natural language processing.pdf:pdf},
isbn = {9781605582054},
issn = {07224028},
pages = {160--167},
pmid = {2975184},
title = {{A unified architecture for natural language processing}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390177},
year = {2008}
}
@article{Lee2017a,
abstract = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.},
archivePrefix = {arXiv},
arxivId = {1707.07045},
author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
doi = {10.18653/v1/D17-1018},
eprint = {1707.07045},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lee et al. - 2017 - End-to-end Neural Coreference Resolution.pdf:pdf},
title = {{End-to-end Neural Coreference Resolution}},
url = {http://arxiv.org/abs/1707.07045},
year = {2017}
}
@inproceedings{Heusel2017,
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr$\backslash$'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.08500},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
eprint = {1706.08500},
file = {:Users/Jakob/Documents/Mendeley Desktop/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:pdf},
issn = {10495258},
title = {{GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}},
url = {https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf},
year = {2017}
}
@article{Levine2016,
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
journal = {arXiv preprint},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
year = {2016}
}
@article{Blaabjerg2016,
author = {Blaabjerg, Lasse and Havtorn, Jakob D. and Svendsen, Winnie E. and Abi, Alireza and Kwasny, Dorota},
file = {:Users/Jakob/Documents/Mendeley Desktop/Blaabjerg et al. - 2016 - Early stage investigation of sensor technologies and related bioassay for in vivo diagnosis of colorectal canc.pdf:pdf},
title = {{Early stage investigation of sensor technologies and related bioassay for in vivo diagnosis of colorectal cancer}},
year = {2016}
}
@article{Lydia2016,
abstract = {Wind speed forecasting aids in estimating the energy produced from wind farms. The soaring energy demands of the world and minimal availability of conventional energy sources have significantly increased the role of non-conventional sources of energy like solar, wind, etc. Development of models for wind speed forecasting with higher reliability and greater accuracy is the need of the hour. In this paper, models for predicting wind speed at 10-min intervals up to 1 h have been built based on linear and non-linear autoregressive moving average models with and without external variables. The autoregressive moving average models based on wind direction and annual trends have been built using data obtained from Sotavento Galicia Plc. and autoregressive moving average models based on wind direction, wind shear and temperature have been built on data obtained from Centre for Wind Energy Technology, Chennai, India. While the parameters of the linear models are obtained using the Gauss-Newton algorithm, the non-linear autoregressive models are developed using three different data mining algorithms. The accuracy of the models has been measured using three performance metrics namely, the Mean Absolute Error, Root Mean Squared Error and Mean Absolute Percentage Error.},
author = {Lydia, M. and {Suresh Kumar}, S. and {Immanuel Selvakumar}, A. and {Edwin Prem Kumar}, G.},
doi = {10.1016/j.enconman.2016.01.007},
file = {:Users/Jakob/Documents/Mendeley Desktop/Lydia et al. - 2016 - Linear and non-linear autoregressive models for short-term wind speed forecasting.pdf:pdf},
issn = {01968904},
journal = {Energy Conversion and Management},
keywords = {Annual trends,Auto-regressive moving average,Data mining,Multivariate models,Time-series forecasting,Wind shear},
pages = {115--124},
title = {{Linear and non-linear autoregressive models for short-term wind speed forecasting}},
url = {https://ac-els-cdn-com.proxy.findit.dtu.dk/S0196890416000236/1-s2.0-S0196890416000236-main.pdf?{\_}tid=01986468-a4de-11e7-a1bc-00000aab0f6b{\&}acdnat=1506666087{\_}2d9de961875491c6e10188d72758b4b6},
volume = {112},
year = {2016}
}
@article{Samuel1959,
abstract = {Two machine-learning procedures have been investigated 1 in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Further- more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
author = {Samuel, Artur L},
doi = {10.1147/rd.33.0210},
file = {:Users/Jakob/Documents/Mendeley Desktop/Samuel - 1959 - Some Studies in Machine Learning using the Game of Checkers.pdf:pdf},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some Studies in Machine Learning using the Game of Checkers}},
volume = {3},
year = {1959}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by âlearning without a teacherâ, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname âneocognitronâ. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of âS-cellsâ, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of âC-cellsâ similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any âteacherâ during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
author = {Fukushima, Kunihiko},
file = {:Users/Jakob/Documents/Mendeley Desktop/Fukushima - 1980 - Neocognitron A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Pos.pdf:pdf},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
title = {{Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position}},
url = {http://www.rctn.org/bruno/public/papers/Fukushima1980.pdf},
volume = {36},
year = {1980}
}
@article{Ferrucci2010,
author = {Ferrucci, David},
issn = {07384602},
journal = {Ai Magazine},
number = {3},
title = {{Building Watson: An Overview of the DeepQA Project}},
url = {https://findit.dtu.dk/en/catalog?utf8=â{\&}locale=en{\&}search{\_}field=all{\_}fields{\&}q=building+watson+an+overview+of+deepqa},
volume = {31},
year = {2010}
}
@article{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
annote = {Combines SGD with Langevin dynamics (a form of MCMC) get a highly scalable approximate MCMC algorithm based minibatch SGD.
Doing Bayesian inference can be as simple as running noisy SGD},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Welling, Max and Teh, Yee Whye},
doi = {10.1515/jip-2012-0071},
eprint = {arXiv:1203.5753v5},
file = {:Users/Jakob/Documents/Mendeley Desktop/Welling, Teh - 2011 - Bayesian posterior contraction rates for linear severely ill-posed inverse problems.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {10495258},
journal = {Proceedings of the International Conference on Machine Learning (ICML)},
keywords = {Gaussian prior,posterior consistency,rate of contraction,severely ill-posed problems},
number = {3},
pages = {681--688},
title = {{Bayesian posterior contraction rates for linear severely ill-posed inverse problems}},
url = {https://www.ics.uci.edu/{~}welling/publications/papers/stoclangevin{\_}v6.pdf},
volume = {22},
year = {2011}
}
@article{Li2012,
abstract = {The fast growth of wind power is in urgent need of more accurate, reliable, and adaptive modeling and data analysis methods for the characterization and prediction of wind resource and wind power, as well as reliability evaluation of wind energy conversion systems. Bayesian methods have shown unique advantages in statistical modeling and data analysis for the quantity of interest with uncertainty and variability. The adoption of Bayesian methods carries great potentials for various aspects in wind energy conversion systems such as improving the accuracy and reliability of wind resource estimation and short-term forecasts. This paper summarizes the basic theories of several Bayesian methods, and extensively reviews the literature addressing the applications of Bayesian methods in wind energy conversion systems. Based on the state-of-the-art review, the prospects of Bayesian methods in wind energy conversion systems are discussed on how to develop new applications and enhance the methods for existing applications. It is believed that Bayesian methods will be gaining more momentum in wind energy applications in the near future.},
author = {Li, Gong and Shi, Jing},
doi = {10.1016/j.renene.2011.12.006},
file = {:Users/Jakob/Documents/Mendeley Desktop/Li, Shi - 2012 - Applications of Bayesian methods in wind energy conversion systems.pdf:pdf},
journal = {Renewable Energy},
pages = {1--8},
title = {{Applications of Bayesian methods in wind energy conversion systems}},
url = {https://ac-els-cdn-com.proxy.findit.dtu.dk/S0960148111006768/1-s2.0-S0960148111006768-main.pdf?{\_}tid=b8c6e76e-a4dd-11e7-89e0-00000aacb360{\&}acdnat=1506665965{\_}1bd4fe3f56b4686e1a91aa8989825f30},
volume = {43},
year = {2012}
}
@article{Goodfellow2013,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
doi = {10.1093/bib/bbw065.},
eprint = {1302.4389},
issn = {1467-5463},
month = {feb},
title = {{Maxout Networks}},
url = {http://arxiv.org/abs/1302.4389},
year = {2013}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel P.},
file = {:Users/Jakob/Documents/Mendeley Desktop/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
journal = {Computing Research Repository},
keywords = {deep learning,natural language processing,neural networks},
pages = {1--34},
title = {{Natural Language Processing (almost) from Scratch}},
url = {https://arxiv.org/pdf/1103.0398v1.pdf},
volume = {abs/1103.0},
year = {2011}
}
@inproceedings{Bengio2013a,
abstract = {After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
archivePrefix = {arXiv},
arxivId = {1212.0901},
author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2013.6639349},
eprint = {1212.0901},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bengio, Boulanger-Lewandowski, Pascanu - 2013 - Advances in optimizing recurrent networks.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Recurrent networks,deep learning,long-term dependencies,representation learning},
pages = {8624--8628},
pmid = {25246403},
title = {{Advances in optimizing recurrent networks}},
url = {https://arxiv.org/pdf/1212.0901.pdf},
year = {2013}
}
@phdthesis{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pages = {1--60},
pmid = {25246403},
school = {University of Toronto},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {http://www.cs.toronto.edu/{~}kriz/learning-features-2009-TR.pdf},
year = {2009}
}
@inproceedings{Kalchbrenner2014,
abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
archivePrefix = {arXiv},
arxivId = {1404.2188v1},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.3115/v1/P14-1062},
eprint = {1404.2188v1},
file = {:Users/Jakob/Documents/Mendeley Desktop/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
isbn = {9781937284725},
issn = {03064573},
pages = {655--665},
pmid = {15003161},
title = {{A Convolutional Neural Network for Modelling Sentences}},
url = {https://arxiv.org/abs/1404.2188},
year = {2014}
}
@inproceedings{Sutskever2014a,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
annote = {From Duplicate 1 (Sequence to sequence learning with neural networks - Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V)

Introduces the sequence to sequence},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3215v3},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
eprint = {arXiv:1409.3215v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {https://arxiv.org/abs/1409.3215},
year = {2014}
}
@inproceedings{Ros2008,
abstract = {This paper proposes a simple modification of the Covariance Matrix$\backslash$nAdaptation Evolution Strategy (CMA-ES) for high dimensional objective$\backslash$nfunctions, reducing the internal time and space complexity from quadratic$\backslash$nto linear. The covariance matrix is constrained to be diagonal and$\backslash$nthe resulting algorithm, sep-CMA-ES, samples each coordinate independently.$\backslash$nBecause the model complexity is reduced, the learning rate for the$\backslash$ncovariance matrix can be increased. Consequently, on essentially$\backslash$nseparable functions, sep-CMA-ES significantly outperforms CMA-ES$\backslash$n. For dimensions larger than a hundred, even on the non-separable$\backslash$nRosenbrock function, the sep-CMA-ES needs fewer function evaluations$\backslash$nthan CMA-ES .},
author = {Ros, Raymond and Hansen, Nikolaus},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87700-4_30},
isbn = {3540876995},
issn = {03029743},
pages = {296--305},
title = {{A simple modification in CMA-ES achieving linear time and space complexity}},
url = {http://link.springer.com/10.1007/978-3-540-87700-4{\_}30},
volume = {5199 LNCS},
year = {2008}
}
@article{Wiseman,
abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa-tions, and we report the best overall score on the CoNLL 2012 English test set to date.},
author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M. and Weston, Jason},
file = {:Users/Jakob/Documents/Mendeley Desktop/Wiseman et al. - Unknown - Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution.pdf:pdf},
title = {{Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution}},
url = {http://people.seas.harvard.edu/{~}srush/acl15.pdf}
}
@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
isbn = {978-1-4673-8851-1},
issn = {01689002},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {779--788},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the âfolk knowledgeâ that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:Users/Jakob/Documents/Mendeley Desktop/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
url = {https://homes.cs.washington.edu/{~}pedrod/papers/cacm12.pdf http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Katharopoulos2018,
abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5{\%} and 17{\%}.},
archivePrefix = {arXiv},
arxivId = {1803.00942},
author = {Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
eprint = {1803.00942},
file = {:Users/Jakob/Documents/Mendeley Desktop/Katharopoulos, Fleuret - 2018 - Not All Samples Are Created Equal Deep Learning with Importance Sampling.pdf:pdf},
journal = {arXiv preprint},
month = {mar},
title = {{Not All Samples Are Created Equal: Deep Learning with Importance Sampling}},
url = {http://arxiv.org/abs/1803.00942},
year = {2018}
}
@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutn{\'{i}}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:Users/Jakob/Documents/Mendeley Desktop/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
url = {https://arxiv.org/pdf/1503.04069.pdf},
volume = {28},
year = {2017}
}
@article{Mostafazadeh2016,
abstract = {There has been an explosion of work in the vision {\&} language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision {\&} language.},
archivePrefix = {arXiv},
arxivId = {1603.06059},
author = {Mostafazadeh, Nasrin and Misra, Ishan and Devlin, Jacob and Mitchell, Margaret and He, Xiaodong and Vanderwende, Lucy},
eprint = {1603.06059},
file = {:Users/Jakob/Documents/Mendeley Desktop/Mostafazadeh et al. - 2016 - Generating Natural Questions About an Image.pdf:pdf},
isbn = {9781510827585},
issn = {9781510827585},
journal = {arXiv preprint},
month = {mar},
title = {{Generating Natural Questions About an Image}},
url = {https://arxiv.org/abs/1603.06059},
year = {2016}
}
@inproceedings{SharifRazavian2014,
author = {{Sharif Razavian}, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {806--813},
title = {{CNN features off-the-shelf: an astounding baseline for recognition}},
year = {2014}
}
@article{Bai,
abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar-chitectures can outperform recurrent networks on tasks such as audio synthesis and machine trans-lation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo-lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our re-sults indicate that a simple convolutional archi-tecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common associ-ation between sequence modeling and recurrent networks should be reconsidered, and convolu-tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
file = {:Users/Jakob/Documents/Mendeley Desktop/Bai, Kolter, Koltun - Unknown - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf:pdf},
title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
url = {https://arxiv.org/pdf/1803.01271.pdf}
}
@article{Simard2003,
abstract = {Neural Networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution neural networks does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, J. C.},
doi = {10.1109/ICDAR.2003.1227801},
eprint = {arXiv:1011.1669v3},
file = {:Users/Jakob/Documents/Mendeley Desktop/Simard, Steinkraus, Platt - 2003 - Best practices for convolutional neural networks applied to visual document analysis.pdf:pdf},
isbn = {0-7695-1960-1},
issn = {15205363},
journal = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
number = {Icdar},
pages = {958--963},
pmid = {25246403},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
url = {https://pdfs.semanticscholar.org/7b1c/c19dec9289c66e7ab45e80e8c42273509ab6.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801},
volume = {1},
year = {2003}
}
@article{Geweke1988,
abstract = {It is proposed to sample antithetically rather than randomly from the posterior density in Bayesian inference using Monte Carlo integration. Conditions are established under which the number of replications required with antithetic sampling relative to the number required with random sampling is inversely proportional to sample size, as sample size increases. The result is illustrated in an experiment using a bivariate vector autoregression. {\textcopyright} 1988.},
author = {Geweke, John},
doi = {10.1016/0304-4076(88)90027-9},
file = {:Users/Jakob/Documents/Mendeley Desktop/Geweke - 1988 - Antithetic acceleration of Monte Carlo integration in Bayesian inference.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
number = {1-2},
pages = {73--89},
publisher = {North-Holland},
title = {{Antithetic acceleration of Monte Carlo integration in Bayesian inference}},
url = {http://production.datastore.cvt.dk/filestore?oid=5331c75e94c29b733479a4f5{\&}targetid=5331c75e94c29b733479a4f9},
volume = {38},
year = {1988}
}
@article{Luong2016,
abstract = {Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.},
archivePrefix = {arXiv},
arxivId = {1604.00788},
author = {Luong, Minh-Thang and Manning, Christopher D.},
eprint = {1604.00788},
file = {:Users/Jakob/Documents/Mendeley Desktop/Luong, Manning - 2016 - Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models.pdf:pdf},
isbn = {9781510827585},
journal = {arXiv preprint},
title = {{Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models}},
url = {http://arxiv.org/abs/1604.00788},
year = {2016}
}
@article{Hinton2012,
author = {Hinton, G and Deng, Li and Yu, Dong and Dahl, GE},
journal = {IEEE Signal},
number = {6},
pages = {82--97},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6296526},
volume = {29},
year = {2012}
}
@article{Garipov2018,
abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56{\%} by running FGE for just 5 epochs.},
archivePrefix = {arXiv},
arxivId = {1802.10026},
author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
eprint = {1802.10026},
file = {:Users/Jakob/Documents/Mendeley Desktop/Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.pdf:pdf},
journal = {arXiv preprint},
title = {{Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}},
url = {http://arxiv.org/abs/1802.10026},
year = {2018}
}
@inproceedings{Szegedy2015,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
pages = {1--9},
title = {{Going deeper with convolutions}},
year = {2015}
}
@inproceedings{Sajjadi,
abstract = {Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.},
archivePrefix = {arXiv},
arxivId = {1612.07919},
author = {Sajjadi, Mehdi S.M. and Scholkopf, Bernhard and Hirsch, Michael},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.481},
eprint = {1612.07919},
file = {:Users/Jakob/Documents/Mendeley Desktop/Sajjadi, Scholkopf, Hirsch - 2017 - EnhanceNet Single Image Super-Resolution Through Automated Texture Synthesis.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {4501--4510},
title = {{EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis}},
url = {https://arxiv.org/pdf/1612.07919.pdf},
year = {2017}
}
@inproceedings{Antol2015,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and {Lawrence Zitnick}, C. and Parikh, Devi},
booktitle = {IEEE International Conference on Computer Vision},
pages = {2425--2433},
title = {{{\{}VQA{\}}: Visual question answering}},
year = {2015}
}
@article{Chatfield2014,
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
journal = {arXiv preprint},
title = {{Return of the devil in the details: Delving deep into convolutional nets}},
year = {2014}
}
@article{Collobert2016,
abstract = {This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.},
archivePrefix = {arXiv},
arxivId = {1609.03193},
author = {Collobert, Ronan and Puhrsch, Christian and Synnaeve, Gabriel},
eprint = {1609.03193},
file = {:Users/Jakob/Documents/Mendeley Desktop/Collobert, Puhrsch, Synnaeve - 2016 - Wav2Letter an End-to-End ConvNet-based Speech Recognition System.pdf:pdf},
month = {sep},
title = {{Wav2Letter: an End-to-End ConvNet-based Speech Recognition System}},
url = {http://arxiv.org/abs/1609.03193},
year = {2016}
}
@article{Hansen2001,
abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
author = {Hansen, Nikolaus and Ostermeier, Andreas},
doi = {10.1162/106365601750190398},
file = {:Users/Jakob/Documents/Mendeley Desktop/Hansen, Ostermeier - 2001 - Completely Derandomized Self-Adaptation in Evolution Strategies.pdf:pdf},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {2},
pages = {159--195},
pmid = {11382355},
title = {{Completely Derandomized Self-Adaptation in Evolution Strategies}},
url = {http://www.mitpressjournals.org/doi/10.1162/106365601750190398},
volume = {9},
year = {2001}
}
@article{Szegedy2016,
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
journal = {arXiv preprint},
title = {{Inception-v4, inception-resnet and the impact of residual connections on learning}},
year = {2016}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
archivePrefix = {arXiv},
arxivId = {1702.00764},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
doi = {10.3115/1073083.1073135},
eprint = {1702.00764},
file = {:Users/Jakob/Documents/Mendeley Desktop/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
journal = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@article{Henderson2013,
abstract = {While belief tracking is known to be im-portant in allowing statistical dialog sys-tems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple be-lief tracking approaches on a shared task. Recent success in using deep learning for speech research motivates the Deep Neu-ral Network approach presented here. The model parameters can be learnt by directly maximising the likelihood of the training data. The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particu-larly on a corpus of dialogs from a system not found in the training.},
author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
file = {:Users/Jakob/Documents/Mendeley Desktop/Henderson, Thomson, Young - 2013 - Deep Neural Network Approach for the Dialog State Tracking Challenge.pdf:pdf},
isbn = {9781937284954},
journal = {Proceedings of the SIGDIAL 2013 Conference},
pages = {467--471},
title = {{Deep Neural Network Approach for the Dialog State Tracking Challenge}},
url = {http://www.aclweb.org/anthology/W/W13/W13-4073},
year = {2013}
}
@article{Suzuki2014,
abstract = {We review basic notions in the field of information geometry such as Fisher metric on statistical manifold, {\$}\backslashalpha{\$}-connection and corresponding curvature following Amari's work . We show application of information geometry to asymptotic statistical inference.},
archivePrefix = {arXiv},
arxivId = {1410.3369},
author = {Suzuki, Mashbat},
eprint = {1410.3369},
file = {:Users/Jakob/Documents/Mendeley Desktop/Suzuki - 2014 - Information Geometry and Statistical Manifold.pdf:pdf},
journal = {arXiv preprint},
keywords = {adaboost,exponential family,logistic model,maximum likelihood,u -divergence,u -loss function,u -model},
month = {oct},
title = {{Information Geometry and Statistical Manifold}},
url = {http://arxiv.org/abs/1410.3369},
year = {2014}
}
@article{Tascikaraoglu2014,
abstract = {With the continuous increase of wind power penetration in power systems, the problems caused by the volatile nature of wind speed and its occurrence in the system operations such as scheduling and dispatching have drawn attention of system operators, utilities and researchers towards the state-of-the-art wind speed and power forecasting methods. These methods have the required capability of reducing the influence of the intermittent wind power on system operations as well as of harvesting the wind energy effectively. In this context, combining different methodologies in order to circumvent the challenging model selection and take advantage of the unique strength of plausible models have recently emerged as a promising research area. Therefore, a comprehensive research about the combined models is called on for how these models are constructed and affect the forecasting performance. Aiming to fill the mentioned research gap, this paper outlines the combined forecasting approaches and presents an up-to date annotated bibliography of the wind forecasting literature. Furthermore, the paper also points out the possible further research directions of combined techniques so as to help the researchers in the field develop more effective wind speed and power forecasting methods.},
author = {Tascikaraoglu, A. and Uzunoglu, M.},
doi = {10.1016/j.rser.2014.03.033},
file = {:Users/Jakob/Documents/Mendeley Desktop/Tascikaraoglu, Uzunoglu - 2014 - A review of combined approaches for prediction of short-term wind speed and power.pdf:pdf},
journal = {Renewable and Sustainable Energy Reviews},
pages = {243--254},
title = {{A review of combined approaches for prediction of short-term wind speed and power}},
url = {https://ac-els-cdn-com.proxy.findit.dtu.dk/S1364032114001944/1-s2.0-S1364032114001944-main.pdf?{\_}tid=c1b05f0e-a4dd-11e7-bf4d-00000aacb360{\&}acdnat=1506665980{\_}ae9aaaadcd1ff9d72bdc376b5cefa3ea},
volume = {34},
year = {2014}
}
